{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG LLM using LlamaIndex & Gemini Model to query the multiple PDFs\n",
        "\n",
        "1. LlamaIndex, in general it uses the openAI API across vectorStore, queryEngine, retrieval, etc.,\n",
        "2. In the below code, we're customizing it total to use the gemini as LLM and HuggingFace as embedding model and gemini API across retrieval, query engine, etc.,"
      ],
      "metadata": {
        "id": "Bhz870oCrw9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jaish19/GenAI---RAG-using-LangChain.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GDTK2Tlsvwe",
        "outputId": "8b33c43a-6367-4cff-ca1e-d5a21217cfc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GenAI---RAG-using-LangChain'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 79 (delta 35), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (79/79), 8.39 MiB | 4.67 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv\n",
        "!pip install llama_index.llms.gemini\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4KZvPBaOaB6D",
        "outputId": "50ac807f-79b0-4cb1-957e-0a312c8786e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.0)\n",
            "Requirement already satisfied: llama_index.llms.gemini in /usr/local/lib/python3.11/dist-packages (0.4.14)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from llama_index.llms.gemini) (0.8.5)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.12 in /usr/local/lib/python3.11/dist-packages (from llama_index.llms.gemini) (0.12.34.post1)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from llama_index.llms.gemini) (10.4.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai>=0.5.2->llama_index.llms.gemini) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama_index.llms.gemini) (1.26.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.11.15)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (4.3.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama_index.llms.gemini) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama_index.llms.gemini) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama_index.llms.gemini) (4.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama_index.llms.gemini) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.26.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama_index.llms.gemini) (4.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.16.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama_index.llms.gemini) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama_index.llms.gemini) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama_index.llms.gemini) (3.2.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (24.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama_index.llms.gemini) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.12->llama_index.llms.gemini) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AOn2afqjthOx",
        "outputId": "be76eb4b-c08e-40c9-90ab-aee23fb5e443"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Using cached llama_index-0.12.34-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Using cached llama_index_agent_openai-0.4.7-py3-none-any.whl.metadata (438 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.1 (from llama-index)\n",
            "  Using cached llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13,>=0.12.34 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.34.post1)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Using cached llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Using cached llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5,>=0.4.0 (from llama-index)\n",
            "  Using cached llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Using cached llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.76.2)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (3.11.15)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2.11.4)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.34->llama-index) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (4.13.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.20-py3-none-any.whl.metadata (914 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.21-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.34->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.34->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.34->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.34->llama-index) (0.16.0)\n",
            "Collecting llama-cloud-services>=0.6.21 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.21-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.34->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.34->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.34->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.34->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.34->llama-index) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.34->llama-index) (3.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.34->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.34->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.21->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.34->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.34-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.7-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_parse-0.6.21-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading llama_cloud_services-0.6.21-py3-none-any.whl (37 kB)\n",
            "Downloading llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, pypdf, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.1.19 llama-cloud-services-0.6.21 llama-index-0.12.34 llama-index-agent-openai-0.4.7 llama-index-cli-0.4.1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.38 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.21 pypdf-5.4.0 striprtf-0.0.26\n",
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.5.3-py3-none-any.whl.metadata (767 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.30.2)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (0.12.34.post1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-huggingface) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.13.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.15)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.1.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.11.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.17.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.20.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (4.3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.2.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-huggingface) (3.0.2)\n",
            "Downloading llama_index_embeddings_huggingface-0.5.3-py3-none-any.whl (9.0 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m757.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, llama-index-embeddings-huggingface\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed llama-index-embeddings-huggingface-0.5.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show llama-index"
      ],
      "metadata": {
        "id": "PTAEUAuHn4PE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a482073b-dc26-4053-bedf-8945b5793b1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: llama-index\n",
            "Version: 0.12.34\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: \n",
            "Author-email: Jerry Liu <jerry@llamaindex.ai>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse, nltk\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxcuxa9oY9bl",
        "outputId": "4f655f0a-c9bc-45f1-dfee-ac45e961215b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "## Retrieval augmented generation\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "from google.colab import userdata\n",
        "sec_key=userdata.get(\"GEMINI_API\")\n",
        "# Replace with your actual Gemini API key\n",
        "gemini_llm = Gemini(api_key=sec_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "collapsed": true,
        "id": "6Epryg0EbEg3",
        "outputId": "dcb5023e-cabc-48b9-bce1-e86ea55d20e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-12fb305e561d>:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  gemini_llm = Gemini(api_key=sec_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HEOyi6jjY9bz"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n",
        "documents=SimpleDirectoryReader(\"/content/GenAI---RAG-using-LangChain/pdf_data_to_read\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PGi8DV68Y9b2",
        "outputId": "744ecb75-cdeb-4608-bccb-bbe73359bf64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='09f116c6-d538-4bdf-ac7d-a6b3549cb917', embedding=None, metadata={'page_label': '1', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Introduction to Convolutional Neural Networks\\nJianxin Wu\\nLAMDA Group\\nNational Key Lab for Novel Software Technology\\nNanjing University, China\\nwujx2001@gmail.com\\nMay 1, 2017\\nContents\\n1 Introduction 2\\n2 Preliminaries 3\\n2.1 Tensor and vectorization . . . . . . . . . . . . . . . . . . . . . . . 3\\n2.2 Vector calculus and the chain rule . . . . . . . . . . . . . . . . . 4\\n3 CNN in a nutshell 5\\n3.1 The architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n3.2 The forward run . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n3.3 Stochastic gradient descent (SGD) . . . . . . . . . . . . . . . . . 6\\n3.4 Error back propagation . . . . . . . . . . . . . . . . . . . . . . . 8\\n4 Layer input, output and notations 9\\n5 The ReLU layer 10\\n6 The convolution layer 11\\n6.1 What is convolution? . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n6.2 Why to convolve? . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n6.3 Convolution as matrix product . . . . . . . . . . . . . . . . . . . 15\\n6.4 The Kronecker product . . . . . . . . . . . . . . . . . . . . . . . 17\\n6.5 Backward propagation: update the parameters . . . . . . . . . . 17\\n6.6 Even higher dimensional indicator matrices . . . . . . . . . . . . 19\\n6.7 Backward propagation: prepare supervision signal for the previ-\\nous layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n6.8 Fully connected layer as a convolution layer . . . . . . . . . . . . 22\\n7 The pooling layer 23\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b0484ca7-d3db-4149-8f32-3a7a012566da', embedding=None, metadata={'page_label': '2', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8 A case study: the VGG-16 net 25\\n8.1 VGG-Verydeep-16 . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n8.2 Receptive ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n9 Remarks 28\\nExercises 28\\n1 Introduction\\nThis is a note that describes how a Convolutional Neural Network (CNN) op-\\nerates from a mathematical perspective. This note is self-contained, and the\\nfocus is to make it comprehensible to beginners in the CNN ﬁeld.\\nThe Convolutional Neural Network (CNN) has shown excellent performance\\nin many computer vision and machine learning problems. Many solid papers\\nhave been published on this topic, and quite some high quality open source CNN\\nsoftware packages have been made available.\\nThere are also well-written CNN tutorials or CNN software manuals. How-\\never, I believe that an introductory CNN material speciﬁcally prepared for be-\\nginners is still needed. Research papers are usually very terse and lack details.\\nIt might be diﬃcult for beginners to read such papers. A tutorial targeting\\nexperienced researchers may not cover all the necessary details to understand\\nhow a CNN runs.\\nThis note tries to present a document that\\n• is self-contained. It is expected that all required mathematical background\\nknowledge are introduced in this note itself (or in other notes for this\\ncourse);\\n• has details for all the derivations. This note tries to explain all the nec-\\nessary math in details. We try not to ignore an important step in a\\nderivation. Thus, it should be possible for a beginner to follow (although\\nan expert may feel this note tautological.)\\n• ignores implementation details. The purpose is for a reader to under-\\nstand how a CNN runs at the mathematical level. We will ignore those\\nimplementation details. In CNN, making correct choices for various im-\\nplementation details is one of the keys to its high accuracy (that is, “the\\ndevil is in the details”). However, we intentionally left this part out,\\nin order for the reader to focus on the mathematics. After understand-\\ning the mathematical principles and details, it is more advantageous to\\nlearn these implementation and design details with hands-on experience\\nby playing with CNN programming.\\nCNN is useful in a lot of applications, especially in image related tasks. Ap-\\nplications of CNN include image classiﬁcation, image semantic segmentation,\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='af4aa824-25e6-4fbc-96ed-acce17a42f8f', embedding=None, metadata={'page_label': '3', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='object detection in images, etc. We will focus on image classiﬁcation (or catego-\\nrization) in this note. In image categorization, every image has a major object\\nwhich occupies a large portion of the image. An image is classiﬁed into one of\\nthe classes based on the identity of its main object, e.g., dog, airplane, bird, etc.\\n2 Preliminaries\\nWe start by a discussion of some background knowledge that are necessary in\\norder to understand how a CNN runs. One can ignore this section if he/she is\\nfamiliar with these basics.\\n2.1 Tensor and vectorization\\nEverybody is familiar with vectors and matrices. We use a symbol shown in\\nboldface to represent a vector, e.g., x ∈RD is a column vector with Delements.\\nWe use a capital letter to denote a matrix, e.g., X ∈RH×W is a matrix with\\nH rows and W columns. The vector x can also be viewed as a matrix with 1\\ncolumn and D rows.\\nThese concepts can be generalized to higher-order matrices, i.e., tensors. For\\nexample, x ∈RH×W×D is an order 3 (or third order) tensor. It contains HWD\\nelements, and each of them can be indexed by an index triplet ( i,j,d ), with\\n0 ≤i<H , 0 ≤j <W, and 0 ≤d<D . Another way to view an order 3 tensor\\nis to treat it as containing D channels of matrices. Every channel is a matrix\\nwith size H×W. The ﬁrst channel contains all the numbers in the tensor that\\nare indexed by (i,j, 0). When D= 1, an order 3 tensor reduces to a matrix.\\nWe have interacted with tensors day-to-day. A scalar value is a zeroth-order\\n(order 0) tensor; a vector is an order 1 tensor; and a matrix is a second order\\ntensor. A color image is in fact an order 3 tensor. An image with H rows and\\nW columns is a tensor with size H ×W ×3: if a color image is stored in the\\nRGB format, it has 3 channels (for R, G and B, respectively), and each channel\\nis a H×W matrix (second order tensor) that contains the R (or G, or B) values\\nof all pixels.\\nIt is beneﬁcial to represent images (or other types of raw data) as a tensor.\\nIn early computer vision and pattern recognition, a color image (which is an\\norder 3 tensor) is often converted to the gray-scale version (which is a matrix)\\nbecause we know how to handle matrices much better than tensors. The color\\ninformation is lost during this conversion. But color is very important in various\\nimage (or video) based learning and recognition problems, and we do want to\\nprocess color information in a principled way, e.g., as in CNN.\\nTensors are essential in CNN. The input, intermediate representation, and\\nparameters in a CNN are all tensors. Tensors with order higher than 3 are\\nalso widely used in a CNN. For example, we will soon see that the convolution\\nkernels in a convolution layer of a CNN form an order 4 tensor.\\nGiven a tensor, we can arrange all the numbers inside it into a long vec-\\ntor, following a pre-speciﬁed order. For example, in Matlab, the (:) operator\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a090b3b9-76a2-49ad-bc0a-30f4ee4450c7', embedding=None, metadata={'page_label': '4', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='converts a matrix into a column vector in the column-ﬁrst order. An example\\nis:\\nA=\\n[ 1 2\\n3 4\\n]\\n, A (:) = (1,3,2,4)T =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n3\\n2\\n4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (1)\\nIn mathematics, we use the notation “vec” to represent this vectorization\\noperator. That is, vec( A) = (1,3,2,4)T in the example in Equation 1. In order\\nto vectorize an order 3 tensor, we could vectorize its ﬁrst channel (which is a\\nmatrix and we already know how to vectorize it), then the second channel, . . . ,\\ntill all channels are vectorized. The vectorization of the order 3 tensor is then\\nthe concatenation of the vectorization of all the channels in this order.\\nThe vectorization of an order 3 tensor is a recursive process, which utilizes\\nthe vectorization of order 2 tensors. This recursive process can be applied to\\nvectorize an order 4 (or even higher order) tensor in the same manner.\\n2.2 Vector calculus and the chain rule\\nThe CNN learning process depends on vector calculus and the chain rule. Sup-\\npose z is a scalar (i.e., z ∈R) and y ∈RH is a vector. If z is a function of y,\\nthen the partial derivative of z with respect to y is a vector, deﬁned as\\n[∂z\\n∂y\\n]\\ni\\n= ∂z\\n∂yi\\n. (2)\\nIn other words, ∂z\\n∂y is a vector having the same size as y, and its i-th element\\nis ∂z\\n∂yi\\n. Also note that ∂z\\n∂yT =\\n(\\n∂z\\n∂y\\n)T\\n.\\nFurthermore, suppose x ∈RW is another vector, and y is a function of x.\\nThen, the partial derivative of y with respect to x is deﬁned as\\n[ ∂y\\n∂xT\\n]\\nij\\n= ∂yi\\n∂xj\\n. (3)\\nThis partial derivative is a H ×W matrix, whose entry at the intersection of\\nthe i-th row and j-th column is ∂yi\\n∂xj\\n.\\nIt is easy to see that z is a function of x in a chain-like argument: a function\\nmaps x to y, and another function maps y to z. The chain rule can be used to\\ncompute ∂z\\n∂xT , as\\n∂z\\n∂xT = ∂z\\n∂yT\\n∂y\\n∂xT . (4)\\nA sanity check for Equation 4 is to check the matrix / vector dimensions.\\nNote that ∂z\\n∂yT is a row vector withH elements, or a 1×H matrix. (Be reminded\\nthat ∂z\\n∂y is a column vector). Since ∂y\\n∂xT is an H×W matrix, the vector / matrix\\nmultiplication between them is valid, and the result should be a row vector with\\nW elements, which matches the dimensionality of ∂z\\n∂xT .\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e6d49ad1-3c8d-4df3-be30-f4d6bbcb8330', embedding=None, metadata={'page_label': '5', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='For speciﬁc rules to calculate partial derivatives of vectors and matrices,\\nplease refer to the Matrix Cookbook.\\n3 CNN in a nutshell\\nIn this section, we will see how a CNN trains and predicts in the abstract level,\\nwith the details left out for later sections.\\n3.1 The architecture\\nA CNN usually takes an order 3 tensor as its input, e.g., an image with H\\nrows, W columns, and 3 channels (R, G, B color channels). Higher order tensor\\ninputs, however, can be handled by CNN in a similar fashion. The input then\\nsequentially goes through a series of processing. One processing step is usually\\ncalled a layer, which could be a convolution layer, a pooling layer, a normal-\\nization layer, a fully connected layer, a loss layer, etc. We will introduce the\\ndetails of these layers later in this note. 1\\nFor now, let us give an abstract description of the CNN structure ﬁrst.\\nx1 −→w1 −→x2 −→···−→ xL−1 −→wL−1 −→xL −→wL −→z (5)\\nThe above Equation 5 illustrates how a CNN runs layer by layer in a forward\\npass. The input is x1, usually an image (order 3 tensor). It goes through the\\nprocessing in the ﬁrst layer, which is the ﬁrst box. We denote the parameters\\ninvolved in the ﬁrst layer’s processing collectively as a tensor w1. The output of\\nthe ﬁrst layer is x2, which also acts as the input to the second layer processing.\\nThis processing proceeds till all layers in the CNN has been ﬁnished, which\\noutputs xL. One additional layer, however, is added for backward error propa-\\ngation, a method that learns good parameter values in the CNN. Let’s suppose\\nthe problem at hand is an image classiﬁcation problem with C classes. A com-\\nmonly used strategy is to output xL as a C dimensional vector, whose i-th\\nentry encodes the prediction (posterior probability of x1 comes from the i-th\\nclass). To make xL a probability mass function, we can set the processing in the\\n(L−1)-th layer as a softmax transformation of xL−1 (cf. the distance metric\\nand data transformation note). In other applications, the output xL may have\\nother forms and interpretations.\\nThe last layer is a loss layer. Let us suppose t is the corresponding target\\n(ground-truth) value for the input x1, then a cost or loss function can be used\\nto measure the discrepancy between the CNN prediction xL and the target t.\\nFor example, a simple loss function could be\\nz= 1\\n2∥t −xL∥2 , (6)\\n1We will give detailed introductions to three types of layers: convolution, pooling, and\\nReLU, which are the key parts of almost all CNN models. Proper normalization, e.g., batch\\nnormalization or cross-layer normalization is important in the optimization process for learning\\ngood parameters in a CNN. I may add these contents in the next update.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a682525e-5a6f-4c3a-8db2-e3168755be36', embedding=None, metadata={'page_label': '6', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='although more complex loss functions are usually used. This squared ℓ2 loss can\\nbe used in a regression problem. In a classiﬁcation problem, the cross entropy\\nloss is often used. The ground-truth in a classiﬁcation problem is a categorical\\nvariable t. We ﬁrst convert the categorical variable tto a C dimensional vector\\nt (cf. the distance metric and data transformation note). Now both t and xL\\nare probability mass functions, and the cross entropy loss measures the distance\\nbetween them. Hence, we can minimize the cross entropy (cf. the information\\ntheory note.) Equation 5 explicitly models the loss function as a loss layer,\\nwhose processing is modeled as a box with parameters wL.\\nNote that some layers may not have any parameters, that is, wi may be\\nempty for some i. The softmax layer is one such example.\\n3.2 The forward run\\nSuppose all the parameters of a CNN model w1,..., wL−1 have been learned,\\nthen we are ready to use this model for prediction. Prediction only involves run-\\nning the CNN model forward, i.e., in the direction of the arrows in Equation 5.\\nLet’s take the image classiﬁcation problem as an example. Starting from\\nthe input x1, we make it pass the processing of the ﬁrst layer (the box with\\nparameters w1), and get x2. In turn, x2 is passed into the second layer, etc.\\nFinally, we achieve xL ∈RC, which estimates the posterior probabilities of x1\\nbelonging to the C categories. We can output the CNN prediction as\\narg max\\ni\\nxL\\ni . (7)\\nNote that the loss layer is not needed in prediction. It is only useful when\\nwe try to learn CNN parameters using a set of training examples. Now, the\\nproblem is: how do we learn the model parameters?\\n3.3 Stochastic gradient descent (SGD)\\nAs in many other learning systems, the parameters of a CNN model are opti-\\nmized to minimize the loss z, i.e., we want the prediction of a CNN model to\\nmatch the ground-truth labels.\\nLet’s suppose one training example x1 is given for training such parameters.\\nThe training process involves running the CNN network in both directions. We\\nﬁrst run the network in the forward pass to get xL to achieve a prediction using\\nthe current CNN parameters. Instead of outputting a prediction, we need to\\ncompare the prediction with the target t corresponding to x1, that is, continue\\nrunning the forward pass till the last loss layer. Finally, we achieve a loss z.\\nThe loss z is then a supervision signal, guiding how the parameters of the\\nmodel should be modiﬁed (updated). And the SGD way of modifying the pa-\\nrameters is\\nwi ←−wi −η ∂z\\n∂wi . (8)\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7013ce83-6b53-43c6-9c55-4fe028369332', embedding=None, metadata={'page_label': '7', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1: Illustration of the gradient descent method.\\nA cautious note about the notation . In most CNN materials, a superscript\\nindicates the “time” (e.g., training epochs). But in this note, we use the su-\\nperscript to denote the layer index. Please do not get confused. We do not\\nuse an additional index variable to represent time. In Equation 8, the ←−sign\\nimplicitly indicates that the parameters wi (of the i-layer) are updated from\\ntime t to t+ 1. If a time index t is explicitly used, this equation will look like\\n(\\nwi)t+1\\n=\\n(\\nwi)t\\n−η ∂z\\n∂(wi)t . (9)\\nIn Equation 8, the partial derivative ∂z\\n∂wi measures the rate of increase of z\\nwith respect to the changes in diﬀerent dimensions of wi. This partial deriva-\\ntive vector is called the gradient in mathematical optimization. Hence, in a\\nsmall local region around the current value of wi, to move wi in the direction\\ndetermined by the gradient will increase the objective value z. In order to min-\\nimize the loss function, we should update wi along the opposite direction of the\\ngradient. This updating rule is called the gradient descent. Gradient descent is\\nillustrated in Figure 1, in which the gradient is denoted by g.\\nIf we move too far in the negative gradient direction, however, the loss\\nfunction may increase. Hence, in every update we only change the parameters\\nby a small proportion of the negative gradient, controlled by η (the learning\\nrate). η >0 is usually set to a small number (e.g., η = 0.001). One update\\nbased on x1 will make the loss smaller for this particular training example if the\\nlearning rate is not too large. However, it is very possible that it will make the\\nloss of some other training examples become larger. Hence, we need to update\\nthe parameters using all training examples. When all training examples have\\nbeen used to update the parameters, we say one epoch has been processed. One\\nepoch will in general reduce the average loss on the training set until the learning\\nsystem overﬁts the training data. Hence, we can repeat the gradient descent\\nupdating epochs and terminate at some point to obtain the CNN parameters\\n(e.g., we can terminate when the average loss on a validation set increases).\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='33903068-eca9-435a-8cfe-3a764795f0ac', embedding=None, metadata={'page_label': '8', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Gradient descent may seem simple in its math form (Equation 8), but it is\\na very tricky operation in practice. For example, if we update the parameters\\nusing only gradient calculated from only one training example, we will observe\\nan unstable loss function: the average loss of all training examples will bounce\\nup and down at very high frequency. This is because the gradient is estimated\\nusing only one training example instead of the entire training set. Updating\\nthe parameters using the gradient estimated from a (usually) small subset of\\ntraining examples is called the stochastic gradient descent . Contrary to single\\nexample based SGD, we can compute the gradient using all training examples\\nand then update the parameters. However, this batch processing requires a lot\\nof computations because the parameters are updated only once in an epoch, and\\nis hence impractical, especially when the number of training examples is large.\\nA compromise is to use a mini-batch of training examples, to compute the\\ngradient using this mini-batch, and to update the parameters correspondingly.\\nFor example, we can set 32 or 64 examples as a mini-batch. Stochastic gradient\\ndescent (SGD) (using the mini-batch strategy) is the mainstream method to\\nlearn a CNN’s parameters. We also want to note that when mini-batch is used,\\nthe input of the CNN becomes an order 4 tensor, e.g., H ×W ×3 ×32 if the\\nmini-batch size is 32.\\nA new problem now becomes apparent: how to compute the gradient, which\\nseems a very complex task?\\n3.4 Error back propagation\\nThe last layer’s partial derivatives are easy to compute. BecausexL is connected\\nto z directly under the control of parameters wL, it is easy to compute ∂z\\n∂wL .\\nThis step is only needed when wL is not empty. In the same spirit, it is also\\neasy to compute ∂z\\n∂xL . For example, if the squared ℓ2 loss is used, we have an\\nempty ∂z\\n∂wL , and ∂z\\n∂xL = xL −t.\\nIn fact, for every layer, we compute two sets of gradients: the partial deriva-\\ntives of z with respect to the layer parameters wi, and that layer’s input xi.\\n• The term ∂z\\n∂wi , as seen in Equation 8, can be used to update the current\\n(i-th) layer’s parameters;\\n• The term ∂z\\n∂xi can be used to update parameters backwards, e.g., to the\\n(i −1)-th layer. An intuitive explanation is: xi is the output of the\\n(i−1)-th layer and ∂z\\n∂xi is how xi should be changed to reduce the loss\\nfunction. Hence, we could view ∂z\\n∂xi as the part of the “error” supervision\\ninformation propagated from z backward till the current layer, in a layer\\nby layer fashion. Thus, we can continue the back propagation process,\\nand use ∂z\\n∂xi to propagate the errors backward to the ( i−1)-th layer.\\nThis layer-by-layer backward updating procedure makes learning a CNN much\\neasier.\\nLet’s take the i-th layer as an example. When we are updating the i-th layer,\\nthe back propagation process for the ( i+ 1)-th layer must have been ﬁnished.\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2a4aadb4-955f-4f4e-ab36-9866829bb85f', embedding=None, metadata={'page_label': '9', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='That is, we already computed the terms ∂z\\n∂wi+1 and ∂z\\n∂xi+1 . Both are stored in\\nmemory and ready for use.\\nNow our task is to compute ∂z\\n∂wi and ∂z\\n∂xi . Using the chain rule, we have\\n∂z\\n∂(vec(wi)T ) = ∂z\\n∂(vec(xi+1)T )\\n∂vec(xi+1)\\n∂(vec(wi)T ) , (10)\\n∂z\\n∂(vec(xi)T ) = ∂z\\n∂(vec(xi+1)T )\\n∂vec(xi+1)\\n∂(vec(xi)T ) . (11)\\nSince ∂z\\n∂xi+1 is already computed and stored in memory, it requires just a\\nmatrix reshaping operation (vec) and an additional transpose operation to get\\n∂z\\n∂(vec(xi+1)T ) , which is the ﬁrst term in the right hand side (RHS) of both equa-\\ntions. So long as we can compute ∂ vec(xi+1)\\n∂(vec(wi)T ) and ∂ vec(xi+1)\\n∂(vec(xi)T ) , we can easily get\\nwhat we want (the left hand side of both equations).\\n∂ vec(xi+1)\\n∂(vec(wi)T ) and ∂ vec(xi+1)\\n∂(vec(xi)T ) are much easier to compute than directly comput-\\ning ∂z\\n∂(vec(wi)T ) and ∂z\\n∂(vec(xi)T ) , because xi is directly related to xi+1, through\\na function with parameters wi. The details of these partial derivatives will be\\ndiscussed in the following sections.\\n4 Layer input, output and notations\\nNow that the CNN architecture is clear, we will discuss in detail the diﬀerent\\ntypes of layers, starting from the ReLU layer, which is the simplest layer among\\nthose we discuss in this note. But before we start, we need to further reﬁne our\\nnotations.\\nSuppose we are considering the l-th layer, whose inputs form an order 3\\ntensor xl with xl ∈RHl×Wl×Dl\\n. Thus, we need a triplet index set ( il,jl,dl) to\\nlocate any speciﬁc element in xl. The triplet ( il,jl,dl) refers to one element in\\nxl, which is in the dl-th channel, and at spatial location (il,jl) (at the il-th row,\\nand jl-th column). In actual CNN learning, the mini-batch strategy is usually\\nused. In that case, xl becomes an order 4 tensor in RHl×Wl×Dl×N where N\\nis the mini-batch size. For simplicity we assume that N = 1 in this note. The\\nresults in this section, however, are easy to adopt to mini-batch versions.\\nIn order to simplify the notations which will appear later, we follow the\\nzero-based indexing convention, which speciﬁes that 0 ≤il <H l, 0 ≤jl <W l,\\nand 0 ≤dl <Dl.\\nIn the l-th layer, a function will transform the input xl to an output y,\\nwhich is also the input to the next layer. Thus, we notice that y and xl+1 in\\nfact refers to the same object, and it is very helpful to keep this point in mind.\\nWe assume the output has sizeHl+1×Wl+1×Dl+1, and an element in the output\\nis indexed by a triplet ( il+1,jl+1,dl+1), 0 ≤il+1 < Hl+1, 0 ≤jl+1 < Wl+1,\\n0 ≤dl+1 <Dl+1.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d9a3d2a-6022-45b7-969c-33450e305afa', embedding=None, metadata={'page_label': '10', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5 The ReLU layer\\nA ReLU layer does not change the size of the input, that is, xl and y share the\\nsame size. In fact, the Rectiﬁed Linear Unit (hence the name ReLU) can be\\nregarded as a truncation performed individually for every element in the input:\\nyi,j,d = max{0,xl\\ni,j,d}, (12)\\nwith 0 ≤i<H l = Hl+1, 0 ≤j <Wl = Wl+1, and 0 ≤d<D l = Dl+1.\\nThere is no parameter inside a ReLU layer, hence no need for parameter\\nlearning in this layer.\\nBased on Equation 12, it is obvious that\\ndyi,j,d\\ndxl\\ni,j,d\\n=\\nq\\nxl\\ni,j,d >0\\ny\\n, (13)\\nwhere J·K is the indicator function, being 1 if its argument is true, and 0 other-\\nwise.\\nHence, we have\\n[∂z\\n∂xl\\n]\\ni,j,d\\n=\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\n[∂z\\n∂y\\n]\\ni,j,d\\nif xl\\ni,j,d >0\\n0 otherwise\\n. (14)\\nNote that y is an alias for xl+1.\\nStrictly speaking, the function max(0,x) is not diﬀerentiable at x= 0, hence\\nEquation 13 is a little bit problematic in theory. In practice, it is not an issue\\nand ReLU is safe to use.\\nThe purpose of ReLU is to increase the nonlinearity of the CNN. Since the\\nsemantic information in an image (e.g., a person and a Husky dog sitting next\\nto each other on a bench in a garden) is obviously a highly nonlinear mapping\\nof pixel values in the input, we want the mapping from CNN input to its output\\nalso be highly nonlinear. The ReLU function, although simple, is a nonlinear\\nfunction, as illustrated in Figure 2.\\nIf we treat xl\\ni,j,d as one of the HlWlDl features extracted by CNN layers 1\\nto l−1, which can be positive, zero or negative. For example, xl\\ni,j,d may be\\npositive if a region inside the input image has certain patterns (like a dog’s head\\nor a cat’s head or some other patterns similar to that); and xl\\ni,j,d is negative or\\nzero when that region does not exhibit these patterns. The ReLU layer will set\\nall negative values to 0, which means that yl\\ni,j,d will be activated only for images\\npossessing these patterns at that particular region. Intuitively, this property is\\nuseful for recognizing complex patterns and objects. For example, it is only\\na weak evidence to support “the input image contains a cat” if a feature is\\nactivated and that feature’s pattern looks like cat’s head. However, if we ﬁnd\\nmany activated features after the ReLU layer whose target patterns correspond\\nto cat’s head, torso, fur, legs, etc., we have higher conﬁdence (at layer l+ 1) to\\nsay that a cat probably exists in the input image.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='823d7a32-0233-40ae-a8e3-370d0a18aaf8', embedding=None, metadata={'page_label': '11', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 2: The ReLU function.\\nOther nonlinear transformations have been used in the neural network re-\\nsearch to produce nonlinearity, for example, the logistic sigmoid function y =\\nσ(x) = 1\\n1+exp(−x) . However, logistic sigmoid works signiﬁcantly worse than\\nReLU in CNN learning. Note that 0 <y <1 if a sigmoid function is used, and\\ndy\\ndx = y(1 −y), we have dy\\ndx ≤1\\n4 . Hence, in the error back propagation process,\\nthe gradient ∂z\\n∂x = ∂z\\n∂y\\ndy\\ndx will have much smaller magnitude than ∂z\\n∂y (at most\\n1\\n4 ). In other words, a sigmoid layer will cause the magnitude of the gradient\\nto signiﬁcantly reduce, and after several sigmoid layers, the gradient will vanish\\n(i.e., all its components will be close to 0). A vanishing gradient makes gradient\\nbased learning (e.g., SGD) very diﬃcult.\\nOn the other hand, the ReLU layer sets the gradient of some features in the\\nl-th layer to 0, but these features are not activated (i.e., we are not interested\\nin them). For those activated features, the gradient is back propagated without\\nany change, which is beneﬁcial for SGD learning. The introduction of ReLU to\\nreplace sigmoid is an important change in CNN, which signiﬁcantly reduces the\\ndiﬃculty in learning CNN parameters and improves its accuracy. There are also\\nmore complex variants of ReLU, for example, parametric ReLU and exponential\\nlinear unit.\\n6 The convolution layer\\nNext, we turn to the convolution layer, which is the most involved one among\\nthose we discuss in this note.\\n6.1 What is convolution?\\nLet us start by convolving a matrix with one single convolution kernel. Suppose\\nthe input image is 3 ×4 and the convolution kernel size is 2 ×2, as illustrated\\nin Figure 3.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='75f7c39d-c0ff-44f2-bfcf-f5130a29f719', embedding=None, metadata={'page_label': '12', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='/g1005 /g1005\\n/g1005 /g1005\\n(a) A 2 × 2 kernel\\n/g1005 /g1006 /g1007 /g1005\\n/g1008 /g1009 /g1010 /g1005\\n/g1011 /g1012 /g1013 /g1005\\n/g1005/g1006 /g1005/g1010 /g1005/g1005\\n/g1006/g1008 /g1006/g1012 /g1005/g1011 (b) The convolution input and output\\nFigure 3: Illustration of the convolution operation.\\nIf we overlap the convolution kernel on top of the input image, we can\\ncompute the product between the numbers at the same location in the kernel\\nand the input, and we get a single number by summing these products together.\\nFor example, if we overlap the kernel with the top left region in the input, the\\nconvolution result at that spatial location is: 1 ×1 + 1×4 + 1×2 + 1×5 = 12.\\nWe then move the kernel down by one pixel and get the next convolution result\\nas 1×4+1 ×7+1 ×5+1 ×8 = 24. We keep move the kernel down till it reaches\\nthe bottom border of the input matrix (image). Then, we return the kernel to\\nthe top, and move the kernel to its right by one element (pixel). We repeat the\\nconvolution for every possible pixel location until we have moved the kernel to\\nthe bottom right corner of the input image, as shown in Figure 3.\\nFor order 3 tensors, the convolution operation is deﬁned similarly. Suppose\\nthe input in the l-th layer is an order 3 tensor with size Hl ×Wl ×Dl. A\\nconvolution kernel is also an order 3 tensor with size H ×W ×Dl. When we\\noverlap the kernel on top of the input tensor at the spatial location (0 ,0,0),\\nwe compute the products of corresponding elements in all the Dl channels and\\nsum the HWDl products to get the convolution result at this spatial location.\\nThen, we move the kernel from top to bottom and from left to right to complete\\nthe convolution.\\nIn a convolution layer, multiple convolution kernels are usually used. As-\\nsuming D kernels are used and each kernel is of spatial span H×W, we denote\\nall the kernels as f. f is an order 4 tensor in RH×W×Dl×D. Similarly, we use\\nindex variables 0 ≤i<H , 0 ≤j <W, 0 ≤dl <Dl and 0 ≤d<D to pinpoint\\na speciﬁc element in the kernels. Also note that the set of kernels f refers to\\nthe same object as the notation wl in Equation 5. We change the notation a\\nbit to make the derivation a little bit simpler. It is also clear that even if the\\nmini-batch strategy is used, the kernels remain unchanged.\\nAs shown in Figure 3, the spatial extent of the output is smaller than that\\nof the input so long as the convolution kernel is larger than 1 ×1. Sometimes\\nwe need the input and output images to have the same height and width, and a\\nsimple padding trick can be used. If the input is Hl×Wl×Dl and the kernel size\\nis H×W×Dl×D, the convolution result has size (Hl−H+1)×(Wl−W+1)×D.\\nFor every channel of the input, if wepad (i.e., insert) ⌊H−1\\n2 ⌋rows above the ﬁrst\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='18401713-9611-4320-9ae9-51a3c99ff5b6', embedding=None, metadata={'page_label': '13', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='row and ⌊H\\n2 ⌋rows below the last row, and pad ⌊W−1\\n2 ⌋columns to the left of\\nthe ﬁrst column and ⌊W\\n2 ⌋columns to the right of the last column of the input,\\nthe convolution output will be Hl ×Wl ×Din size, i.e., having the same spatial\\nextent as the input. ⌊·⌋is the ﬂoor functions. Elements of the padded rows and\\ncolumns are usually set to 0, but other values are also possible.\\nStride is another important concept in convolution. In Figure 3, we convolve\\nthe kernel with the input at every possible spatial location, which corresponds\\nto the stride s = 1. However, if s >1, every movement of the kernel skip\\ns−1 pixel locations (i.e., the convolution is performed once every spixels both\\nhorizontally and vertically).\\nIn this section, we consider the simple case when the stride is 1 and no\\npadding is used. Hence, we have y (or xl+1) in RHl+1×Wl+1×Dl+1\\n, with Hl+1 =\\nHl −H+ 1, Wl+1 = Wl −W + 1, and Dl+1 = D.\\nIn precise mathematics, the convolution procedure can be expressed as an\\nequation:\\nyil+1,jl+1,d =\\nH∑\\ni=0\\nW∑\\nj=0\\nDl\\n∑\\ndl=0\\nfi,j,dl,d ×xl\\nil+1+i,jl+1+j,dl . (15)\\nEquation 15 is repeated for all 0 ≤d≤D= Dl+1, and for any spatial location\\n(il+1,jl+1) satisfying 0 ≤il+1 <H l −H+ 1 = Hl+1,0 ≤jl+1 <W l −W + 1 =\\nWl+1. In this equation, xl\\nil+1+i,jl+1+j,dl refers to the element of xl indexed by\\nthe triplet (il+1 + i,jl+1 + j,dl).\\nA bias term bd is usually added to yil+1,jl+1,d. We omit this term in this\\nnote for clearer presentation.\\n6.2 Why to convolve?\\nFigure 4 shows a color input image (4a) and its convolution results using two\\ndiﬀerent kernels (4b and 4c). A 3 ×3 convolution matrix K =\\n[ 1 2 1\\n0 0 0\\n−1 −2 −1\\n]\\nis\\nused. The convolution kernel should be of size 3 ×3 ×3, in which we set every\\nchannel to K. When there is a horizontal edge at location ( x,y) (i.e., when the\\npixels at spatial location ( x+ 1,y) and (x−1,y) diﬀer by a large amount), we\\nexpect the convolution result to have high magnitude. As shown in Figure 4b,\\nthe convolution results indeed highlight the horizontal edges. When we set every\\nchannel of the convolution kernel to KT (the transpose of K), the convolution\\nresult ampliﬁes vertical edges, as shown in Figure 4c. The matrix (or ﬁlter) K\\nand KT are called the Sobel operators. 2\\nIf we add a bias term to the convolution operation, we can make the convo-\\nlution result positive at horizontal (vertical) edges in a certain direction (e.g.,\\na horizontal edge with the pixels above it brighter than the pixels below it),\\nand negative at other locations. If the next layer is a ReLU layer, the output\\nof the next layer in fact deﬁnes many “edge detection features”, which activate\\n2The Sobel operator is named after Irwin Sobel, an American researcher in digital image\\nprocessing.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6c0c3417-d935-42f0-8f5e-dca46833f0a0', embedding=None, metadata={'page_label': '14', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='(a) Lenna\\n (b) Horizontal edge\\n (c) Vertical edge\\nFigure 4: The Lenna image and the eﬀect of diﬀerent convolution kernels.\\nonly at horizontal or vertical edges in certain directions. If we replace the So-\\nbel kernel by other kernels (e.g., those learned by SGD), we can learn features\\nthat activate for edges with diﬀerent angles. When we move further down in the\\ndeep network, subsequent layers can learn to activate only for speciﬁc (but more\\ncomplex) patterns, e.g., groups of edges that form a particular shape. These\\nmore complex patterns will be further assembled by deeper layers to activate for\\nsemantically meaningful object parts or even a particular type of object, e.g.,\\ndog, cat, tree, beach, etc.\\nOne more beneﬁt of the convolution layer is that all spatial locations share\\nthe same convolution kernel, which greatly reduces the number of parameters\\nneeded for a convolution layer. For example, if multiple dogs appear in an input\\nimage, the same “dog-head-like pattern” feature will be activated at multiple\\nlocations, corresponding to heads of diﬀerent dogs.\\nIn a deep neural network setup, convolution also encourages parameter shar-\\ning. For example, suppose “dog-head-like pattern” and “cat-head-like pattern”\\nare two features learned by a deep convolutional network. The CNN does not\\nneed to devote two sets of disjoint parameters (e.g., convolution kernels in mul-\\ntiple layers) for them. The CNN’s bottom layers can learn “eye-like pattern”\\nand “animal-fur-texture pattern”, which are shared by both these more abstract\\nfeatures. In short, the combination of convolution kernels and deep and hier-\\narchical structures are very eﬀective in learning good representations (features)\\nfrom images for visual recognition tasks.\\nWe want to add a note here. Although we have used phrases such as “dog-\\nhead-like pattern”, the representation or feature learned by a CNN may not\\ncorrespond exactly to semantic concepts such as “dog’s head”. A CNN feature\\nmay activate frequently for dogs’ heads and often be deactivated for other types\\nof patterns. However, there are also possible false activations at other locations,\\nand possible deactivations at dogs’ heads.\\nIn fact, a key concept in CNN (or more generally deep learning) isdistributed\\nrepresentation. For example, suppose our task is to recognize N diﬀerent types\\nof objects and a CNN extracts M features from any input image. It is most\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b09eeabf-6d89-4a5a-a585-e4351130d8ba', embedding=None, metadata={'page_label': '15', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='likely that any one of the M features is useful for recognizing all N object\\ncategories; and to recognize one object type requires the joint eﬀort of all M\\nfeatures.\\n6.3 Convolution as matrix product\\nEquation 15 seems pretty complex. There is a way to expand xl and simplify\\nthe convolution as a matrix product.\\nLet’s consider a special case with Dl = D = 1, H = W = 2, and Hl = 3,\\nWl = 4. That is, we consider convolving a small single channel 3 ×4 matrix (or\\nimage) with one 2 ×2 ﬁlter. Using the example in Figure 3, we have\\n\\uf8ee\\n\\uf8f0\\n1 2 3 1\\n4 5 6 1\\n7 8 9 1\\n\\uf8f9\\n\\uf8fb∗\\n[\\n1 1\\n1 1\\n]\\n=\\n[\\n12 16 11\\n24 28 17\\n]\\n, (16)\\nwhere the ﬁrst matrix is denoted as A, and ∗is the convolution operator.\\nNow let’s run a Matlab command B=im2col(A,[2 2]), we arrive at a B\\nmatrix that is an expanded version of A:\\nB =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1 4 2 5 3 6\\n4 7 5 8 6 9\\n2 5 3 6 1 1\\n5 8 6 9 1 1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.\\nIt is obvious that the ﬁrst column of B corresponds to the ﬁrst 2 ×2 region\\nin A, in a column-ﬁrst order, corresponding to ( il+1,jl+1) = (0 ,0). Similarly,\\nthe second to last column in Bcorrespond to regions in Awith (il+1,jl+1) being\\n(1,0), (0,1), (1,1), (0,2) and (1 ,2), respectively. That is, the Matlab im2col\\nfunction explicitly expands the required elements for performing each individual\\nconvolution into a column in the matrix B. The transpose of B, BT , is called\\nthe im2row expansion of A.\\nNow, if we vectorize the convolution kernel itself into a vector (in the same\\ncolumn-ﬁrst order) (1,1,1,1)T , we ﬁnd that3\\nBT\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n12\\n24\\n16\\n28\\n11\\n17\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n. (17)\\n3The notation and presentation of this note is heavily aﬀected by the MatConvNet software\\npackage’s manual (http://arxiv.org/abs/1412.4564, which is Matlab based). The transpose\\nof an im2col expansion is equivalent to an im2row expansion, in which the numbers involved\\nin one convolution is one row in the im2row expanded matrix. The derivation in this section\\nuses im2row, complying with the implementation in MatConvNet. Caﬀe, a widely used CNN\\nsoftware package ( http://caffe.berkeleyvision.org/, which is C++ based) uses im2col.\\nThese formulations are mathematically equivalent to each other.\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='25108a90-e3c8-4de8-a40c-924720e89f63', embedding=None, metadata={'page_label': '16', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='If we reshape this resulting vector in Equation 17 properly, we get the exact\\nconvolution result matrix in Equation 16. That is, the convolution operator is\\na linear one. We can multiply the expanded input matrix and the vectorized\\nﬁlter to get a result vector, and by reshaping this vector properly we get the\\ncorrect convolution results.\\nWe can generalize this idea to more complex situations and formalize them.\\nIf Dl > 1 (that is, the input xl has more than one channels), the expansion\\noperator could ﬁrst expand the ﬁrst channel of xl, then the second, . . . , till all\\nDl channels are expanded. The expanded channels will be stacked together,\\nthat is, one row in the im2row expansion will have H×W×Dl elements, rather\\nthan H×W.\\nMore formally, suppose xl is a third order tensor in RHl×Wl×Dl\\n, with one\\nelement in xl being indexed by a triplet ( il,jl,dl). We also consider a set of\\nconvolution kernels f, whose spatial extent are all H×W. Then, the expansion\\noperator (im2row) converts xl into a matrix φ(xl). We use two indexes ( p,q)\\nto index an element in this matrix. The expansion operator copies the element\\nat (il,jl,dl) in xl to the (p,q)-th entry in φ(xl).\\nFrom the description of the expansion process, it is clear that given a ﬁxed\\n(p,q), we can calculate its corresponding ( il,jl,dl) triplet, because obviously\\np= il+1 + (Hl −H+ 1) ×jl+1 , (18)\\nq= i+ H×j+ H×W ×dl , (19)\\nil = il+1 + i, (20)\\njl = jl+1 + j. (21)\\nIn Equation 19, dividing qby HW and take the integer part of the quotient,\\nwe can determine which channel (dl) does it belong to. Similarly, we can get the\\noﬀsets inside the convolution kernel as (i,j), in which 0 ≤i<H and 0 ≤j <W.\\nq completely determines one speciﬁc location inside the convolution kernel by\\nthe triplet (i,j,d l).\\nNote that the convolution result is xl+1, whose spatial extent is Hl+1 =\\nHl −H + 1 and Wl+1 = Wl −W + 1. Thus, in Equation 18, the remainder\\nand quotient of dividing p by Hl+1 = Hl −H+ 1 will give us the oﬀset in the\\nconvolved result (il+1,jl+1), or, the top-left spatial location of the region in xl\\n(which is to be convolved with the kernel).\\nBased on the deﬁnition of convolution, it is clear that we can use Equa-\\ntions 20 and 21 to ﬁnd the oﬀset in the inputxl as il = il+1 +iand jl = jl+1 +j.\\nThat is, the mapping from ( p,q) to ( il,jl,dl) is one-to-one. However, we want\\nto emphasize that the reverse mapping from (il,jl,dl) to (p,q) is one-to-many, a\\nfact that is useful in deriving the back propagation rules in a convolution layer.\\nNow we use the standard vec operator to convert the set of convolution\\nkernels f (order 4 tensor) into a matrix. Let’s start from one kernel, which\\ncan be vectorized into a vector in RHWD l\\n. Thus, all convolution kernels can\\nbe reshaped into a matrix with HWDl rows and D columns (remember that\\nDl+1 = D.) Let’s call this matrix F.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7c16b93e-dda9-459a-8dab-a479c1c3e26d', embedding=None, metadata={'page_label': '17', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Finally, with all these notations, we have a beautiful equation to calculate\\nconvolution results (cf. Equation 17, in which φ(xl) is BT ):\\nvec(y) = vec(xl+1) = vec\\n(\\nφ(xl)F\\n)\\n. (22)\\nNote that vec( y) ∈RHl+1Wl+1D, φ(xl) ∈R(Hl+1Wl+1)×(HWD l), and F ∈\\nR(HWD l)×D. The matrix multiplication φ(xl)F results in a matrix of size\\n(Hl+1Wl+1) ×D. The vectorization of this resultant matrix generates a vector\\nin RHl+1Wl+1D, which matches the dimensionality of vec( y).\\n6.4 The Kronecker product\\nA short detour to the Kronecker product is needed to compute the derivatives.\\nGiven two matrices A∈Rm×n and B ∈Rp×q, the Kronecker product A⊗B\\nis a mp×nq matrix, deﬁned as a block matrix\\nA⊗B =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\na11B ··· a1nB\\n... ... ...\\nam1B ··· amnB\\n\\uf8f9\\n\\uf8fa\\uf8fb. (23)\\nThe Kronecker product has the following properties that will be useful for\\nus:\\n(A⊗B)T = AT ⊗BT , (24)\\nvec(AXB) = (BT ⊗A) vec(X) , (25)\\nfor matrices A, X, and B with proper dimensions (e.g., when the matrix mul-\\ntiplication AXB is deﬁned.) Note that Equation 25 can be utilized from both\\ndirections.\\nWith the help of ⊗, we can write down\\nvec(y) = vec\\n(\\nφ(xl)FI\\n)\\n=\\n(\\nI⊗φ(xl)\\n)\\nvec(F) , (26)\\nvec(y) = vec\\n(\\nIφ(xl)F\\n)\\n= (FT ⊗I) vec(φ(xl)) , (27)\\nwhere I is an identity matrix of proper size. In Equation 26, the size of I is\\ndetermined by the number of columns in F, hence I ∈RD×D in Equation 26.\\nSimilarly, in Equation 27, I ∈R(Hl+1Wl+1)×(Hl+1Wl+1).\\nThe derivation for gradient computation rules in a convolution layer involves\\nmany variables and notations. We summarize the variables used in this deriva-\\ntion in Table 1. Note that some of these notations have not been introduced\\nyet.\\n6.5 Backward propagation: update the parameters\\nAs previously mentioned, we need to compute two derivatives: ∂z\\n∂ vec(xl) and\\n∂z\\n∂ vec(F) , where the ﬁrst term ∂z\\n∂ vec(xl) will be used for backward propagation\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='971ff347-7cf1-48aa-b7d1-f75d7a235535', embedding=None, metadata={'page_label': '18', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Variables, their sizes and meanings. Note that “alias” means a variable\\nhas a diﬀerent name or can be reshaped into another form.\\nAlias Size & Meaning\\nX xl HlWl × Dl, the input tensor\\nF f, wl HWDl × D, D kernels, eachH× W and Dl channels\\nY y, xl+1 Hl+1Wl+1 × Dl+1, the output,Dl+1 = D\\nφ(xl) Hl+1Wl+1 × HWDl, theim2row expansion ofxl\\nM Hl+1Wl+1HWDl × HlWlDl, the indicator matrix forφ(xl)\\n∂z\\n∂Y\\n∂z\\n∂ vec(y) Hl+1Wl+1 × Dl+1, gradient fory\\n∂z\\n∂F\\n∂z\\n∂ vec(f) HWDl × D, gradient to update the convolution kernels\\n∂z\\n∂X\\n∂z\\n∂ vec(xl) HlWl × Dl, gradient forxl, useful for back propagation\\nto the previous (( l−1)-th) layer, and the second term will determine how the\\nparameters of the current ( l-th) layer will be updated. A friendly reminder\\nis to remember that f, F and wi refer to the same thing (modulo reshaping\\nof the vector or matrix or tensor). Similarly, we can reshape y into a matrix\\nY ∈R(Hl+1Wl+1)×D, then y, Y and xl+1 refer to the same object (again modulo\\nreshaping).\\nFrom the chain rule (Equation 10), it is easy to compute ∂z\\n∂ vec(F) as\\n∂z\\n∂(vec(F))T = ∂z\\n∂(vec(Y)T )\\n∂vec(y)\\n∂(vec(F)T ) . (28)\\nThe ﬁrst term in the RHS is already computed in the (l+1)-th layer as (equiva-\\nlently) ∂z\\n∂(vec(xl+1))T . The second term, based on Equation 26, is pretty straight-\\nforward:\\n∂vec(y)\\n∂(vec(F)T ) = ∂\\n((\\nI⊗φ(xl)\\n)\\nvec(F)\\n)\\n∂(vec(F)T ) = I⊗φ(xl) . (29)\\nNote that we have used the fact ∂XaT\\n∂a = X or ∂Xa\\n∂aT = X so long as the matrix\\nmultiplications are well deﬁned. This equation leads to\\n∂z\\n∂(vec(F))T = ∂z\\n∂(vec(y)T )(I⊗φ(xl)) . (30)\\nMaking a transpose, we get\\n∂z\\n∂vec(F) =\\n(\\nI⊗φ(xl)\\n)T ∂z\\n∂vec(y) (31)\\n=\\n(\\nI⊗φ(xl)T )\\nvec\\n(∂z\\n∂Y\\n)\\n(32)\\n= vec\\n(\\nφ(xl)T ∂z\\n∂Y I\\n)\\n(33)\\n= vec\\n(\\nφ(xl)T ∂z\\n∂Y\\n)\\n. (34)\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='620ada48-bb5e-4f42-8dec-8f88318b8705', embedding=None, metadata={'page_label': '19', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Note that both Equation 25 (from RHS to LHS) and Equation 24 are used in\\nthe above derivation.\\nThus, we conclude that\\n∂z\\n∂F = φ(xl)T ∂z\\n∂Y , (35)\\nwhich is a simple rule to update the parameters in the l-th layer: the gradient\\nwith respect to the convolution parameters is the product between φ(xl)T (the\\nim2col expansion) and ∂z\\n∂Y (the supervision signal transferred from the (l+1)-th\\nlayer).\\n6.6 Even higher dimensional indicator matrices\\nThe function φ(·) has been very useful in our analysis. It is pretty high dimen-\\nsional, e.g., φ(xl) has Hl+1Wl+1HWDl elements. From the above, we know\\nthat an element in φ(xl) is indexed by a pair p and q.\\nA quick recap about φ(xl): 1) from q we can determine dl, which channel\\nof the convolution kernel is used; and can also determine i and j, the spatial\\noﬀsets inside the kernel; 2) from p we can determine il+1 and jl+1, the spatial\\noﬀsets inside the convolved result xl+1; and, 3) the spatial oﬀsets in the input\\nxl can be determined as il = il+1 + i and jl = jl+1 + j.\\nThat is, the mapping m : ( p,q) ↦→ (il,jl,dl) is one-to-one, and thus is\\na valid function. The inverse mapping, however, is one-to-many (thus not a\\nvalid function). If we use m−1 to represent the inverse mapping, we know that\\nm−1(il,jl,dl) is a set S, where each (p,q) ∈S satisﬁes that m(p,q) = (il,jl,dl).\\nNow we take a look at φ(xl) from a diﬀerent perspective. In order to fully\\nspecify φ(xl), what information is required? It is obvious that the following\\nthree types of information are needed (and only those). For every element of\\nφ(xl), we need to know\\n(A) Which region does it belong to, i.e., what is the value of p (0 ≤p <\\nHl+1Wl+1)?\\n(B) Which element is it inside the region (or equivalently inside the convolution\\nkernel), i.e., what is the value of q (0 ≤q <HWDl)?\\nThe above two types of information determines a location ( p,q) inside φ(xl).\\nThe only missing information is\\n(C) What is the value in that position, i.e.,\\n[\\nφ(xl)\\n]\\npq?\\nSince every element in φ(xl) is a verbatim copy of one element from xl, we\\ncan turn [C] into a diﬀerent but equivalent one:\\n(C.1) For\\n[\\nφ(xl)\\n]\\npq, where is this value copied from? Or, what is its original\\nlocation inside xl, i.e., an index u that satisﬁes 0 ≤u<H lWlDl?\\n(C.2) The entire xl.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f01e1c02-d35e-481a-beb6-eb5238bc901b', embedding=None, metadata={'page_label': '20', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='It is easy to see that the collective information in [A, B, C.1] (for the en-\\ntire range of p, q and u), and [C.2] ( xl) contains exactly the same amount of\\ninformation as φ(xl).\\nSince 0 ≤p < Hl+1Wl+1, 0 ≤q < HWDl, and 0 ≤u < HlWlDl, we can\\nuse a a matrix M ∈R(Hl+1Wl+1HWD l)×(HlWlDl) to encode the information in\\n[A, B, C.1]. One row index of this matrix corresponds to one location inside\\nφ(xl) (i.e., a (p,q) pair). One row of M has HlWlDl elements, and each element\\ncan be indexed by ( il,jl,dl). Thus, each element in this matrix is indexed by a\\n5-tuple: ( p,q,i l,jl,dl).\\nThen, we can use the “indicator” method to encode the function m(p,q) =\\n(il,jl,dl) into M. That is, for any possible element in M, its row index x\\ndetermines a ( p,q) pair, and its column index y determines a ( il,jl,dl) triplet,\\nand M is deﬁned as\\nM(x,y) =\\n{\\n1 if m(p,q) = (il,jl,dl)\\n0 otherwise . (36)\\nThe M matrix has the following properties:\\n• It is very high dimensional;\\n• But it is also very sparse: there is only 1 non-zero entry in the HlWlDl\\nelements in one row, because m is a function;\\n• M, which uses information [A, B, C.1], only encodes the one-to-one cor-\\nrespondence between any element in φ(xl) and any element in xl, it does\\nnot encode any speciﬁc value in xl;\\n• Most importantly, putting together the one-to-one correspondence infor-\\nmation in M and the value information in xl, obviously we have\\nvec(φ(xl)) = Mvec(xl) . (37)\\n6.7 Backward propagation: prepare supervision signal for\\nthe previous layer\\nIn the l-th layer, we still need to compute ∂z\\n∂ vec(xl) . For this purpose, we want to\\nreshape xl into a matrix X ∈R(HlWl)×Dl\\n, and use these two equivalent forms\\n(modulo reshaping) interchangeably.\\nThe chain rule states that ∂z\\n∂(vec(xl)T ) = ∂z\\n∂(vec(y)T )\\n∂ vec(y)\\n∂(vec(xl)T ) (cf. Equa-\\ntion 11). We will start by studying the second term in the RHS (utilizing\\nEquations 27 and 37):\\n∂vec(y)\\n∂(vec(xl)T ) = ∂(FT ⊗I) vec(φ(xl))\\n∂(vec(xl)T ) = (FT ⊗I)M. (38)\\nThus,\\n∂z\\n∂(vec(xl)T ) = ∂z\\n∂(vec(y)T )(FT ⊗I)M. (39)\\n20', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cf2f2a69-b473-43e8-a73d-b0b520792c48', embedding=None, metadata={'page_label': '21', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Since (using Equation 25 from right to left)\\n∂z\\n∂(vec(y)T )(FT ⊗I) =\\n(\\n(F ⊗I) ∂z\\n∂vec(y)\\n)T\\n(40)\\n=\\n(\\n(F ⊗I) vec\\n(∂z\\n∂Y\\n))T\\n(41)\\n= vec\\n(\\nI ∂z\\n∂Y FT\\n)T\\n(42)\\n= vec\\n(∂z\\n∂Y FT\\n)T\\n, (43)\\nwe have\\n∂z\\n∂(vec(xl)T ) = vec\\n(∂z\\n∂Y FT\\n)T\\nM, (44)\\nor equivalently\\n∂z\\n∂(vec(xl)) = MT vec\\n(∂z\\n∂Y FT\\n)\\n. (45)\\nLet’s have a closer look at the RHS. ∂z\\n∂Y FT ∈R(Hl+1Wl+1)×(HWD l), and\\nvec\\n(∂z\\n∂Y FT )\\nis a vector in RHl+1Wl+1HWD l\\n. On the other hand, MT is an\\nindicator matrix in R(HlWlDl)×(Hl+1Wl+1HWD l).\\nIn order to pinpoint one element in vec( xl) or one row in MT , we need an\\nindex triplet ( il,jl,dl), with 0 ≤il < Hl, 0 ≤jl < Wl, and 0 ≤dl < Dl.\\nSimilarly, to locate a column in MT or an element in ∂z\\n∂Y FT , we need an index\\npair (p,q), with 0 ≤p<H l+1Wl+1 and 0 ≤q <HWDl.\\nThus, the ( il,jl,dl)-th entry of ∂z\\n∂(vec(xl)) equals the multiplication of two\\nvectors: the row in MT (or the column in M) that is indexed by (il,jl,dl), and\\nvec\\n(∂z\\n∂Y FT )\\n.\\nFurthermore, since MT is an indicator matrix, in the row vector indexed\\nby (il,jl,dl), only those entries whose index ( p,q) satisﬁes m(p,q) = (il,jl,dl)\\nhave a value 1, all other entries are 0. Thus, the ( il,jl,dl)-th entry of ∂z\\n∂(vec(xl))\\nequals the sum of these corresponding entries in vec\\n(∂z\\n∂Y FT )\\n.\\nTransferring the above description into precise mathematical form, we get\\nthe following succinct equation:\\n[∂z\\n∂X\\n]\\n(il,jl,dl)\\n=\\n∑\\n(p,q)∈m−1(il,jl,dl)\\n[∂z\\n∂Y FT\\n]\\n(p,q)\\n. (46)\\nIn other words, to compute ∂z\\n∂X , we do not need to explicitly use the ex-\\ntremely high dimensional matrix M. Instead, Equation 46 and Equations 18\\nto 21 can be used to eﬃciently ﬁnd ∂z\\n∂X .\\nWe use the simple convolution example in Figure 3 to illustrate the inverse\\nmapping m−1, which is shown in Figure 5.\\n21', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bc6d69e2-7b7e-4077-86da-de84ffe1f664', embedding=None, metadata={'page_label': '22', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='/g1005 /g1006 /g1007 /g1005\\n/g1008/g1009/g1010 /g1005\\n/g1011 /g1012 /g1013 /g1005\\nFigure 5: Illustration of how to compute ∂z\\n∂X .\\nIn the right half of Figure 5, the 6 ×4 matrix is ∂z\\n∂Y FT . In order to compute\\nthe partial derivative of z with respect to one element in the input X, we need\\nto ﬁnd which elements in ∂z\\n∂Y FT is involved and add them. In the left half of\\nFigure 5, we show that the input element 5 (shown in larger font) is involved\\nin 4 convolution operations, shown by the red, green, blue and black boxes,\\nrespectively. These 4 convolution operations correspond to p = 1,2,3,4. For\\nexample, when p= 2 (the green box), 5 is the third element in the convolution,\\nhence q = 3 when p = 2 and we put a green circle in the (2 ,3)-th element of\\nthe ∂z\\n∂Y FT matrix. After all 4 circles are put in the ∂z\\n∂Y FT matrix, the partial\\nderivative is the sum of elements in these four locations of ∂z\\n∂Y FT .\\nThe set m−1(il,jl,dl) contains at mostHWDl elements. Hence, Equation 46\\nrequires at most HWDl summations to compute one element of ∂z\\n∂X .4\\n6.8 Fully connected layer as a convolution layer\\nAs aforementioned, one beneﬁt of the convolution layer is that convolution is a\\nlocal operation. The spatial extent of a kernel is often small (e.g., 3 ×3). One\\nelement in xl+1 is usually computed using only a small number of elements in\\nits input xl.\\nA fully connected layer refers to a layer if the computation of any element in\\nthe output xl+1 (or y) requires all elements in the input xl. A fully connected\\nlayer is sometimes useful at the end of a deep CNN model. For example, if after\\nmany convolution, ReLU and pooling (which will be discussed soon) layers, the\\noutput of the current layer contain distributed representations for the input\\nimage, we want to use all these features in the current layer to build features\\nwith stronger capabilities in the next one. A fully connected layer is useful for\\nthis purpose.\\nSuppose the input of a layer xl has size Hl ×Wl ×Dl. If we use convolution\\nkernels whose size is Hl ×Wl ×Dl, then D such kernels form an order 4 tensor\\n4In Caﬀe, this computation is implemented by a function called col2im. In MatConvNet,\\nthis operation is operated in a row2im manner, although the name row2im is not explicitly\\nused.\\n22', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b042ff28-c094-4639-b978-f45506180a11', embedding=None, metadata={'page_label': '23', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='in Hl ×Wl ×Dl ×D. The output is y ∈RD. It is obvious that to compute any\\nelement in y, we need to use all elements in the input xl. Hence, this layer is\\na fully connected layer, but can be implemented as a convolution layer. Hence,\\nwe do not need to derive learning rules for a fully connected layer separately.\\n7 The pooling layer\\nWe will use the same notation inherited from the convolution layer. Let xl ∈\\nRHl×Wl×Dl\\nbe the input to the l-th layer, which is now a pooling layer. The\\npooling operation requires no parameter (i.e., wi is null, hence parameter learn-\\ning is not needed for this layer). The spatial extent of the pooling ( H×W) is\\nspeciﬁed in the design of the CNN structure. Assume that H divides Hl and W\\ndivides Wl and the stride equals the pooling spatial extent,5 the output of pool-\\ning (y or equivalently xl+1) will be an order 3 tensor of sizeHl+1 ×Wl+1 ×Dl+1,\\nwith\\nHl+1 = Hl\\nH , W l+1 = Wl\\nW , D l+1 = Dl . (47)\\nA pooling layer operates upon xl channel by channel independently. Within\\neach channel, the matrix with Hl ×Wl elements are divided into Hl+1 ×Wl+1\\nnonoverlapping subregions, each subregion being H ×W in size. The pooling\\noperator then maps a subregion into a single number.\\nTwo types of pooling operators are widely used: max pooling and average\\npooling. In max pooling, the pooling operator maps a subregion to its maximum\\nvalue, while the average pooling maps a subregion to its average value. In precise\\nmathematics,\\nmax : yil+1,jl+1,d = max\\n0≤i<H,0≤j<W\\nxl\\nil+1×H+i,jl+1×W+j,d , (48)\\naverage : yil+1,jl+1,d = 1\\nHW\\n∑\\n0≤i<H,0≤j<W\\nxl\\nil+1×H+i,jl+1×W+j,d , (49)\\nwhere 0 ≤il+1 <H l+1, 0 ≤jl+1 <W l+1, and 0 ≤d<D l+1 = Dl.\\nPooling is a local operator, and its forward computation is pretty straight-\\nforward. Now we focus on the back propagation. Only max pooling is discussed\\nand we can resort to the indicator matrix again. 6 All we need to encode in this\\nindicator matrix is: for every element in y, where does it come from in xl?\\nWe need a triplet ( il,jl,dl) to pinpoint one element in the input xl, and\\nanother triplet (il+1,jl+1,dl+1) to locate one element in y. The pooling output\\nyil+1,jl+1,dl+1 comes from xl\\nil,jl,dl, if and only if the following conditions are met:\\n• They are in the same channel;\\n• The (il,jl)-th spatial entry belongs to the ( il+1,jl+1)-th subregion;\\n5That is, the strides in the vertical and horizontal direction are H and W, respectively.\\nThe most widely used pooling setup is H = W = 2 with a stride 2.\\n6Average pooling can be dealt with using a similar idea.\\n23', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2caa4aba-f67f-4cae-b204-a4886b5c1a56', embedding=None, metadata={'page_label': '24', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='• The (il,jl)-th spatial entry is the largest one in that subregion.\\nTranslating these conditions into equations, we get\\ndl+1 = dl , (50)\\n⌊il\\nH\\n⌋\\n= il+1,\\n⌊jl\\nW\\n⌋\\n= jl+1 , (51)\\nxl\\nil,jl,dl ≥yi+il+1×H,j+jl+1×W,dl,∀0 ≤i<H, 0 ≤j <W, (52)\\nwhere ⌊·⌋is the ﬂoor function. If the stride is not H (W) in the vertical (hori-\\nzontal) direction, Equation 51 must be changed accordingly.\\nGiven a (il+1,jl+1,dl+1) triplet, there is only one ( il,jl,dl) triplet that sat-\\nisﬁes all these conditions. Thus, we deﬁne an indicator matrix\\nS(xl) ∈R(Hl+1Wl+1Dl+1)×(HlWlDl) . (53)\\nOne triplet of indexes ( il+1,jl+1,dl+1) speciﬁes a row in S, while ( il,jl,dl)\\nspeciﬁes a column. These two triplets together pinpoint one element in S(xl).\\nWe set that element to 1 if Equations 50 to 52 are simultaneously satisﬁed, and\\n0 otherwise. One row of S(xl) corresponds to one element in y, and one column\\ncorresponds to one element in xl.\\nWith the help of this indicator matrix, we have\\nvec(y) = S(xl) vec(xl) . (54)\\nThen, it is obvious that\\n∂vec(y)\\n∂(vec(xl)T ) = S(xl), ∂z\\n∂(vec(xl)T ) = ∂z\\n∂(vec(y)T )S(xl) , (55)\\nand consequently\\n∂z\\n∂vec(xl) = S(xl)T ∂z\\n∂vec(y) . (56)\\nS(xl) is very sparse. It has exactly one nonzero entry in every row. Thus, we\\ndo not need to use the entire matrix in the computation. Instead, we just need\\nto record the locations of those nonzero entries—there are only Hl+1Wl+1Dl+1\\nsuch entries in S(xl).\\nA simple example can explain the meaning of these equations. Let us con-\\nsider a 2 ×2 max pooling with stride 2. For a given channel dl, the ﬁrst spatial\\nsubregion contains four elements in the input, with ( i,j) = (0,0), (1,0), (0,1)\\nand (1,1), and let us suppose the element at spatial location (0 ,1) is the largest\\namong them. In the forward pass, the value indexed by (0 ,1,dl) in the input\\n(i.e., xl\\n0,1,dl) will be assigned to the element in the (0 ,0,dl)-th element in the\\noutput (i.e., y0,0,dl).\\nOne column in S(xl) contains at most one nonzero element if the strides are\\nH and W, respectively. In the above example, the column of S(xl) indexed by\\n24', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5ba97b3b-8bca-4cc5-9547-fa25da49666a', embedding=None, metadata={'page_label': '25', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='(0,0,dl), (1,0,dl) and (1 ,1,dl) are all zero vectors. The column corresponding\\nto (0,1,dl) contains only one nonzero entry, whose row index is determined by\\n(0,0,dl). Hence, in the back propagation, we have\\n[ ∂z\\n∂vec(xl)\\n]\\n(0,1,dl)\\n=\\n[ ∂z\\n∂vec(y)\\n]\\n(0,0,dl)\\n,\\nand\\n[ ∂z\\n∂vec(xl)\\n]\\n(0,0,dl)\\n=\\n[ ∂z\\n∂vec(xl)\\n]\\n(1,0,dl)\\n=\\n[ ∂z\\n∂vec(xl)\\n]\\n(1,1,dl)\\n= 0 .\\nHowever, if the pooling strides are smaller than H and W in the vertical\\nand horizontal directions, respectively, one element in the input tensor may be\\nthe largest element in several pooling subregions. Hence, there can have more\\nthan one nonzero entries in one column of S(xl). Let us consider the example\\ninput in Figure 5. If a 2 ×2 max pooling is applied to it and the stride is 1 in\\nboth directions, the element 9 is the largest in two pooling regions: [ 5 6\\n8 9] and\\n[ 6 1\\n9 1]. Hence, in the column of S(xl) corresponding to the element 9 (indexed by\\n(2,2,dl) in the input tensor), there are two nonzero entries whose row indexes\\ncorrespond to ( il+1,jl+1,dl+1) = (1,1,dl) and (1,2,dl). Thus, in this example,\\nwe have\\n[ ∂z\\n∂vec(xl)\\n]\\n(2,2,dl)\\n=\\n[ ∂z\\n∂vec(y)\\n]\\n(1,1,dl)\\n+\\n[ ∂z\\n∂vec(y)\\n]\\n(1,2,dl)\\n.\\n8 A case study: the VGG-16 net\\nWe have introduced the convolution, pooling, ReLU and fully connected layers\\ntill now, and have brieﬂy mentioned the softmax layer. With these layers, we\\ncan build many powerful deep CNN models.\\n8.1 VGG-Verydeep-16\\nThe VGG-Verydeep-16 CNN model is a pretrained CNN model released by the\\nOxford VGG group.7 We use it as an example to study the detailed structure\\nof CNN networks. The VGG-16 model architecture is listed in Table 2.\\nThere are six types of layers in this model.\\nConvolution A convolution layer is abbreviated as “Conv”. Its description\\nincludes three parts: number of channels; kernel spatial extent (kernel\\nsize); padding (‘p’) and stride (‘st’) size.\\nReLU No description is needed for a ReLU layer.\\n7http://www.robots.ox.ac.uk/~vgg/research/very_deep/\\n25', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5fc97833-0d78-4c68-9247-646560bdd839', embedding=None, metadata={'page_label': '26', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 2: The VGG-Verydeep-16 architecture and receptive ﬁeld\\ntype description r. size type description r. size\\n1 Conv 64;3x3;p=1,st=1 212 20 Conv 512;3x3;p=1,st=1 20\\n2 ReLU 210 21 ReLU 18\\n3 Conv 64;3x3;p=1,st=1 210 22 Conv 512;3x3;p=1,st=1 18\\n4 ReLU 208 23 ReLU 16\\n5 Pool 2x2;st=2 208 24 Pool 2x2;st=2 16\\n6 Conv 128;3x3;p=1,st=1 104 25 Conv 512;3x3;p=1,st=1 8\\n7 ReLU 102 26 ReLU 6\\n8 Conv 128;3x3;p=1,st=1 102 27 Conv 512;3x3;p=1,st=1 6\\n9 ReLU 100 28 ReLU 4\\n10 Pool 2x2;st=2 100 29 Conv 512;3x3;p=1,st=1 4\\n11 Conv 256;3x3;p=1,st=1 50 30 ReLU 2\\n12 ReLU 48 31 Pool 2\\n13 Conv 256;3x3;p=1,st=1 48 32 FC (7x7x512)x4096 1\\n14 ReLU 46 33 ReLU\\n15 Conv 256;3x3;p=1,st=1 46 34 Drop 0.5\\n16 ReLU 44 35 FC 4096x4096\\n17 Pool 2x2;st=2 44 36 ReLU\\n18 Conv 512;3x3;p=1,st=1 22 37 Drop 0.5\\n19 ReLU 20 38 FC 4096x1000\\n39 σ (softmax layer)\\nPool A pooling layer is abbreviated as “Pool”. Only max pooling is used in\\nVGG-16. The pooling kernel size is always 2 ×2 and the stride is always\\n2 in VGG-16.\\nFully connectedA fully connected layer is abbreviated as “FC”. Fully con-\\nnected layers are implemented using convolution in VGG-16. Its size is\\nshown in the format n1 ×n2, where n1 is the size of the input tensor, and\\nn2 is the size of the output tensor. Although n1 can be a triplet (such as\\n7 ×7 ×512, n2 is always an integer.\\nDropout A dropout layer is abbreviated as “Drop”. Dropout is a technique to\\nimprove the generalization of deep learning methods. It sets the weights\\nconnected to a certain percentage of nodes in the network to 0 (and VGG-\\n16 set the percentage to 0.5 in the two dropout layers).\\nSoftmax It is abbreviated as “ σ”.\\nWe want to add a few notes about this example deep CNN architecture.\\n• A convolution layer is always followed by a ReLU layer in VGG-16. The\\nReLU layers increase the nonlinearity of the CNN model.\\n• The convolution layers between two pooling layers have the same number\\nof channels, kernel size and stride. In fact, stacking two 3 ×3 convolution\\nlayers is equivalent to one 5×5 convolution layer; and stacking three 3×3\\nconvolution kernels replaces a 7 ×7 convolution layer. Stacking a few (2\\nor 3) smaller convolution kernels, however, computes faster than a large\\n26', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='12af5235-c447-4ff7-8be1-016901aedbbf', embedding=None, metadata={'page_label': '27', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='convolution kernel. In addition, the number of parameters is also reduced,\\ne.g., 2 ×3 ×3 = 18 < 25 = 5 ×5. The ReLU layers inserted in between\\nsmall convolution layers are also helpful.\\n• The input to VGG-16 is an image with size 224 ×224 ×3. Because the\\npadding is one in the convolution kernels (meaning one row or column is\\nadded outside of the four edges of the input), convolution will not change\\nthe spatial extent. The pooling layers will reduce the input size by a factor\\nof 2. Hence, the output after the last (5th) pooling layer has spatial extent\\n7 ×7 (and 512 channels). We may interpret this tensor as 7 ×7 ×512 =\\n25088 “features”. The ﬁrst fully connected layer converts them into 4096\\nfeatures. The number of features remains at 4096 after the second fully\\nconnected layer.\\n• The VGG-16 is trained for the ImageNet classiﬁcation challenge, which is\\nan object recognition problem with 1000 classes. The last fully connected\\nlayer (4096 ×1000) output a length 1000 vector for every input image,\\nand the softmax layer converts this length 1000 vector into the estimated\\nposterior probability for the 1000 classes.\\n8.2 Receptive ﬁeld\\nAnother important concept in CNN is the receptive ﬁeld size (abbreviated as\\n“r. size” in Table 2). Let us look at one element in the input to the ﬁrst fully\\nconnected layer ( 32|FC). Because it is the output of a max pooling, we need\\nvalues in a 2 ×2 spatial extent in the input to the max pool layer to compute\\nthis element (and we only need elements in this spatial extent). This 2 ×2\\nspatial extent is called the receptive ﬁeld for this element. In Table 2, we listed\\nthe spatial extent for any element in the output of the last pooling layer. Note\\nthat because the receptive ﬁeld is square, we only use one number (e.g., 48 for\\n48 ×48). The receptive ﬁeld size listed for one layer is the spatial extent in the\\ninput to that layer.\\nA 3 ×3 convolution layer will increase the receptive ﬁeld by 2 and a pooling\\nlayer will double the spatial extent. As shown in Table 2, receptive ﬁeld size in\\nthe input to the ﬁrst layer is 212×212. In other words, in order to compute any\\nsingle element in the 7 ×7 ×512 output of the last pooling layer, a 212 ×212\\nimage patch is required (including the padded pixels in all convolution layers).\\nIt is obvious that the receptive ﬁeld size increases when the network becomes\\ndeeper, especially when a pooling layer is added to the deep net. Unlike tra-\\nditional computer vision and image processing features which depend only on\\na small receptive ﬁeld (e.g., 16 ×16), deep CNN computes its representation\\n(or features) using large receptive ﬁelds. The larger receptive ﬁeld characteris-\\ntic is an important reason why CNN has achieved higher accuracy than classic\\nmethods in image recognition.\\n27', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9d26403a-70ed-4c62-83dc-1ab2966370ce', embedding=None, metadata={'page_label': '28', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='9 Remarks\\nWe hope this introductory note on CNN is clear, self-contained, and easy to\\nunderstand to our readers.\\nOnce a reader is conﬁdent in his/her understanding of CNN at the math-\\nematical level, in the next step it is very helpful to get some hands on CNN\\nexperience. For example, one can validate what has been talked about in this\\nnote using the MatConvNet software package if you prefer the Matlab environ-\\nment.8 For C++ lovers, Caﬀe is a widely used tool. 9 The Theano package\\nis a python package for deep learning. 10 Many more resources for deep learn-\\ning (not only CNN) are available, e.g., Torch, 11, TensorFlow,12 and more will\\nemerge soon.\\nExercises\\n1. Dropout is a very useful technique in training neural networks, which is\\nproposed by Srivastava et al. in a paper titled “Dropout: A Simple Way\\nto Prevent Neural Networks from Overﬁtting” in JMLR.13 Carefully read\\nthis paper and answer the following questions (please organize your answer\\nto every question in one brief sentence).\\n(a) How does dropout operate during training?\\n(b) How does dropout operate during testing?\\n(c) What is the beneﬁt of dropout?\\n(d) Why dropout can achieve this beneﬁt?\\n2. The VGG16 CNN model (also called VGG-Verydeep-16) was publicized\\nby Karen Simonyan and Andrew Zisserman in a paper titled “Very Deep\\nConvolutional Networks for Large-Scale Image Recognition” in the arXiv\\npreprint server.14 And, the GoogLeNet model was publicized by Szegedy\\net al. in a paper titled “Going Deeper with Convolutions” in the arXiv\\npreprint server.15 These two papers were publicized around the same time\\nand share some similar ideas. Carefully read both papers and answer the\\nfollowing questions (please organize your answer to every question in one\\nbrief sentence).\\n(a) Why do they use small convolution kernels (mainly 3 ×3) rather than\\n8http://www.vlfeat.org/matconvnet/\\n9http://caffe.berkeleyvision.org/\\n10http://deeplearning.net/software/theano/\\n11http://torch.ch/\\n12https://www.tensorflow.org/\\n13Available at http://jmlr.org/papers/v15/srivastava14a.html\\n14Available at https://arxiv.org/abs/1409.1556, later published in ICLR 2015 as a confer-\\nence track paper.\\n15Available at https://arxiv.org/abs/1409.4842, later published in CVPR 2015.\\n28', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b06cf58e-9868-47cb-b564-20c6366327c8', embedding=None, metadata={'page_label': '29', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='larger ones?\\n(b) Why both networks are quite deep (i.e., with many layers, around 20)?\\n(c) Which diﬃculty is caused by the large depth? How are they solved in\\nthese two networks?\\n3. Batch Normalization (BN) is another very useful technique in training\\ndeep neural networks, which is proposed by Sergey Ioﬀe and Christian\\nSzegedy, in a paper titled “Batch Normalization: Accelerating Deep Net-\\nwork Training by Reducing Internal Covariate Shift” in ICML 2015. 16\\nCarefully read this paper and answer the following questions (please or-\\nganize your answer to every question in one brief sentence).\\n(a) What is internal covariate shift?\\n(b) How does BN deal with this?\\n(c) How does BN operate in a convolution layer?\\n(d) What is the beneﬁt of using BN?\\n4. ResNet is a very deep neural network learning technique proposed by He\\net al. in a paper titled “Deep Residual Learning for Image Recognition” in\\nCVPR 2016.17 Carefully read this paper and answer the following ques-\\ntions (please organize your answer to every question in one brief sentence).\\n(a) Although VGG16 and GoogLeNet have encountered diﬃculties in\\ntraining networks around 20–30 layers, what enables ResNet to train net-\\nworks as deep as 1000 layers?\\n(b) VGG16 is a feed-forward network, where each layer has only one input\\nand only one output. While GoogLeNet and ResNet are DAGs (directed\\nacyclic graph), where one layer can have multiple inputs and multiple\\noutputs, so long as the data ﬂow in the network structure does not form\\na cycle. What is the beneﬁt of DAG vs. feed-forward?\\n(c) VGG16 has two fully connected layers (fc6 and fc7), while ResNet and\\nGoogLeNet do not have fully connected layers (except the last layer for\\nclassiﬁcation). What is used to replace FC in them? What is the beneﬁt?\\n5. AlexNet refers to the deep convolutional neural network trained on the\\nILSVRC challenge data, which is a groundbreaking work of deep CNN\\nfor computer vision tasks. The technical details of AlexNet is reported\\nin the paper “ImageNet Classiﬁcation with Deep Convolutional Neural\\nNetworks”, by Alex Krizhevsky, Ilya Sutskever and Geoﬀrey E. Hinton\\nin NIPS 25. 18 It proposed the ReLU activation function and creatively\\nused GPUs to accelerate the computations. Carefully read this paper\\n16Available at http://jmlr.org/proceedings/papers/v37/ioﬀe15.pdf\\n17Available at https://arxiv.org/pdf/1512.03385.pdf\\n18This paper is available at http://papers.nips.cc/paper/4824-imagenet-classiﬁcation-with-\\ndeep-convolutional-neural-networks\\n29', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8e6c8820-c4be-4d9c-9b04-38804de63e4e', embedding=None, metadata={'page_label': '30', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='and answer the following questions (please organize your answer to every\\nquestion in one brief sentence).\\n(a) Describe your understanding of how ReLU helps its success? And,\\nhow do the GPUs help out?\\n(b) Using the average of predictions from several networks help reduce the\\nerror rates. Why?\\n(c) Where is the dropout technique applied? How does it help? And what\\nis the cost of using dropout?\\n(d) How many parameters are there in AlexNet? Why the dataset size\\n(1.2 million) is important for the success of AlexNet?\\n6. We will try diﬀerent CNN structures on the MNIST dataset. We denote\\nthe “baseline” network in the MNIST example in MatConvNet as BASE\\nin this question. 19 In this question, a convolution layer is denoted as\\n“x×y×nIn ×nOut”, whose kernel size is x×y, with nIn input and nOut\\noutput channels, with stride equal 1 and pad equal 0. The pooling layers\\nare 2 ×2 max pooling with stride equal 2. The BASE network has four\\nblocks. The ﬁrst consists of a 5 ×5×1×20 convolution and a max pooling;\\nthe second block is composed of a 5 ×5 ×20 ×50 convolution and a max\\npooling; the third block is a 4 ×4 ×50 ×500 convolution (FC) plus a\\nReLU layer; and the ﬁnal block is the classiﬁcation layer (1 ×1 ×500 ×10\\nconvolution).\\n(a) The MNIST dataset is available at yann.lecun.com/exdb/mnist. Read\\nthe instructions in that page, and write a program to transform the data\\nto formats that suit your favorite deep learning software.\\n(b) Learning deep learning models often involve random numbers. Before\\nthe training starts, set the random number generator’s seed to 0. Then,\\nuse the BASE network structure and the ﬁrst 10000 training examples\\nto learn its parameters. What is test set error rate (on the 10000 test\\nexamples) after 20 training epochs?\\n(c) From now on, if not otherwise speciﬁed, we assume the ﬁrst 10000\\ntraining examples and 20 epochs are used. Now we deﬁne the BN network\\nstructure, which adds a batch normalization layer after every convolution\\nlayer in the ﬁrst three blocks. What is its error rate? What will you say\\nabout BN vs. BASE?\\n(d) If you add a dropout layer after the classiﬁcation layer in the 4th block.\\nWhat is the new error rate of BASE and BN? What you will comment on\\ndropout?\\n(e) Now we deﬁne the SK network structure, which refers to small kernel\\nsize. SK is based on BN. The ﬁrst block (5 ×5 convolution plus pooling)\\nnow is changed to two 3×3 convolutions, and BN + ReLU is applied after\\n19MatConvNet version 1.0-beta20. Please refer to MatConvNet for all the details of BASE,\\nsuch as parameter initialization and learning rate.\\n30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='91cc579f-9d26-432c-a044-82e4c87c1963', embedding=None, metadata={'page_label': '31', 'file_name': 'CNN.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/CNN.pdf', 'file_type': 'application/pdf', 'file_size': 784149, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='every convolution. For example, block 1 is now 3 ×3 ×1 ×20 convolution\\n+ BN + ReLU + 3×3×20×20 convolution + BN + ReLU + pool. What\\nis SK’s error rate? How will you comment on that (e.g., how and why the\\nerror rate changes?)\\n(f) Now we deﬁne the SK-s networks structure. The notation ‘s’ refers to\\na multiplier that changes the number of channels in convolution layers.\\nFor example, SK is the same as SK-1. And, SK-2 means the number of\\nchannels in all convolution layers (except the one in block 4) are multiplied\\nby 2. Train networks for SK-2, SK-1.5, SK-1, SK-0.5 and SK-0.2. Report\\ntheir error rates and comment on them.\\n(g) Now we experiment with diﬀerent training set sizes using the SK-0.2\\nnetwork structure. Using the ﬁrst 500, 1000, 2000, 5000, 10000, 20000, and\\n60000 (all) training examples, what error rates do you achieve? Comment\\non your observations.\\n(h) Using the SK-0.2 network structure, study how diﬀerent training sets\\naﬀect its performance. Train 6 networks, and use the (10000 ×(i−1)+1)-\\nth to ( i×10000)-th training examples in training the i-th network. Are\\nCNNs stable in terms of diﬀerent training sets?\\n(i) Now we study how randomness aﬀects CNN learning. Instead of set\\nthe random number generator’s seed to 0, use 1, 12, 123, 1234, 12345 and\\n123456 as the seed to train 6 diﬀerent SK-0.2 networks. What are their\\nerror rates? Comment on your observations.\\n(j) Finally, in SK-0.2, change all ReLU layers to sigmoid layers. How do\\nyou comment on the comparison on error rates of using ReLU and sigmoid\\nactivation functions?\\n31', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4c1c2101-8157-40f9-8912-762197bc4175', embedding=None, metadata={'page_label': '1', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Neural Networks and Deep Learning\\nMichael Nielsen\\nThe original online book can be found at\\nhttp://neuralnetworksanddeeplearning.com', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ba280db6-aa57-4960-bfc1-0042ca27543d', embedding=None, metadata={'page_label': '2', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='977fff21-8168-4e60-8bf0-5d5c38b6237b', embedding=None, metadata={'page_label': 'i', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c i\\nContents\\nWhat this book is about iii\\nOn the exercises and problems v\\n1 Using neural nets to recognize handwritten digits 1\\n1.1 Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.2 Sigmoid neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.3 The architecture of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n1.4 A simple network to classify handwritten digits . . . . . . . . . . . . . . . . . . . 12\\n1.5 Learning with gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n1.6 Implementing our network to classify digits . . . . . . . . . . . . . . . . . . . . . 24\\n1.7 Toward deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2 How the backpropagation algorithm works 39\\n2.1 Warm up: a fast matrix-based approach to computing the output from a neural\\nnetwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.2 The two assumptions we need about the cost function . . . . . . . . . . . . . . 42\\n2.3 The Hadamard product, s ⊙t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n2.4 The four fundamental equations behind backpropagation . . . . . . . . . . . . 43\\n2.5 Proof of the four fundamental equations (optional) . . . . . . . . . . . . . . . . 48\\n2.6 The backpropagation algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n2.7 The code for backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n2.8 In what sense is backpropagation a fast algorithm? . . . . . . . . . . . . . . . . 52\\n2.9 Backpropagation: the big picture . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n3 Improving the way neural networks learn 59\\n3.1 The cross-entropy cost function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n3.1.1 Introducing the cross-entropy cost function . . . . . . . . . . . . . . . . 62\\n3.1.2 Using the cross-entropy to classify MNIST digits . . . . . . . . . . . . . . 67\\n3.1.3 What does the cross-entropy mean? Where does it come from? . . . . 68\\n3.1.4 Softmax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\n3.2 Overﬁtting and regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\\n3.2.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\\n3.2.2 Why does regularization help reduce overﬁtting? . . . . . . . . . . . . . 83\\n3.2.3 Other techniques for regularization . . . . . . . . . . . . . . . . . . . . . 87\\n3.3 Weight initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n3.4 Handwriting recognition revisited: the code . . . . . . . . . . . . . . . . . . . . . 98\\n3.5 How to choose a neural network’s hyper-parameters? . . . . . . . . . . . . . . . 107\\n3.6 Other techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4624cb51-1e40-4085-bc3e-2bdd79e008f7', embedding=None, metadata={'page_label': 'ii', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ii\\n\\x0c\\x0c\\x0c Contents\\n3.6.1 Variations on stochastic gradient descent . . . . . . . . . . . . . . . . . . 118\\n4 A visual proof that neural nets can compute any function 127\\n4.1 Two caveats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\\n4.2 Universality with one input and one output . . . . . . . . . . . . . . . . . . . . . 130\\n4.3 Many input variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n4.4 Extension beyond sigmoid neurons . . . . . . . . . . . . . . . . . . . . . . . . . . 146\\n4.5 Fixing up the step functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\\n5 Why are deep neural networks hard to train? 151\\n5.1 The vanishing gradient problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\\n5.2 What’s causing the vanishing gradient problem? Unstable gradients in deep\\nneural nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\n5.3 Unstable gradients in more complex networks . . . . . . . . . . . . . . . . . . . 163\\n5.4 Other obstacles to deep learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n6 Deep learning 167\\n6.1 Introducing convolutional networks . . . . . . . . . . . . . . . . . . . . . . . . . . 169\\n6.2 Convolutional neural networks in practice . . . . . . . . . . . . . . . . . . . . . . 176\\n6.3 The code for our convolutional networks . . . . . . . . . . . . . . . . . . . . . . . 185\\n6.4 Recent progress in image recognition . . . . . . . . . . . . . . . . . . . . . . . . . 196\\n6.5 Other approaches to deep neural nets . . . . . . . . . . . . . . . . . . . . . . . . . 202\\n6.6 On the future of neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\nA Is there a simple algorithm for intelligence? 211', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c848e06d-b9e0-424c-a832-d983ce3d8746', embedding=None, metadata={'page_label': 'iii', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c iii\\nWhat this book is about\\nNeural networks are one of the most beautiful programming paradigms ever invented. In\\nthe conventional approach to programming, we tell the computer what to do, breaking big\\nproblems up into many small, precisely deﬁned tasks that the computer can easily perform.\\nBy contrast, in a neural network we don’t tell the computer how to solve our problem. Instead,\\nit learns from observational data, ﬁguring out its own solution to the problem at hand.\\nAutomatically learning from data sounds promising. However, until 2006 we didn’t\\nknow how to train neural networks to surpass more traditional approaches, except for\\na few specialized problems. What changed in 2006 was the discovery of techniques for\\nlearning in so-called deep neural networks. These techniques are now known as deep\\nlearning. They’ve been developed further, and today deep neural networks and deep learning\\nachieve outstanding performance on many important problems in computer vision, speech\\nrecognition, and natural language processing. They’re being deployed on a large scale by\\ncompanies such as Google, Microsoft, and Facebook.\\nThe purpose of this book is to help you master the core concepts of neural networks,\\nincluding modern techniques for deep learning. After working through the book you will\\nhave written code that uses neural networks and deep learning to solve complex pattern\\nrecognition problems. And you will have a foundation to use neural networks and deep\\nlearning to attack problems of your own devising.\\nA principle-oriented approach\\nOne conviction underlying the book is that it’s better to obtain a solid understanding of the\\ncore principles of neural networks and deep learning, rather than a hazy understanding\\nof a long laundry list of ideas. If you’ve understood the core ideas well, you can rapidly\\nunderstand other new material. In programming language terms, think of it as mastering\\nthe core syntax, libraries and data structures of a new language. You may still only “know” a\\ntiny fraction of the total language – many languages have enormous standard libraries – but\\nnew libraries and data structures can be understood quickly and easily .\\nThis means the book is emphatically not a tutorial in how to use some particular neural\\nnetwork library . If you mostly want to learn your way around a library , don’t read this book!\\nFind the library you wish to learn, and work through the tutorials and documentation. But\\nbe warned. While this has an immediate problem-solving payoff, if you want to understand\\nwhat’s really going on in neural networks, if you want insights that will still be relevant\\nyears from now, then it’s not enough just to learn some hot library . You need to understand\\nthe durable, lasting insights underlying how neural networks work. Technologies come and\\ntechnologies go, but insight is forever.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b34ad833-9b7a-4f4b-ba2a-e540c351702f', embedding=None, metadata={'page_label': 'iv', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='iv\\n\\x0c\\x0c\\x0c What this book is about\\nA hands-on approach\\nWe’ll learn the core principles behind neural networks and deep learning by attacking a\\nconcrete problem: the problem of teaching a computer to recognize handwritten digits. This\\nproblem is extremely difﬁcult to solve using the conventional approach to programming.\\nAnd yet, as we’ll see, it can be solved pretty well using a simple neural network, with just a\\nfew tens of lines of code, and no special libraries. What’s more, we’ll improve the program\\nthrough many iterations, gradually incorporating more and more of the core ideas about\\nneural networks and deep learning.\\nThis hands-on approach means that you’ll need some programming experience to read\\nthe book. But you don’t need to be a professional programmer. I’ve written the code in Python\\n(version 2.7), which, even if you don’t program in Python, should be easy to understand with\\njust a little effort. Through the course of the book we will develop a little neural network\\nlibrary , which you can use to experiment and to build understanding. All the code is available\\nfor download here. Once you’ve ﬁnished the book, or as you read it, you can easily pick up\\none of the more feature-complete neural network libraries intended for use in production.\\nOn a related note, the mathematical requirements to read the book are modest. There\\nis some mathematics in most chapters, but it’s usually just elementary algebra and plots of\\nfunctions, which I expect most readers will be okay with. I occasionally use more advanced\\nmathematics, but have structured the material so you can follow even if some mathematical\\ndetails elude you. The one chapter which uses heavier mathematics extensively is Chapter 2,\\nwhich requires a little multivariable calculus and linear algebra. If those aren’t familiar, I\\nbegin Chapter 2 with a discussion of how to navigate the mathematics. If you’re ﬁnding it\\nreally heavy going, you can simply skip to the summary of the chapter’s main results. In any\\ncase, there’s no need to worry about this at the outset.\\nIt’s rare for a book to aim to be both principle-oriented and hands-on. But I believe\\nyou’ll learn best if we build out the fundamental ideas of neural networks. We’ll develop\\nliving code, not just abstract theory , code which you can explore and extend. This way you’ll\\nunderstand the fundamentals, both in theory and practice, and be well set to add further to\\nyour knowledge.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='88cda27b-da7c-4480-b8bc-df8501837a2b', embedding=None, metadata={'page_label': 'v', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c v\\nOn the exercises and problems\\nIt’s not uncommon for technical books to include an admonition from the author that readers\\nmust do the exercises and problems. I always feel a little peculiar when I read such warnings.\\nWill something bad happen to me if I don’t do the exercises and problems? Of course not.\\nI’ll gain some time, but at the expense of depth of understanding. Sometimes that’s worth it.\\nSometimes it’s not.\\nSo what’s worth doing in this book? My advice is that you really should attempt most of\\nthe exercises, and you should aim not to do most of the problems.\\nYou should do most of the exercises because they’re basic checks that you’ve understood\\nthe material. If you can’t solve an exercise relatively easily , you’ve probably missed something\\nfundamental. Of course, if you do get stuck on an occasional exercise, just move on – chances\\nare it’s just a small misunderstanding on your part, or maybe I’ve worded something poorly .\\nBut if most exercises are a struggle, then you probably need to reread some earlier material.\\nThe problems are another matter. They’re more difﬁcult than the exercises, and you’ll\\nlikely struggle to solve some problems. That’s annoying, but, of course, patience in the face\\nof such frustration is the only way to truly understand and internalize a subject.\\nWith that said, I don’t recommend working through all the problems. What’s even\\nbetter is to ﬁnd your own project. Maybe you want to use neural nets to classify your music\\ncollection. Or to predict stock prices. Or whatever. But ﬁnd a project you care about. Then\\nyou can ignore the problems in the book, or use them simply as inspiration for work on your\\nown project. Struggling with a project you care about will teach you far more than working\\nthrough any number of set problems. Emotional commitment is a key to achieving mastery .\\nOf course, you may not have such a project in mind, at least up front. That’s ﬁne. Work\\nthrough those problems you feel motivated to work on. And use the material in the book to\\nhelp you search for ideas for creative personal projects.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d0e9f851-45a5-447d-8586-fa37ff292832', embedding=None, metadata={'page_label': 'vi', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='vi\\n\\x0c\\x0c\\x0c On the exercises and problems', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a5721e1e-e75a-48ee-bb56-fb6ba197da47', embedding=None, metadata={'page_label': '1', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 1\\n11111\\nUsing neural nets to recognize\\nhandwritten digits\\nThe human visual system is one of the wonders of the world. Consider the following sequence\\nof handwritten digits:\\nMost people effortlessly recognize those digits as 504192. That ease is deceptive. In each\\nhemisphere of our brain, humans have a primary visual cortex, also known as V1, containing\\n140 million neurons, with tens of billions of connections between them. And yet human\\nvision involves not just V1, but an entire series of visual cortices – V2, V3, V4, and V5 – doing\\nprogressively more complex image processing. We carry in our heads a supercomputer, tuned\\nby evolution over hundreds of millions of years, and superbly adapted to understand the\\nvisual world. Recognizing handwritten digits isn’t easy . Rather, we humans are stupendously ,\\nastoundingly good at making sense of what our eyes show us. But nearly all that work is\\ndone unconsciously. And so we don’t usually appreciate how tough a problem our visual\\nsystems solve.\\nThe difﬁculty of visual pattern recognition becomes apparent if you attempt to write\\na computer program to recognize digits like those above. What seems easy when we do it\\nourselves suddenly becomes extremely difﬁcult. Simple intuitions about how we recognize\\nshapes – “a 9 has a loop at the top, and a vertical stroke in the bottom right” – turn out to\\nbe not so simple to express algorithmically. When you try to make such rules precise, you\\nquickly get lost in a morass of exceptions and caveats and special cases. It seems hopeless.\\nNeural networks approach the problem in a different way. The idea is to take a large\\nnumber of handwritten digits, known as training examples,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='20216fd1-3e21-4ab8-8858-193c65dd4d4f', embedding=None, metadata={'page_label': '2', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nand then develop a system which can learn from those training examples. In other words, the\\nneural network uses the examples to automatically infer rules for recognizing handwritten\\ndigits. Furthermore, by increasing the number of training examples, the network can learn\\nmore about handwriting, and so improve its accuracy . So while I’ve shown just 100 training\\ndigits above, perhaps we could build a better handwriting recognizer by using thousands or\\neven millions or billions of training examples.\\nIn this chapter we’ll write a computer program implementing a neural network that\\nlearns to recognize handwritten digits. The program is just 74 lines long, and uses no special\\nneural network libraries. But this short program can recognize digits with an accuracy over\\n96 percent, without human intervention. Furthermore, in later chapters we’ll develop ideas\\nwhich can improve accuracy to over 99 percent. In fact, the best commercial neural networks\\nare now so good that they are used by banks to process cheques, and by post ofﬁces to\\nrecognize addresses.\\nWe’re focusing on handwriting recognition because it’s an excellent prototype problem for\\nlearning about neural networks in general. As a prototype it hits a sweet spot: it’s challenging\\n– it’s no small feat to recognize handwritten digits – but it’s not so difﬁcult as to require an\\nextremely complicated solution, or tremendous computational power. Furthermore, it’s a\\ngreat way to develop more advanced techniques, such as deep learning. And so throughout\\nthe book we’ll return repeatedly to the problem of handwriting recognition. Later in the\\nbook, we’ll discuss how these ideas may be applied to other problems in computer vision,\\nand also in speech, natural language processing, and other domains.\\nOf course, if the point of the chapter was only to write a computer program to recognize\\nhandwritten digits, then the chapter would be much shorter! But along the way we’ll develop\\nmany key ideas about neural networks, including two important types of artiﬁcial neuron\\n(the perceptron and the sigmoid neuron), and the standard learning algorithm for neural\\nnetworks, known as stochastic gradient descent. Throughout, I focus on explaining why\\nthings are done the way they are, and on building your neural networks intuition. That\\nrequires a lengthier discussion than if I just presented the basic mechanics of what’s going on,\\nbut it’s worth it for the deeper understanding you’ll attain. Amongst the payoffs, by the end\\nof the chapter we’ll be in position to understand what deep learning is, and why it matters.\\n1.1 Perceptrons\\nWhat is a neural network? To get started, I’ll explain a type of artiﬁcial neuron called a\\nperceptron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5e5e098b-049c-4dab-b0a1-6c6fe45f1cc0', embedding=None, metadata={'page_label': '3', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1. Perceptrons\\n\\x0c\\x0c\\x0c 3\\nRosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. Today , it’s more\\ncommon to use other models of artiﬁcial neurons – in this book, and in much modern work\\non neural networks, the main neuron model used is one called the sigmoid neuron. We’ll get\\nto sigmoid neurons shortly. But to understand why sigmoid neurons are deﬁned the way\\nthey are, it’s worth taking the time to ﬁrst understand perceptrons.\\nSo how do perceptrons work? A perceptron takes several binary inputs, x1, x2, . . ., and\\nproduces a single binary output:\\nIn the example shown the perceptron has three inputs, x1, x2, x3. In general it could\\nhave more or fewer inputs. Rosenblatt proposed a simple rule to compute the output. He\\nintroduced weights, w1,w2, . . ., real numbers expressing the importance of the respective\\ninputs to the output. The neuron’s output, 0 or 1, is determined by whether the weighted\\nsum\\n∑\\nj wj xj is less than or greater than some threshold value. Just like the weights, the\\nthreshold is a real number which is a parameter of the neuron. To put it in more precise\\nalgebraic terms:\\noutput =\\n¨\\n0 if\\n∑\\nj wj xj ≤threshold\\n1 if\\n∑\\nj wj xj >threshold\\n(1.1)\\nThat’s all there is to how a perceptron works!\\nThat’s the basic mathematical model. A way you can think about the perceptron is that\\nit’s a device that makes decisions by weighing up evidence. Let me give an example. It’s\\nnot a very realistic example, but it’s easy to understand, and we’ll soon get to more realistic\\nexamples. Suppose the weekend is coming up, and you’ve heard that there’s going to be a\\ncheese festival in your city . You like cheese, and are trying to decide whether or not to go to\\nthe festival. You might make your decision by weighing up three factors:\\n1. Is the weather good?\\n2. Does your boyfriend or girlfriend want to accompany you?\\n3. Is the festival near public transit? (You don’t own a car).\\nWe can represent these three factors by corresponding binary variables x1, x2 and x3. For\\ninstance, we’d havex1 = 1 if the weather is good, and x1 = 0 if the weather is bad. Similarly ,\\nx2 = 1 if your boyfriend or girlfriend wants to go, and x2 = 0 if not. And similarly again for\\nx3 and public transit.\\nNow, suppose you absolutely adore cheese, so much so that you’re happy to go to the\\nfestival even if your boyfriend or girlfriend is uninterested and the festival is hard to get to.\\nBut perhaps you really loathe bad weather, and there’s no way you’d go to the festival if\\nthe weather is bad. You can use perceptrons to model this kind of decision-making. One\\nway to do this is to choose a weight w1 = 6 for the weather, and w2 = 2 and w3 = 2 for\\nthe other conditions. The larger value of w1 indicates that the weather matters a lot to you,\\nmuch more than whether your boyfriend or girlfriend joins you, or the nearness of public\\ntransit. Finally , suppose you choose a threshold of 5 for the perceptron. With these choices,\\nthe perceptron implements the desired decision-making model, outputting 1 whenever the\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ddba7682-61c1-4725-a595-ac32f2f4370f', embedding=None, metadata={'page_label': '4', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nweather is good, and 0 whenever the weather is bad. It makes no difference to the output\\nwhether your boyfriend or girlfriend wants to go, or whether public transit is nearby .\\nBy varying the weights and the threshold, we can get different models of decision-making.\\nFor example, suppose we instead chose a threshold of 3. Then the perceptron would decide\\nthat you should go to the festival whenever the weather was good or when both the festival\\nwas near public transit and your boyfriend or girlfriend was willing to join you. In other\\nwords, it’d be a different model of decision-making. Dropping the threshold means you’re\\nmore willing to go to the festival.\\nObviously , the perceptron isn’t a complete model of human decision-making! But what\\nthe example illustrates is how a perceptron can weigh up different kinds of evidence in order\\nto make decisions. And it should seem plausible that a complex network of perceptrons\\ncould make quite subtle decisions:\\nIn this network, the ﬁrst column of perceptrons – what we’ll call the ﬁrstlayer of perceptrons\\n– is making three very simple decisions, by weighing the input evidence. What about the\\nperceptrons in the second layer? Each of those perceptrons is making a decision by weighing\\nup the results from the ﬁrst layer of decision-making. In this way a perceptron in the second\\nlayer can make a decision at a more complex and more abstract level than perceptrons in\\nthe ﬁrst layer. And even more complex decisions can be made by the perceptron in the third\\nlayer. In this way , a many-layer network of perceptrons can engage in sophisticated decision\\nmaking.\\nIncidentally , when I deﬁned perceptrons I said that a perceptron has just a single output.\\nIn the network above the perceptrons look like they have multiple outputs. In fact, they’re\\nstill single output. The multiple output arrows are merely a useful way of indicating that the\\noutput from a perceptron is being used as the input to several other perceptrons. It’s less\\nunwieldy than drawing a single output line which then splits.\\nLet’s simplify the way we describe perceptrons. The condition\\n∑\\nj wj xj >threshold is\\ncumbersome, and we can make two notational changes to simplify it. The ﬁrst change\\nis to write\\n∑\\nj wj xj as a dot product, w ·x =\\n∑\\nj wj xj, where w and x are vectors whose\\ncomponents are the weights and inputs, respectively. The second change is to move the\\nthreshold to the other side of the inequality, and to replace it by what’s known as the\\nperceptron’sbias, b≡−threshold. Using the bias instead of the threshold, the perceptron\\nrule can be rewritten:\\noutput =\\n¨\\n0 if w ·x + b ≤0\\n1 if w ·x + b >0\\n(1.2)\\nYou can think of the bias as a measure of how easy it is to get the perceptron to output\\na 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2a4d1296-e92a-49cf-a736-0ad9bb30a6a4', embedding=None, metadata={'page_label': '5', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1. Perceptrons\\n\\x0c\\x0c\\x0c 5\\nthe perceptron to ﬁre. For a perceptron with a really big bias, it’s extremely easy for the\\nperceptron to output a 1. But if the bias is very negative, then it’s difﬁcult for the perceptron\\nto output a 1. Obviously, introducing the bias is only a small change in how we describe\\nperceptrons, but we’ll see later that it leads to further notational simpliﬁcations. Because of\\nthis, in the remainder of the book we won’t use the threshold, we’ll always use the bias.\\nI’ve described perceptrons as a method for weighing evidence to make decisions. Another\\nway perceptrons can be used is to compute the elementary logical functions we usually think\\nof as underlying computation, functions such as AND, OR, and NAND. For example, suppose\\nwe have a perceptron with two inputs, each with weight –2, and an overall bias of 3. Here’s\\nour perceptron:\\nThen we see that input 00 produces output 1, since (−2) ∗0 + (−2) ∗0 + 3 = 3 is positive.\\nHere, I’ve introduced the∗symbol to make the multiplications explicit. Similar calculations\\nshow that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since\\n(−2) ∗1 + (−2) ∗1 + 3 = −1 is negative. And so our perceptron implements a NAND gate!\\nThe NAND example shows that we can use perceptrons to compute simple logical\\nfunctions. In fact, we can use networks of perceptrons to compute any logical function at\\nall. The reason is that the NAND gate is universal for computation, that is, we can build any\\ncomputation up out of NAND gates. For example, we can use NAND gates to build a circuit\\nwhich adds two bits, x1 and x2. This requires computing the bitwise sum, x1\\n⨁x2, as well\\nas a carry bit which is set to 1 when both x1 and x2 are 1, i.e., the carry bit is just the bitwise\\nproduct x1 x2:\\nTo get an equivalent network of perceptrons we replace all the NAND gates by perceptrons\\nwith two inputs, each with weight –2, and an overall bias of 3. Here’s the resulting network.\\nNote that I’ve moved the perceptron corresponding to the bottom right NAND gate a little,\\njust to make it easier to draw the arrows on the diagram:\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='40ae59a7-8404-46be-80f3-7aeb5ac926db', embedding=None, metadata={'page_label': '6', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nOne notable aspect of this network of perceptrons is that the output from the leftmost per-\\nceptron is used twice as input to the bottommost perceptron. When I deﬁned the perceptron\\nmodel I didn’t say whether this kind of double-output-to-the-same-place was allowed. Actu-\\nally, it doesn’t much matter. If we don’t want to allow this kind of thing, then it’s possible\\nto simply merge the two lines, into a single connection with a weight of –4 instead of two\\nconnections with –2 weights. (If you don’t ﬁnd this obvious, you should stop and prove to\\nyourself that this is equivalent.) With that change, the network looks as follows, with all\\nunmarked weights equal to –2, all biases equal to 3, and a single weight of –4, as marked:\\nUp to now I’ve been drawing inputs like x1 and x2 as variables ﬂoating to the left of the\\nnetwork of perceptrons. In fact, it’s conventional to draw an extra layer of perceptrons – the\\ninput layer – to encode the inputs:\\nThis notation for input perceptrons, in which we have an output, but no inputs,\\nis a shorthand. It doesn’t actually mean a perceptron with no inputs. To see this, suppose\\nwe did have a perceptron with no inputs. Then the weighted sum\\n∑\\nj wj xj would always be\\nzero, and so the perceptron would output 1 if b >0, and 0 if b ≤0. That is, the perceptron\\nwould simply output a ﬁxed value, not the desired value ( x1, in the example above). It’s\\nbetter to think of the input perceptrons as not really being perceptrons at all, but rather\\nspecial units which are simply deﬁned to output the desired values, x1, x2, . . . .\\nThe adder example demonstrates how a network of perceptrons can be used to simulate a\\ncircuit containing many NAND gates. And because NAND gates are universal for computation,\\nit follows that perceptrons are also universal for computation.\\nThe computational universality of perceptrons is simultaneously reassuring and disap-\\npointing. It’s reassuring because it tells us that networks of perceptrons can be as powerful as\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cf902ccf-e287-49e1-b411-d2ab7822c1cb', embedding=None, metadata={'page_label': '7', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2. Sigmoid neurons\\n\\x0c\\x0c\\x0c 7\\nany other computing device. But it’s also disappointing, because it makes it seem as though\\nperceptrons are merely a new type of NAND gate. That’s hardly big news!\\nHowever, the situation is better than this view suggests. It turns out that we can devise\\nlearning algorithms which can automatically tune the weights and biases of a network\\nof artiﬁcial neurons. This tuning happens in response to external stimuli, without direct\\nintervention by a programmer. These learning algorithms enable us to use artiﬁcial neurons\\nin a way which is radically different to conventional logic gates. Instead of explicitly laying\\nout a circuit of NAND and other gates, our neural networks can simply learn to solve problems,\\nsometimes problems where it would be extremely difﬁcult to directly design a conventional\\ncircuit.\\n1.2 Sigmoid neurons\\nLearning algorithms sound terriﬁc. But how can we devise such algorithms for a neural\\nnetwork? Suppose we have a network of perceptrons that we’d like to use to learn to solve\\nsome problem. For example, the inputs to the network might be the raw pixel data from\\na scanned, handwritten image of a digit. And we’d like the network to learn weights and\\nbiases so that the output from the network correctly classiﬁes the digit. To see how learning\\nmight work, suppose we make a small change in some weight (or bias) in the network. What\\nwe’d like is for this small change in weight to cause only a small corresponding change in\\nthe output from the network. As we’ll see in a moment, this property will make learning\\npossible. Schematically, here’s what we want (obviously this network is too simple to do\\nhandwriting recognition!):\\nIf it were true that a small change in a weight (or bias) causes only a small change in output,\\nthen we could use this fact to modify the weights and biases to get our network to behave\\nmore in the manner we want. For example, suppose the network was mistakenly classifying\\nan image as an “8” when it should be a “9”. We could ﬁgure out how to make a small change\\nin the weights and biases so the network gets a little closer to classifying the image as a “9”.\\nAnd then we’d repeat this, changing the weights and biases over and over to produce better\\nand better output. The network would be learning.\\nThe problem is that this isn’t what happens when our network contains perceptrons.\\nIn fact, a small change in the weights or bias of any single perceptron in the network can\\nsometimes cause the output of that perceptron to completely ﬂip, say from 0 to 1. That\\nﬂip may then cause the behaviour of the rest of the network to completely change in some\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cbd0e49d-1dbc-4556-aba8-e72223b00d10', embedding=None, metadata={'page_label': '8', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nvery complicated way . So while your “9” might now be classiﬁed correctly , the behaviour of\\nthe network on all the other images is likely to have completely changed in some hard-to-\\ncontrol way . That makes it difﬁcult to see how to gradually modify the weights and biases so\\nthat the network gets closer to the desired behaviour. Perhaps there’s some clever way of\\ngetting around this problem. But it’s not immediately obvious how we can get a network of\\nperceptrons to learn.\\nWe can overcome this problem by introducing a new type of artiﬁcial neuron called a\\nsigmoid neuron. Sigmoid neurons are similar to perceptrons, but modiﬁed so that small\\nchanges in their weights and bias cause only a small change in their output. That’s the crucial\\nfact which will allow a network of sigmoid neurons to learn.\\nOkay , let me describe the sigmoid neuron. We’ll depict sigmoid neurons in the same way\\nwe depicted perceptrons:\\nJust like a perceptron, the sigmoid neuron has inputs, x1, x2, . . .. But instead of being just 0\\nor 1, these inputs can also take on any values between 0 and 1. So, for instance,0.638 . . .is a\\nvalid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights\\nfor each input, w1, w2, . . ., and an overall bias, b. But the output is not 0 or 1. Instead, it’s\\nσ(wx + b), where σis called the sigmoid function1, and is deﬁned by:\\nσ(z) ≡ 1\\n1 + e−z . (1.3)\\nTo put it all a little more explicitly, the output of a sigmoid neuron with inputs x1,x2,. . .,\\nweights w1, w2, . . ., and biasb is\\n1\\n1 + exp\\n\\x80\\n−\\n∑\\nj wj xj −b\\n\\x8a. (1.4)\\nAt ﬁrst sight, sigmoid neurons appear very different to perceptrons. The algebraic form of\\nthe sigmoid function may seem opaque and forbidding if you’re not already familiar with\\nit. In fact, there are many similarities between perceptrons and sigmoid neurons, and the\\nalgebraic form of the sigmoid function turns out to be more of a technical detail than a true\\nbarrier to understanding.\\nTo understand the similarity to the perceptron model, suppose z ≡w ·x + b is a large\\npositive number. Then e−z ≈0 and so σ(z) ≈1. In other words, when z = w ·x + b is large\\nand positive, the output from the sigmoid neuron is approximately 1, just as it would have\\nbeen for a perceptron. Suppose on the other hand that z = w ·x + b is very negative. Then\\ne−z →∞, and σ(z) ≈0. So when z = w ·x + b is very negative, the behaviour of a sigmoid\\n1Incidentally ,σis sometimes called the logistic function, and this new class of neurons called logistic\\nneurons. It’s useful to remember this terminology , since these terms are used by many people working\\nwith neural nets. However, we’ll stick with the sigmoid terminology .\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2635c5f9-e451-4871-ac17-c26650d424b6', embedding=None, metadata={'page_label': '9', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2. Sigmoid neurons\\n\\x0c\\x0c\\x0c 9\\nneuron also closely approximates a perceptron. It’s only when w ·x + b is of modest size\\nthat there’s much deviation from the perceptron model.\\nWhat about the algebraic form of σ? How can we understand that? In fact, the exact\\nform of σisn’t so important – what really matters is the shape of the function when plotted.\\nHere’s the shape:\\n−6 −4 −2 0 2 4 6\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSigmoid function\\nThis shape is a smoothed out version of a step function:\\n−6 −4 −2 0 2 4 6\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nStep function\\nIf σhad in fact been a step function, then the sigmoid neuron would be a perceptron, since\\nthe output would be 1 or 0 depending on whether w ·x + b was positive or negative2. By\\nusing the actual σfunction we get, as already implied above, a smoothed out perceptron.\\nIndeed, it’s the smoothness of theσfunction that is the crucial fact, not its detailed form.\\nThe smoothness of σmeans that small changes ∆wj in the weights and ∆b in the bias will\\nproduce a small change ∆output in the output from the neuron. In fact, calculus tells us\\nthat ∆output is well approximated by\\n∆output ≈\\n∑\\nj\\n∂output\\n∂wj\\n∆wj + ∂output\\n∂b ∆b (1.5)\\n2Actually , whenw ·x + b = 0 the perceptron outputs 0, while the step function outputs 1. So, strictly\\nspeaking, we’d need to modify the step function at that one point. But you get the idea.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='da678392-ff79-4ca2-80cc-32871ee8a744', embedding=None, metadata={'page_label': '10', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nwhere the sum is over all the weights, wj, and ∂output/∂wj and ∂output/∂b denote partial\\nderivatives of the output with respect to wj and b, respectively. Don’t panic if you’re not\\ncomfortable with partial derivatives! While the expression above looks complicated, with all\\nthe partial derivatives, it’s actually saying something very simple (and which is very good\\nnews): ∆output is a linear function of the changes ∆wj and ∆b in the weights and bias.\\nThis linearity makes it easy to choose small changes in the weights and biases to achieve\\nany desired small change in the output. So while sigmoid neurons have much of the same\\nqualitative behavior as perceptrons, they make it much easier to ﬁgure out how changing\\nthe weights and biases will change the output.\\nIf it’s the shape of σwhich really matters, and not its exact form, then why use the\\nparticular form used for σin Equation 1.3? In fact, later in the book we will occasionally\\nconsider neurons where the output is f (w·x + b) for some other activation function f (·). The\\nmain thing that changes when we use a different activation function is that the particular\\nvalues for the partial derivatives in Equation 1.5 change. It turns out that when we compute\\nthose partial derivatives later, using σwill simplify the algebra, simply because exponentials\\nhave lovely properties when differentiated. In any case, σis commonly-used in work on\\nneural nets, and is the activation function we’ll use most often in this book.\\nHow should we interpret the output from a sigmoid neuron? Obviously , one big difference\\nbetween perceptrons and sigmoid neurons is that sigmoid neurons don’t just output 0 or\\n1. They can have as output any real number between 0 and 1, so values such as 0.173 . . .\\nand 0.689. . .are legitimate outputs. This can be useful, for example, if we want to use the\\noutput value to represent the average intensity of the pixels in an image input to a neural\\nnetwork. But sometimes it can be a nuisance. Suppose we want the output from the network\\nto indicate either “the input image is a 9” or “the input image is not a 9”. Obviously , it’d be\\neasiest to do this if the output was a 0 or a 1, as in a perceptron. But in practice we can\\nset up a convention to deal with this, for example, by deciding to interpret any output of at\\nleast 0.5 as indicating a “9”, and any output less than 0.5 as indicating “not a 9”. I’ll always\\nexplicitly state when we’re using such a convention, so it shouldn’t cause any confusion.\\nExercises\\n• Sigmoid neurons simulating perceptrons, part I Suppose we take all the weights\\nand biases in a network of perceptrons, and multiply them by a positive constant, c>0.\\nShow that the behavior of the network doesn’t change.\\n• Sigmoid neurons simulating perceptrons, part II Suppose we have the same setup\\nas the last problem – a network of perceptrons. Suppose also that the overall input to\\nthe network of perceptrons has been chosen. We won’t need the actual input value, we\\njust need the input to have been ﬁxed. Suppose the weights and biases are such that\\nw ·x + b ̸= 0 for the input x to any particular perceptron in the network. Now replace\\nall the perceptrons in the network by sigmoid neurons, and multiply the weights and\\nbiases by a positive constant c >0. Show that in the limit as c →∞the behaviour of\\nthis network of sigmoid neurons is exactly the same as the network of perceptrons.\\nHow can this fail when w ·x + b = 0 for one of the perceptrons?\\n1.3 The architecture of neural networks\\nIn the next section I’ll introduce a neural network that can do a pretty good job classifying\\nhandwritten digits. In preparation for that, it helps to explain some terminology that lets us\\nname different parts of a network. Suppose we have the network:\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2259efe7-d89d-4bf4-b40d-991cfd685d53', embedding=None, metadata={'page_label': '11', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3. The architecture of neural networks\\n\\x0c\\x0c\\x0c 11\\nAs mentioned earlier, the leftmost layer in this network is called the input layer, and the\\nneurons within the layer are called input neurons. The rightmost or output layer contains\\nthe output neurons, or, as in this case, a single output neuron. The middle layer is called a\\nhidden layer, since the neurons in this layer are neither inputs nor outputs. The term “hidden”\\nperhaps sounds a little mysterious – the ﬁrst time I heard the term I thought it must have\\nsome deep philosophical or mathematical signiﬁcance – but it really means nothing more\\nthan “not an input or an output”. The network above has just a single hidden layer, but some\\nnetworks have multiple hidden layers. For example, the following four-layer network has\\ntwo hidden layers:\\nSomewhat confusingly, and for historical reasons, such multiple layer networks are some-\\ntimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons,\\nnot perceptrons. I’m not going to use the MLP terminology in this book, since I think it’s\\nconfusing, but wanted to warn you of its existence.\\nThe design of the input and output layers in a network is often straightforward. For\\nexample, suppose we’re trying to determine whether a handwritten image depicts a “9” or not.\\nA natural way to design the network is to encode the intensities of the image pixels into the\\ninput neurons. If the image is a 64 by 64 greyscale image, then we’d have4, 096 = 64 ×64\\ninput neurons, with the intensities scaled appropriately between 0 and 1. The output layer\\nwill contain just a single neuron, with output values of less than 0.5 indicating “input image\\nis not a 9”, and values greater than 0.5 indicating “input image is a 9”.\\nWhile the design of the input and output layers of a neural network is often straight-\\nforward, there can be quite an art to the design of the hidden layers. In particular, it’s not\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a42ec2bb-d28f-4b88-98c0-4a9df83628c8', embedding=None, metadata={'page_label': '12', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\npossible to sum up the design process for the hidden layers with a few simple rules of thumb.\\nInstead, neural networks researchers have developed many design heuristics for the hidden\\nlayers, which help people get the behaviour they want out of their nets. For example, such\\nheuristics can be used to help determine how to trade off the number of hidden layers against\\nthe time required to train the network. We’ll meet several such design heuristics later in this\\nbook.\\nUp to now, we’ve been discussing neural networks where the output from one layer is\\nused as input to the next layer. Such networks are called feedforward neural networks. This\\nmeans there are no loops in the network – information is always fed forward, never fed\\nback. If we did have loops, we’d end up with situations where the input to theσfunction\\ndepended on the output. That’d be hard to make sense of, and so we don’t allow such loops.\\nHowever, there are other models of artiﬁcial neural networks in which feedback loops\\nare possible. These models are called recurrent neural networks. The idea in these models is\\nto have neurons which ﬁre for some limited duration of time, before becoming quiescent.\\nThat ﬁring can stimulate other neurons, which may ﬁre a little while later, also for a limited\\nduration. That causes still more neurons to ﬁre, and so over time we get a cascade of neurons\\nﬁring. Loops don’t cause problems in such a model, since a neuron’s output only affects its\\ninput at some later time, not instantaneously .\\nRecurrent neural nets have been less inﬂuential than feedforward networks, in part\\nbecause the learning algorithms for recurrent nets are (at least to date) less powerful. But\\nrecurrent networks are still extremely interesting. They’re much closer in spirit to how our\\nbrains work than feedforward networks. And it’s possible that recurrent networks can solve\\nimportant problems which can only be solved with great difﬁculty by feedforward networks.\\nHowever, to limit our scope, in this book we’re going to concentrate on the more widely-used\\nfeedforward networks.\\n1.4 A simple network to classify handwritten digits\\nHaving deﬁned neural networks, let’s return to handwriting recognition. We can split the\\nproblem of recognizing handwritten digits into two sub-problems. First, we’d like a way\\nof breaking an image containing many digits into a sequence of separate images, each\\ncontaining a single digit. For example, we’d like to break the image\\ninto six separate images,\\nWe humans solve this segmentation problem with ease, but it’s challenging for a computer\\nprogram to correctly break up the image. Once the image has been segmented, the program\\nthen needs to classify each individual digit. So, for instance, we’d like our program to\\nrecognize that the ﬁrst digit above,\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4f5521f6-20ca-4e05-8822-30d84b71d277', embedding=None, metadata={'page_label': '13', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.4. A simple network to classify handwritten digits\\n\\x0c\\x0c\\x0c 13\\nis a 5.\\nWe’ll focus on writing a program to solve the second problem, that is, classifying individual\\ndigits. We do this because it turns out that the segmentation problem is not so difﬁcult to\\nsolve, once you have a good way of classifying individual digits. There are many approaches\\nto solving the segmentation problem. One approach is to trial many different ways of\\nsegmenting the image, using the individual digit classiﬁer to score each trial segmentation.\\nA trial segmentation gets a high score if the individual digit classiﬁer is conﬁdent of its\\nclassiﬁcation in all segments, and a low score if the classiﬁer is having a lot of trouble in one\\nor more segments. The idea is that if the classiﬁer is having trouble somewhere, then it’s\\nprobably having trouble because the segmentation has been chosen incorrectly. This idea\\nand other variations can be used to solve the segmentation problem quite well. So instead of\\nworrying about segmentation we’ll concentrate on developing a neural network which can\\nsolve the more interesting and difﬁcult problem, namely , recognizing individual handwritten\\ndigits.\\nTo recognize individual digits we will use a three-layer neural network:\\nThe input layer of the network contains neurons encoding the values of the input pixels. As\\ndiscussed in the next section, our training data for the network will consist of many 28 by 28\\npixel images of scanned handwritten digits, and so the input layer contains 784 = 28 ×28\\nneurons. For simplicity I’ve omitted most of the 784 input neurons in the diagram above. The\\ninput pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing\\nblack, and in between values representing gradually darkening shades of grey .\\nThe second layer of the network is a hidden layer. We denote the number of neurons in\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ece40221-691e-49e6-aa3d-f905a16bc6a9', embedding=None, metadata={'page_label': '14', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='14\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nthis hidden layer by n, and we’ll experiment with different values forn. The example shown\\nillustrates a small hidden layer, containing just n = 15 neurons.\\nThe output layer of the network contains 10 neurons. If the ﬁrst neuron ﬁres, i.e., has\\nan output ≈1, then that will indicate that the network thinks the digit is a 0. If the second\\nneuron ﬁres then that will indicate that the network thinks the digit is a 1. And so on. A\\nlittle more precisely , we number the output neurons from 0 through 9, and ﬁgure out which\\nneuron has the highest activation value. If that neuron is, say , neuron number 6, then our\\nnetwork will guess that the input digit was a 6. And so on for the other output neurons.\\nYou might wonder why we use 10 output neurons. After all, the goal of the network\\nis to tell us which digit (0,1,2,. . .,9) corresponds to the input image. A seemingly natural\\nway of doing that is to use just 4 output neurons, treating each neuron as taking on a binary\\nvalue, depending on whether the neuron’s output is closer to 0 or to 1. Four neurons are\\nenough to encode the answer, since24 = 16 is more than the 10 possible values for the input\\ndigit. Why should our network use 10 neurons instead? Isn’t that inefﬁcient? The ultimate\\njustiﬁcation is empirical: we can try out both network designs, and it turns out that, for this\\nparticular problem, the network with 10 output neurons learns to recognize digits better\\nthan the network with 4 output neurons. But that leaves us wondering why using 10 output\\nneurons works better. Is there some heuristic that would tell us in advance that we should\\nuse the 10-output encoding instead of the 4-output encoding?\\nTo understand why we do this, it helps to think about what the neural network is\\ndoing from ﬁrst principles. Consider ﬁrst the case where we use 10 output neurons. Let’s\\nconcentrate on the ﬁrst output neuron, the one that’s trying to decide whether or not the\\ndigit is a 0. It does this by weighing up evidence from the hidden layer of neurons. What\\nare those hidden neurons doing? Well, just suppose for the sake of argument that the ﬁrst\\nneuron in the hidden layer detects whether or not an image like the following is present:\\nIt can do this by heavily weighting input pixels which overlap with the image, and only\\nlightly weighting the other inputs. In a similar way , let’s suppose for the sake of argument\\nthat the second, third, and fourth neurons in the hidden layer detect whether or not the\\nfollowing images are present:\\nAs you may have guessed, these four images together make up the 0 image that we saw in\\nthe line of digits shown earlier:\\nSo if all four of these hidden neurons are ﬁring then we can conclude that the digit is a 0. Of\\ncourse, that’s not the only sort of evidence we can use to conclude that the image was a 0\\n– we could legitimately get a 0 in many other ways (say, through translations of the above\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e0083e2b-ad79-4a45-8e49-55d7c9b6ec1b', embedding=None, metadata={'page_label': '15', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.5. Learning with gradient descent\\n\\x0c\\x0c\\x0c 15\\nimages, or slight distortions). But it seems safe to say that at least in this case we’d conclude\\nthat the input was a 0.\\nSupposing the neural network functions in this way , we can give a plausible explanation\\nfor why it’s better to have 10 outputs from the network, rather than 4. If we had 4 outputs,\\nthen the ﬁrst output neuron would be trying to decide what the most signiﬁcant bit of\\nthe digit was. And there’s no easy way to relate that most signiﬁcant bit to simple shapes\\nlike those shown above. It’s hard to imagine that there’s any good historical reason the\\ncomponent shapes of the digit will be closely related to (say) the most signiﬁcant bit in the\\noutput.\\nNow, with all that said, this is all just a heuristic. Nothing says that the three-layer\\nneural network has to operate in the way I described, with the hidden neurons detecting\\nsimple component shapes. Maybe a clever learning algorithm will ﬁnd some assignment of\\nweights that lets us use only 4 output neurons. But as a heuristic the way of thinking I’ve\\ndescribed works pretty well, and can save you a lot of time in designing good neural network\\narchitectures.\\nExercise\\n• There is a way of determining the bitwise representation of a digit by adding an extra\\nlayer to the three-layer network above. The extra layer converts the output from the\\nprevious layer into a binary representation, as illustrated in the ﬁgure below. Find a\\nset of weights and biases for the new output layer. Assume that the ﬁrst 3 layers of\\nneurons are such that the correct output in the third layer (i.e., the old output layer)\\nhas activation at least 0.99, and incorrect outputs have activation less than 0.01.\\n1.5 Learning with gradient descent\\nNow that we have a design for our neural network, how can it learn to recognize digits? The\\nﬁrst thing we’ll need is a data set to learn from – a so-called training data set. We’ll use the\\nMNIST data set, which contains tens of thousands of scanned images of handwritten digits,\\ntogether with their correct classiﬁcations. MNIST’s name comes from the fact that it is a\\nmodiﬁed subset of two data sets collected by NIST , the United States’ National Institute of\\nStandards and Technology . Here’s a few images from MNIST:\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e6c972cf-9fcd-407f-ac64-6b9a1bb3c2d4', embedding=None, metadata={'page_label': '16', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nAs you can see, these digits are, in fact, the same as those shown at the beginning of this\\nchapter as a challenge to recognize. Of course, when testing our network we’ll ask it to\\nrecognize images which aren’t in the training set!\\nThe MNIST data comes in two parts. The ﬁrst part contains 60,000 images to be used\\nas training data. These images are scanned handwriting samples from 250 people, half of\\nwhom were US Census Bureau employees, and half of whom were high school students.\\nThe images are greyscale and 28 by 28 pixels in size. The second part of the MNIST data\\nset is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images.\\nWe’ll use the test data to evaluate how well our neural network has learned to recognize\\ndigits. To make this a good test of performance, the test data was taken from a different\\nset of 250 people than the original training data (albeit still a group split between Census\\nBureau employees and high school students). This helps give us conﬁdence that our system\\ncan recognize digits from people whose writing it didn’t see during training.\\nWe’ll use the notationx to denote a training input. It’ll be convenient to regard each\\ntraining input x as a 28 ×28 = 784-dimensional vector. Each entry in the vector represents\\nthe grey value for a single pixel in the image. We’ll denote the corresponding desired output\\nby y = y(x), where y is a 10-dimensional vector. For example, if a particular training image,\\nx, depicts a 6, then y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)T is the desired output from the network.\\nNote that T here is the transpose operation, turning a row vector into an ordinary (column)\\nvector.\\nWhat we’d like is an algorithm which lets us ﬁnd weights and biases so that the output\\nfrom the network approximates y(x) for all training inputs x. To quantify how well we’re\\nachieving this goal we deﬁne a cost function3:\\nC(w, b) ≡ 1\\n2n\\n∑\\nx\\n∥y(x) −a∥2 (1.6)\\nHere, w denotes the collection of all weights in the network, b all the biases, n is the total\\nnumber of training inputs, a is the vector of outputs from the network when x is input, and\\nthe sum is over all training inputs, x. Of course, the output a depends on x, w and b, but to\\nkeep the notation simple I haven’t explicitly indicated this dependence. The notation∥v∥\\njust denotes the usual length function for a vector v. We’ll callC the quadratic cost function;\\nit’s also sometimes known as themean squared error or just MSE. Inspecting the form of the\\nquadratic cost function, we see that C(w, b) is non-negative, since every term in the sum\\nis non-negative. Furthermore, the cost C(w, b) becomes small, i.e., C(w, b) ≈0, precisely\\nwhen y(x) is approximately equal to the output, a, for all training inputs, x. So our training\\nalgorithm has done a good job if it can ﬁnd weights and biases so that C(w, b) ≈0. By\\ncontrast, it’s not doing so well when C(w, b) is large – that would mean that y(x) is not\\nclose to the output a for a large number of inputs. So the aim of our training algorithm will\\nbe to minimize the cost C(w, b) as a function of the weights and biases. In other words, we\\nwant to ﬁnd a set of weights and biases which make the cost as small as possible. We’ll do\\nthat using an algorithm known as gradient descent.\\n3Sometimes referred to as a loss or objective function. We use the term cost function throughout\\nthis book, but you should note the other terminology , since it’s often used in research papers and other\\ndiscussions of neural networks.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ee248a1f-83db-4d89-b53b-70f3f0928b60', embedding=None, metadata={'page_label': '17', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.5. Learning with gradient descent\\n\\x0c\\x0c\\x0c 17\\nWhy introduce the quadratic cost? After all, aren’t we primarily interested in the number\\nof images correctly classiﬁed by the network? Why not try to maximize that number directly ,\\nrather than minimizing a proxy measure like the quadratic cost? The problem with that\\nis that the number of images correctly classiﬁed is not a smooth function of the weights\\nand biases in the network. For the most part, making small changes to the weights and\\nbiases won’t cause any change at all in the number of training images classiﬁed correctly.\\nThat makes it difﬁcult to ﬁgure out how to change the weights and biases to get improved\\nperformance. If we instead use a smooth cost function like the quadratic cost it turns out to\\nbe easy to ﬁgure out how to make small changes in the weights and biases so as to get an\\nimprovement in the cost. That’s why we focus ﬁrst on minimizing the quadratic cost, and\\nonly after that will we examine the classiﬁcation accuracy .\\nEven given that we want to use a smooth cost function, you may still wonder why we\\nchoose the quadratic function used in Equation 1.6. Isn’t this a rather ad hoc choice? Perhaps\\nif we chose a different cost function we’d get a totally different set of minimizing weights\\nand biases? This is a valid concern, and later we’ll revisit the cost function, and make some\\nmodiﬁcations. However, the quadratic cost function of Equation 1.6 works perfectly well for\\nunderstanding the basics of learning in neural networks, so we’ll stick with it for now.\\nRecapping, our goal in training a neural network is to ﬁnd weights and biases which\\nminimize the quadratic cost function C(w, b). This is a well-posed problem, but it’s got a lot\\nof distracting structure as currently posed – the interpretation of w and b as weights and\\nbiases, the σfunction lurking in the background, the choice of network architecture, MNIST ,\\nand so on. It turns out that we can understand a tremendous amount by ignoring most of\\nthat structure, and just concentrating on the minimization aspect. So for now we’re going to\\nforget all about the speciﬁc form of the cost function, the connection to neural networks,\\nand so on. Instead, we’re going to imagine that we’ve simply been given a function of many\\nvariables and we want to minimize that function. We’re going to develop a technique called\\ngradient descent which can be used to solve such minimization problems. Then we’ll come\\nback to the speciﬁc function we want to minimize for neural networks.\\nOkay, let’s suppose we’re trying to minimize some function,C(v). This could be any\\nreal-valued function of many variables, v = v1, v2, . . .. Note that I’ve replaced thew and b\\nnotation by v to emphasize that this could be any function – we’re not speciﬁcally thinking in\\nthe neural networks context any more. To minimize C(v) it helps to imagine C as a function\\nof just two variables, which we’ll callv1 and v2:\\nWhat we’d like is to ﬁnd where C achieves its global minimum. Now, of course, for the\\nfunction plotted above, we can eyeball the graph and ﬁnd the minimum. In that sense, I’ve\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='56c8ae90-d60f-4559-ae77-0297346009b5', embedding=None, metadata={'page_label': '18', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='18\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nperhaps shown slightly too simple a function! A general function, C, may be a complicated\\nfunction of many variables, and it won’t usually be possible to just eyeball the graph to ﬁnd\\nthe minimum.\\nOne way of attacking the problem is to use calculus to try to ﬁnd the minimum analytically .\\nWe could compute derivatives and then try using them to ﬁnd places whereC is an extremum.\\nWith some luck that might work when C is a function of just one or a few variables. But\\nit’ll turn into a nightmare when we have many more variables. And for neural networks\\nwe’ll often want far more variables – the biggest neural networks have cost functions which\\ndepend on billions of weights and biases in an extremely complicated way . Using calculus to\\nminimize that just won’t work!\\n(After asserting that we’ll gain insight by imaginingC as a function of just two variables,\\nI’ve turned around twice in two paragraphs and said, “hey, but what if it’s a function of\\nmany more than two variables?” Sorry about that. Please believe me when I say that it\\nreally does help to imagine C as a function of two variables. It just happens that sometimes\\nthat picture breaks down, and the last two paragraphs were dealing with such breakdowns.\\nGood thinking about mathematics often involves juggling multiple intuitive pictures, learning\\nwhen it’s appropriate to use each picture, and when it’s not.)\\nOkay , so calculus doesn’t work. Fortunately , there is a beautiful analogy which suggests\\nan algorithm which works pretty well. We start by thinking of our function as a kind of a\\nvalley . If you squint just a little at the plot above, that shouldn’t be too hard. And we imagine\\na ball rolling down the slope of the valley. Our everyday experience tells us that the ball\\nwill eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to\\nﬁnd a minimum for the function? We’d randomly choose a starting point for an (imaginary)\\nball, and then simulate the motion of the ball as it rolled down to the bottom of the valley.\\nWe could do this simulation simply by computing derivatives (and perhaps some second\\nderivatives) of C – those derivatives would tell us everything we need to know about the\\nlocal “shape” of the valley , and therefore how our ball should roll.\\nBased on what I’ve just written, you might suppose that we’ll be trying to write down\\nNewton’s equations of motion for the ball, considering the effects of friction and gravity,\\nand so on. Actually, we’re not going to take the ball-rolling analogy quite that seriously –\\nwe’re devising an algorithm to minimize C, not developing an accurate simulation of the\\nlaws of physics! The ball’s-eye view is meant to stimulate our imagination, not constrain our\\nthinking. So rather than get into all the messy details of physics, let’s simply ask ourselves:\\nif we were declared God for a day , and could make up our own laws of physics, dictating to\\nthe ball how it should roll, what law or laws of motion could we pick that would make it so\\nthe ball always rolled to the bottom of the valley?\\nTo make this question more precise, let’s think about what happens when we move the\\nball a small amount ∆v1 in the v1 direction, and a small amount ∆v2 in the v2 direction.\\nCalculus tells us that C changes as follows:\\n∆C ≈∂C\\n∂v1\\n∆v1 + ∂C\\n∂v2\\n∆v2. (1.7)\\nWe’re going to ﬁnd a way of choosing∆v1 and ∆v2 so as to make ∆C negative; i.e., we’ll\\nchoose them so the ball is rolling down into the valley. To ﬁgure out how to make such a\\nchoice it helps to deﬁne ∆v to be the vector of changes in v, ∆v ≡(∆v1, ∆v2)T , where T is\\nagain the transpose operation, turning row vectors into column vectors. We’ll also deﬁne\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d0ed66d5-15f3-4037-b3fe-79c72cefbcac', embedding=None, metadata={'page_label': '19', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.5. Learning with gradient descent\\n\\x0c\\x0c\\x0c 19\\nthe gradient of C to be the vector of partial derivatives,\\n\\x80\\n∂C\\n∂v1\\n, ∂C\\n∂v2\\n\\x8aT\\n. We denote the gradient\\nvector by ∇C, i.e.:\\n∇C ≡\\n\\x81∂C\\n∂v1\\n, ∂C\\n∂v2\\n\\x8bT\\n. (1.8)\\nIn a moment we’ll rewrite the change ∆C in terms of ∆v and the gradient, ∇C. Before\\ngetting to that, though, I want to clarify something that sometimes gets people hung up on\\nthe gradient. When meeting the ∇C notation for the ﬁrst time, people sometimes wonder\\nhow they should think about the ∇symbol. What, exactly, does ∇C mean? In fact, it’s\\nperfectly ﬁne to think of ∇C as a single mathematical object – the vector deﬁned above –\\nwhich happens to be written using two symbols. In this point of view, ∇C is just a piece of\\nnotational ﬂag-waving, telling you “hey ,∇C is a gradient vector”. There are more advanced\\npoints of view where ∇C can be viewed as an independent mathematical entity in its own\\nright (for example, as a differential operator), but we won’t need such points of view.\\nWith these deﬁnitions, the expression 1.7 for ∆C can be rewritten as\\n∆C ≈∇C ·∆v (1.9)\\nThis equation helps explain why ∇C is called the gradient vector: ∇C relates changes in\\nv to changes in C, just as we’d expect something called a gradient to do. But what’s really\\nexciting about the equation is that it lets us see how to choose∆v so as to make ∆C negative.\\nIn particular, suppose we choose\\n∆v = −η∇C, (1.10)\\nwhere ηis a small, positive parameter (known as the learning rate). Then Equation 1.9 tells\\nus that ∆C ≈−η∇C ·∇C = −η∥∇C∥2. Because ∥∇C∥2 ≥0, this guarantees that ∆C ≤0,\\ni.e., C will always decrease, never increase, if we change v according to the prescription in\\n1.10. (Within, of course, the limits of the approximation in Equation 1.9). This is exactly the\\nproperty we wanted! And so we’ll take Equation 1.10 to deﬁne the “law of motion” for the\\nball in our gradient descent algorithm. That is, we’ll use Equation 1.10 to compute a value\\nfor ∆v, then move the ball’s positionv by that amount:\\nv →v′= v −η∇C. (1.11)\\nThen we’ll use this update rule again, to make another move. If we keep doing this, over\\nand over, we’ll keep decreasingC until – we hope – we reach a global minimum.\\nSumming up, the way the gradient descent algorithm works is to repeatedly compute\\nthe gradient ∇C, and then to move in the opposite direction, “falling down” the slope of the\\nvalley . We can visualize it like this:\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6aa6aba6-c601-4dd9-bf66-64f970418e55', embedding=None, metadata={'page_label': '20', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='20\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nNotice that with this rule gradient descent doesn’t reproduce real physical motion. In real\\nlife a ball has momentum, and that momentum may allow it to roll across the slope, or\\neven (momentarily) roll uphill. It’s only after the effects of friction set in that the ball is\\nguaranteed to roll down into the valley . By contrast, our rule for choosing∆v just says “go\\ndown, right now”. That’s still a pretty good rule for ﬁnding the minimum!\\nTo make gradient descent work correctly, we need to choose the learning rateηto be\\nsmall enough that Equation 1.9 is a good approximation. If we don’t, we might end up with\\n∆C >0, which obviously would not be good! At the same time, we don’t wantηto be too\\nsmall, since that will make the changes ∆v tiny, and thus the gradient descent algorithm\\nwill work very slowly. In practical implementations, ηis often varied so that Equation 1.9\\nremains a good approximation, but the algorithm isn’t too slow. We’ll see later how this\\nworks.\\nI’ve explained gradient descent whenC is a function of just two variables. But, in fact,\\neverything works just as well even when C is a function of many more variables. Suppose in\\nparticular that C is a function of m variables, v1, . . . ,vm. Then the change ∆C in C produced\\nby a small change ∆v = (∆v1, . . . ,∆vm)T is\\n∆C ≈∇C ·∆v, (1.12)\\nwhere the gradient ∇C is the vector\\n∇C ≡\\n\\x81∂C\\n∂v1\\n, . . . ,∂C\\n∂vm\\n\\x8bT\\n. (1.13)\\nJust as for the two variable case, we can choose\\n∆v = −η∇C, (1.14)\\nand we’re guaranteed that our (approximate) expression 1.12 for∆C will be negative. This\\ngives us a way of following the gradient to a minimum, even when C is a function of many\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5096f584-8d92-4f67-946e-62585f10a70d', embedding=None, metadata={'page_label': '21', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.5. Learning with gradient descent\\n\\x0c\\x0c\\x0c 21\\nvariables, by repeatedly applying the update rule\\nv →v′= v −η∇C. (1.15)\\nYou can think of this update rule as deﬁning the gradient descent algorithm. It gives us a\\nway of repeatedly changing the position v in order to ﬁnd a minimum of the function C. The\\nrule doesn’t always work – several things can go wrong and prevent gradient descent from\\nﬁnding the global minimum of C, a point we’ll return to explore in later chapters. But, in\\npractice gradient descent often works extremely well, and in neural networks we’ll ﬁnd that\\nit’s a powerful way of minimizing the cost function, and so helping the net learn.\\nIndeed, there’s even a sense in which gradient descent is the optimal strategy for searching\\nfor a minimum. Let’s suppose that we’re trying to make a move ∆v in position so as to\\ndecrease C as much as possible. This is equivalent to minimizing ∆C ≈∇C ·∆v. We’ll\\nconstrain the size of the move so that ∥∆v∥= εfor some small ﬁxed ε> 0. In other words,\\nwe want a move that is a small step of a ﬁxed size, and we’re trying to ﬁnd the movement\\ndirection which decreases C as much as possible. It can be proved that the choice of ∆v\\nwhich minimizes ∇C ·∆v is ∆v = −η∇C, where η= ε/∥∇C∥is determined by the size\\nconstraint ∥∆v∥= ε. So gradient descent can be viewed as a way of taking small steps in\\nthe direction which does the most to immediately decrease C.\\nExercises\\n• Prove the assertion of the last paragraph. Hint: If you’re not already familiar with the\\nCauchy-Schwarz inequality, you may ﬁnd it helpful to familiarize yourself with it.\\n• I explained gradient descent when C is a function of two variables, and when it’s\\na function of more than two variables. What happens when C is a function of just\\none variable? Can you provide a geometric interpretation of what gradient descent is\\ndoing in the one-dimensional case?\\nPeople have investigated many variations of gradient descent, including variations\\nthat more closely mimic a real physical ball. These ball-mimicking variations have some\\nadvantages, but also have a major disadvantage: it turns out to be necessary to compute\\nsecond partial derivatives of C, and this can be quite costly . To see why it’s costly , suppose\\nwe want to compute all the second partial derivatives ∂2C/∂vj∂vk. If there are a million\\nsuch vj variables then we’d need to compute something like a trillion (i.e., a million squared)\\nsecond partial derivatives4! That’s going to be computationally costly . With that said, there\\nare tricks for avoiding this kind of problem, and ﬁnding alternatives to gradient descent is\\nan active area of investigation. But in this book we’ll use gradient descent (and variations)\\nas our main approach to learning in neural networks.\\nHow can we apply gradient descent to learn in a neural network? The idea is to use\\ngradient descent to ﬁnd the weights wk and biases bl which minimize the cost in Equation\\n1.6. To see how this works, let’s restate the gradient descent update rule, with the weights\\nand biases replacing the variables vj. In other words, our “position” now has componentswk\\nand bl , and the gradient vector ∇C has corresponding components ∂C/∂wk and ∂C/∂bl .\\n4Actually , more like half a trillion, since∂2C/∂vj∂vk = ∂2C/∂vk∂vj. Still, you get the point.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='07d6fd8e-e882-46bc-b2e4-616565d12580', embedding=None, metadata={'page_label': '22', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='22\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nWriting out the gradient descent update rule in terms of components, we have\\nwk → w′\\nk = wk −η∂C\\n∂wk\\n(1.16)\\nbl → b′\\nl = bl −η∂C\\n∂bl\\n. (1.17)\\nBy repeatedly applying this update rule we can “roll down the hill”, and hopefully ﬁnd a\\nminimum of the cost function. In other words, this is a rule which can be used to learn in a\\nneural network.\\nThere are a number of challenges in applying the gradient descent rule. We’ll look\\ninto those in depth in later chapters. But for now I just want to mention one problem.\\nTo understand what the problem is, let’s look back at the quadratic cost in Equation 1.6.\\nNotice that this cost function has the form C = 1\\nn\\n∑\\nx Cx , that is, it’s an average over costs\\nCx ≡∥y(x)−a∥2\\n2 for individual training examples. In practice, to compute the gradient ∇C we\\nneed to compute the gradients ∇Cx separately for each training input, x, and then average\\nthem, ∇C = 1\\nn\\n∑\\nx ∇Cx . Unfortunately , when the number of training inputs is very large this\\ncan take a long time, and learning thus occurs slowly .\\nAn idea called stochastic gradient descent can be used to speed up learning. The idea\\nis to estimate the gradient ∇C by computing ∇Cx for a small sample of randomly chosen\\ntraining inputs. By averaging over this small sample it turns out that we can quickly get a\\ngood estimate of the true gradient ∇C, and this helps speed up gradient descent, and thus\\nlearning.\\nTo make these ideas more precise, stochastic gradient descent works by randomly picking\\nout a small number m of randomly chosen training inputs. We’ll label those random training\\ninputs X1, X2, . . . ,Xm, and refer to them as a mini-batch. Provided the sample size m is large\\nenough we expect that the average value of the ∇CX j will be roughly equal to the average\\nover all ∇Cx , that is, ∑m\\nj=1 ∇CX j\\nm ≈\\n∑\\nx ∇Cx\\nn = ∇C, (1.18)\\nwhere the second sum is over the entire set of training data. Swapping sides we get\\n∇C ≈ 1\\nm\\nm∑\\nj=1\\n∇CX j , (1.19)\\nconﬁrming that we can estimate the overall gradient by computing gradients just for the\\nrandomly chosen mini-batch.\\nTo connect this explicitly to learning in neural networks, suppose wk and bl denote the\\nweights and biases in our neural network. Then stochastic gradient descent works by picking\\nout a randomly chosen mini-batch of training inputs, and training with those,\\nwk → w′\\nk = wk −η\\nm\\n∑\\nj\\n∂CX j\\n∂wk\\n(1.20)\\nbl → b′\\nl = bl −η\\nm\\n∑\\nj\\n∂CX j\\n∂bl\\n, (1.21)\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a33744bf-8631-4960-8bbd-558f8f4d7baf', embedding=None, metadata={'page_label': '23', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.5. Learning with gradient descent\\n\\x0c\\x0c\\x0c 23\\nwhere the sums are over all the training examples X j in the current mini-batch. Then we\\npick out another randomly chosen mini-batch and train with those. And so on, until we’ve\\nexhausted the training inputs, which is said to complete an epoch of training. At that point\\nwe start over with a new training epoch.\\nIncidentally , it’s worth noting that conventions vary about scaling of the cost function and\\nof mini-batch updates to the weights and biases. In Equation 1.6 we scaled the overall cost\\nfunction by a factor 1\\nn . People sometimes omit the 1\\nn , summing over the costs of individual\\ntraining examples instead of averaging. This is particularly useful when the total number\\nof training examples isn’t known in advance. This can occur if more training data is being\\ngenerated in real time, for instance. And, in a similar way , the mini-batch update rules 1.20\\nand 1.21 sometimes omit the 1\\nm term out the front of the sums. Conceptually this makes little\\ndifference, since it’s equivalent to rescaling the learning rateη. But when doing detailed\\ncomparisons of different work it’s worth watching out for.\\nWe can think of stochastic gradient descent as being like political polling: it’s much easier\\nto sample a small mini-batch than it is to apply gradient descent to the full batch, just as\\ncarrying out a poll is easier than running a full election. For example, if we have a training\\nset of size n=60,000, as in MNIST , and choose a mini-batch size of (say)m = 10, this means\\nwe’ll get a factor of 6,000 speedup in estimating the gradient! Of course, the estimate won’t\\nbe perfect – there will be statistical ﬂuctuations – but it doesn’t need to be perfect: all we\\nreally care about is moving in a general direction that will help decrease C, and that means\\nwe don’t need an exact computation of the gradient. In practice, stochastic gradient descent\\nis a commonly used and powerful technique for learning in neural networks, and it’s the\\nbasis for most of the learning techniques we’ll develop in this book.\\nExercize\\n• An extreme version of gradient descent is to use a mini-batch size of just 1. That is,\\ngiven a training input, x, we update our weights and biases according to the rules\\nwk →w′\\nk = wk −η∂Cx /∂wk and bl →b′\\nl = bl −η∂Cx /∂bl . Then we choose another\\ntraining input, and update the weights and biases again. And so on, repeatedly . This\\nprocedure is known as online, on-line, or incremental learning. In online learning, a\\nneural network learns from just one training input at a time (just as human beings\\ndo). Name one advantage and one disadvantage of online learning, compared to\\nstochastic gradient descent with a mini-batch size of, say , 20.\\nLet me conclude this section by discussing a point that sometimes bugs people new to gradient\\ndescent. In neural networks the cost C is, of course, a function of many variables – all the\\nweights and biases – and so in some sense deﬁnes a surface in a very high-dimensional\\nspace. Some people get hung up thinking: “Hey, I have to be able to visualize all these\\nextra dimensions”. And they may start to worry: “I can’t think in four dimensions, let\\nalone ﬁve (or ﬁve million)”. Is there some special ability they’re missing, some ability that\\n“real” supermathematicians have? Of course, the answer is no. Even most professional\\nmathematicians can’t visualize four dimensions especially well, if at all. The trick they use,\\ninstead, is to develop other ways of representing what’s going on. That’s exactly what we\\ndid above: we used an algebraic (rather than visual) representation of ∆C to ﬁgure out how\\nto move so as to decrease C. People who are good at thinking in high dimensions have a\\nmental library containing many different techniques along these lines; our algebraic trick is\\njust one example. Those techniques may not have the simplicity we’re accustomed to when\\nvisualizing three dimensions, but once you build up a library of such techniques, you can get\\npretty good at thinking in high dimensions. I won’t go into more detail here, but if you’re\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0e292203-5492-4f3d-a527-ae318e03d791', embedding=None, metadata={'page_label': '24', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='24\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\ninterested then you may enjoy reading this discussion of some of the techniques professional\\nmathematicians use to think in high dimensions. While some of the techniques discussed are\\nquite complex, much of the best content is intuitive and accessible, and could be mastered\\nby anyone.\\n1.6 Implementing our network to classify digits\\nAlright, let’s write a program that learns how to recognize handwritten digits, using stochastic\\ngradient descent and the MNIST training data. We’ll do this with a short Python (2.7) program,\\njust 74 lines of code! The ﬁrst thing we need is to get the MNIST data. If you’re a git user\\nthen you can obtain the data by cloning the code repository for this book,\\ngit clone https://github.com/mnielsen/neural -networks - and -deep -learning.git\\nIf you don’t use git then you can download the data and code here.\\nIncidentally, when I described the MNIST data earlier, I said it was split into 60,000\\ntraining images, and 10,000 test images. That’s the ofﬁcial MNIST description. Actually,\\nwe’re going to split the data a little differently . We’ll leave the test images as is, but split the\\n60,000-image MNIST training set into two parts: a set of 50,000 images, which we’ll use\\nto train our neural network, and a separate 10,000 image validation set. We won’t use the\\nvalidation data in this chapter, but later in the book we’ll ﬁnd it useful in ﬁguring out how to\\nset certain hyper-parameters of the neural network – things like the learning rate, and so on,\\nwhich aren’t directly selected by our learning algorithm. Although the validation data isn’t\\npart of the original MNIST speciﬁcation, many people use MNIST in this fashion, and the\\nuse of validation data is common in neural networks. When I refer to the “MNIST training\\ndata” from now on, I’ll be referring to our 50,000 image data set, not the original 60,000\\nimage data set5.\\nApart from the MNIST data we also need a Python library called Numpy , for doing fast\\nlinear algebra. If you don’t already have Numpy installed, you can get it here.\\nLet me explain the core features of the neural networks code, before giving a full listing,\\nbelow. The centerpiece is a Network class, which we use to represent a neural network.\\nHere’s the code we use to initialize a Network object:\\nclass Network( object ):\\ndef __init__(self , sizes):\\nself.num_layers = len (sizes)\\nself.sizes = sizes\\nself.biases = [np.random.randn(y, 1) for y in sizes [1:]]\\nself.weights = [np.random.randn(y, x) for x, y in zip (sizes[:-1], sizes [1:])]\\nIn this code, the list sizes contains the number of neurons in the respective layers. So, for\\nexample, if we want to create a Network object with 2 neurons in the ﬁrst layer, 3 neurons\\nin the second layer, and 1 neuron in the ﬁnal layer, we’d do this with the code:\\n5As noted earlier, the MNIST data set is based on two data sets collected by NIST , the United States’\\nNational Institute of Standards and Technology . To construct MNIST the NIST data sets were stripped\\ndown and put into a more convenient format by Yann LeCun, Corinna Cortes, and Christopher J. C.\\nBurges. See this link for more details. The data set in my repository is in a form that makes it easy to\\nload and manipulate the MNIST data in Python. I obtained this particular form of the data from the\\nLISA machine learning laboratory at the University of Montreal (link).\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='73da248b-6c59-47d1-b15c-60eb7f75366e', embedding=None, metadata={'page_label': '25', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.6. Implementing our network to classify digits\\n\\x0c\\x0c\\x0c 25\\nnet = Network([2, 3, 1])\\nThe biases and weights in the Network object are all initialized randomly , using the Numpy\\nnp.random.randn function to generate Gaussian distributions with mean 0 and standard\\ndeviation 1. This random initialization gives our stochastic gradient descent algorithm a\\nplace to start from. In later chapters we’ll ﬁnd better ways of initializing the weights and\\nbiases, but this will do for now. Note that the Network initialization code assumes that the\\nﬁrst layer of neurons is an input layer, and omits to set any biases for those neurons, since\\nbiases are only ever used in computing the outputs from later layers.\\nNote also that the biases and weights are stored as lists of Numpy matrices. So, for\\nexample net.weights[1] is a Numpy matrix storing the weights connecting the second and\\nthird layers of neurons. (It’s not the ﬁrst and second layers, since Python’s list indexing starts\\nat 0.) Since net.weights[1] is rather verbose, let’s just denote that matrixw. It’s a matrix\\nsuch that wjk is the weight for the connection between the k-th neuron in the second layer,\\nand the j-th neuron in the third layer. This ordering of thej and k indices may seem strange –\\nsurely it’d make more sense to swap thej and k indices around? The big advantage of using\\nthis ordering is that it means that the vector of activations of the third layer of neurons is:\\na′= σ(wa + b). (1.22)\\nThere’s quite a bit going on in this equation, so let’s unpack it piece by piece.a is the vector\\nof activations of the second layer of neurons. To obtaina′we multiply a by the weight matrix\\nw, and add the vector b of biases. We then apply the function σelementwise to every entry\\nin the vector wa + b6. It’s easy to verify that Equation 1.22 gives the same result as our\\nearlier rule, Equation 1.4, for computing the output of a sigmoid neuron.\\nExercise\\n• Write out Equation 1.22 in component form, and verify that it gives the same result\\nas the rule 1.4 for computing the output of a sigmoid neuron.\\nWith all this in mind, it’s easy to write code computing the output from a Network instance.\\nWe begin by deﬁning the sigmoid function:\\ndef sigmoid(z):\\nreturn 1.0/(1.0+np.exp(-z))\\nNote that when the input z is a vector or Numpy array, Numpy automatically applies the\\nfunction sigmoid elementwise, that is, in vectorized form.\\nWe then add a feedforward method to the Network class, which, given an input a for\\nthe network, returns the corresponding output7. All the method does is applies Equation\\n1.22 for each layer:\\ndef feedforward(self , a):\\n\"\"\"Return the output of the network if \"a\" is input.\"\"\"\\nfor b, w in zip (self.biases , self.weights):\\na = sigmoid(np.dot(w, a)+b)\\n6This is called vectorizing the function σ.\\n7It is assumed that the inputa is an (n, 1) Numpy ndarray , not a(n,) vector. Here, n is the number\\nof inputs to the network. If you try to use an (n,) vector as input you’ll get strange results. Although\\nusing an (n,) vector appears the more natural choice, using an (n, 1) ndarray makes it particularly\\neasy to modify the code to feedforward multiple inputs at once, and that is sometimes convenient.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6980241d-970e-449c-8cd6-5b9a9a9f836e', embedding=None, metadata={'page_label': '26', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='26\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nreturn a\\nOf course, the main thing we want our Network objects to do is to learn. To that end\\nwe’ll give them anSGD method which implements stochastic gradient descent. Here’s the\\ncode. It’s a little mysterious in a few places, but I’ll break it down below, after the listing.\\ndef SGD(self , training_data , epochs , mini_batch_size , eta , test_data=None):\\n\"\"\"Train the neural network using mini -batch stochastic gradient descent. The\\n\"training_data\" is a list of tuples \"(x, y)\" representing the training\\ninputs and the desired outputs. The other non -optional parameters are self -\\nexplanatory. If \"test_data\" is provided then the network will be evaluated\\nagainst the test data after each epoch , and partial progress printed out.\\nThis is useful for tracking progress , but slows things down substantially.\\n\"\"\"\\nif test_data:\\nn_test = len (test_data)\\nn = len (training_data)\\nfor j in xrange (epochs):\\nrandom.shuffle(training_data)\\nmini_batches = [training_data[k:k+mini_batch_size] for k in xrange (0, n,\\nmini_batch_size)]\\nfor mini_batch in mini_batches:\\nself.update_mini_batch(mini_batch , eta)\\nif test_data:\\nprint \"Epoch {0}: {1} / {2}\". format (j, self.evaluate(test_data), n_test)\\nelse :\\nprint \"Epoch {0} complete\". format (j)\\nThe training_data is a list of tuples (x, y) representing the training inputs and corre-\\nsponding desired outputs. The variables epochs and mini_batch_size are what you’d\\nexpect – the number of epochs to train for, and the size of the mini-batches to use when\\nsampling. eta is the learning rate, η. If the optional argument test_data is supplied, then\\nthe program will evaluate the network after each epoch of training, and print out partial\\nprogress. This is useful for tracking progress, but slows things down substantially .\\nThe code works as follows. In each epoch, it starts by randomly shufﬂing the training\\ndata, and then partitions it into mini-batches of the appropriate size. This is an easy way of\\nsampling randomly from the training data. Then for each mini_batch we apply a single\\nstep of gradient descent. This is done by the code self.update_mini_batch(mini_batch\\n, eta), which updates the network weights and biases according to a single iteration\\nof gradient descent, using just the training data in mini_batch. Here’s the code for the\\nupdate_mini_batch method:\\ndef update_mini_batch(self , mini_batch , eta):\\n\"\"\"Update the network’s weights and biases by applying gradient descent using\\nbackpropagation to a single mini batch. The \"mini_batch\" is a list of tuples\\n\"(x, y)\", and \"eta\" is the learning rate.\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\nfor x, y in mini_batch:\\ndelta_nabla_b , delta_nabla_w = self.backprop(x, y)\\nnabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)]\\nnabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)]\\nself.weights = [w-(eta/ len (mini_batch))*nw for w, nw in zip (self.weights ,\\nnabla_w)]\\nself.biases = [b-(eta/ len (mini_batch))*nb for b, nb in zip (self.biases , nabla_b\\n)]\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8f0b2120-ad9d-463f-84b6-07552cfc83c7', embedding=None, metadata={'page_label': '27', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.6. Implementing our network to classify digits\\n\\x0c\\x0c\\x0c 27\\nMost of the work is done by the line\\ndelta_nabla_b , delta_nabla_w = self.backprop(x, y)\\nThis invokes something called the backpropagation algorithm, which is a fast way of comput-\\ning the gradient of the cost function. So update_mini_batch works simply by computing\\nthese gradients for every training example in the mini_batch, and then updating self\\n.weights and self.biases appropriately .\\nI’m not going to show the code for self.backprop right now. We’ll study how back-\\npropagation works in the next chapter, including the code forself.backprop. For now, just\\nassume that it behaves as claimed, returning the appropriate gradient for the cost associated\\nto the training example x.\\nLet’s look at the full program, including the documentation strings, which I omitted\\nabove. Apart from self.backprop the program is self-explanatory – all the heavy lift-\\ning is done in self.SGD and self.update_mini_batch, which we’ve already discussed.\\nThe self.backprop method makes use of a few extra functions to help in computing the\\ngradient, namely sigmoid_prime, which computes the derivative of the σfunction, and\\nself.cost_derivative , which I won’t describe here. You can get the gist of these (and\\nperhaps the details) just by looking at the code and documentation strings. We’ll look at\\nthem in detail in the next chapter. Note that while the program appears lengthy , much of the\\ncode is documentation strings intended to make the code easy to understand. In fact, the\\nprogram contains just 74 lines of non-whitespace, non-comment code. All the code may be\\nfound on GitHub here.\\n\"\"\"\\nnetwork.py\\n~~~~~~~~~~\\nA module to implement the stochastic gradient descent learning\\nalgorithm for a feedforward neural network. Gradients are calculated\\nusing backpropagation. Note that I have focused on making the code\\nsimple , easily readable , and easily modifiable. It is not optimized ,\\nand omits many desirable features.\\n\"\"\"\\n#### Libraries\\n# Standard library\\nimport random\\n# Third -party libraries\\nimport numpy as np\\nclass Network( object ):\\ndef __init__(self , sizes):\\n\"\"\"The list ‘‘sizes ‘‘ contains the number of neurons in the\\nrespective layers of the network. For example , if the list\\nwas [2, 3, 1] then it would be a three -layer network , with the\\nfirst layer containing 2 neurons , the second layer 3 neurons ,\\nand the third layer 1 neuron. The biases and weights for the\\nnetwork are initialized randomly , using a Gaussian\\ndistribution with mean 0, and variance 1. Note that the first\\nlayer is assumed to be an input layer , and by convention we\\nwon’t set any biases for those neurons , since biases are only\\never used in computing the outputs from later layers.\"\"\"\\nself.num_layers = len (sizes)\\nself.sizes = sizes\\nself.biases = [np.random.randn(y, 1) for y in sizes [1:]]\\nself.weights = [np.random.randn(y, x) for x, y in zip (sizes[:-1], sizes [1:])]\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1d56e687-d16b-4869-b512-a7a32e60171e', embedding=None, metadata={'page_label': '28', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='28\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\ndef feedforward(self , a):\\n\"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\"\\nfor b, w in zip (self.biases , self.weights):\\na = sigmoid(np.dot(w, a)+b)\\nreturn a\\ndef SGD(self , training_data , epochs , mini_batch_size , eta , test_data=None):\\n\"\"\"Train the neural network using mini -batch stochastic\\ngradient descent. The ‘‘training_data ‘‘ is a list of tuples\\n‘‘(x, y)‘‘ representing the training inputs and the desired\\noutputs. The other non -optional parameters are\\nself -explanatory. If ‘‘test_data ‘‘ is provided then the\\nnetwork will be evaluated against the test data after each\\nepoch , and partial progress printed out. This is useful for\\ntracking progress , but slows things down substantially.\"\"\"\\nif test_data:\\nn_test = len (test_data)\\nn = len (training_data)\\nfor j in xrange (epochs):\\nrandom.shuffle(training_data)\\nmini_batches = [\\ntraining_data[k:k+mini_batch_size]\\nfor k in xrange (0, n, mini_batch_size)]\\nfor mini_batch in mini_batches:\\nself.update_mini_batch(mini_batch , eta)\\nif test_data:\\nprint \"Epoch {0}: {1} / {2}\". format (\\nj, self.evaluate(test_data), n_test)\\nelse :\\nprint \"Epoch {0} complete\". format (j)\\ndef update_mini_batch(self , mini_batch , eta):\\n\"\"\"Update the network’s weights and biases by applying\\ngradient descent using backpropagation to a single mini batch.\\nThe ‘‘mini_batch ‘‘ is a list of tuples ‘‘(x, y)‘‘, and ‘‘eta ‘‘\\nis the learning rate.\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\nfor x, y in mini_batch:\\ndelta_nabla_b , delta_nabla_w = self.backprop(x, y)\\nnabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)]\\nnabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)]\\nself.weights = [w-(eta/ len (mini_batch))*nw\\nfor w, nw in zip (self.weights , nabla_w)]\\nself.biases = [b-(eta/ len (mini_batch))*nb\\nfor b, nb in zip (self.biases , nabla_b)]\\ndef backprop(self , x, y):\\n\"\"\"Return a tuple ‘‘(nabla_b , nabla_w)‘‘ representing the\\ngradient for the cost function C_x. ‘‘nabla_b ‘‘ and\\n‘‘nabla_w ‘‘ are layer -by-layer lists of numpy arrays , similar\\nto ‘‘self.biases ‘‘ and ‘‘self.weights ‘‘.\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\n# feedforward\\nactivation = x\\nactivations = [x] # list to store all the activations , layer by layer\\nzs = [] # list to store all the z vectors , layer by layer\\nfor b, w in zip (self.biases , self.weights):\\nz = np.dot(w, activation)+b\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='67fd32c4-2aac-45f5-a941-97e770279c0c', embedding=None, metadata={'page_label': '29', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.6. Implementing our network to classify digits\\n\\x0c\\x0c\\x0c 29\\nzs.append(z)\\nactivation = sigmoid(z)\\nactivations.append(activation)\\n# backward pass\\ndelta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\\nnabla_b[-1] = delta\\nnabla_w[-1] = np.dot(delta , activations[-2].transpose())\\n# Note that the variable l in the loop below is used a little\\n# differently to the notation in Chapter 2 of the book. Here ,\\n# l = 1 means the last layer of neurons , l = 2 is the\\n# second -last layer , and so on. It’s a renumbering of the\\n# scheme in the book , used here to take advantage of the fact\\n# that Python can use negative indices in lists.\\nfor l in xrange (2, self.num_layers):\\nz = zs[-l]\\nsp = sigmoid_prime(z)\\ndelta = np.dot(self.weights[-l+1]. transpose(), delta) * sp\\nnabla_b[-l] = delta\\nnabla_w[-l] = np.dot(delta , activations[-l-1]. transpose())\\nreturn (nabla_b , nabla_w)\\ndef evaluate(self , test_data):\\n\"\"\"Return the number of test inputs for which the neural\\nnetwork outputs the correct result. Note that the neural\\nnetwork’s output is assumed to be the index of whichever\\nneuron in the final layer has the highest activation.\"\"\"\\ntest_results = [(np.argmax(self.feedforward(x)), y)\\nfor (x, y) in test_data]\\nreturn sum (int (x == y) for (x, y) in test_results)\\ndef cost_derivative(self , output_activations , y):\\n\"\"\"Return the vector of partial derivatives \\\\partial C_x /\\n\\\\partial a for the output activations.\"\"\"\\nreturn (output_activations -y)\\n#### Miscellaneous functions\\ndef sigmoid(z):\\n\"\"\"The sigmoid function.\"\"\"\\nreturn 1.0/(1.0+np.exp(-z))\\ndef sigmoid_prime(z):\\n\"\"\"Derivative of the sigmoid function.\"\"\"\\nreturn sigmoid(z)*(1-sigmoid(z))\\nHow well does the program recognize handwritten digits? Well, let’s start by loading in the\\nMNIST data. I’ll do this using a little helper program,mnist_loader.py, to be described\\nbelow. We execute the following commands in a Python shell,\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\nOf course, this could also be done in a separate Python program, but if you’re following\\nalong it’s probably easiest to do in a Python shell.\\nAfter loading the MNIST data, we’ll set up a Network with 30 hidden neurons. We do\\nthis after importing the Python program listed above, which is named network,\\n>>> import network\\n>>> net = network.Network([784, 30, 10])\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5756f8ce-6de1-41fc-bbb0-bf7e4da12138', embedding=None, metadata={'page_label': '30', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='30\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nFinally , we’ll use stochastic gradient descent to learn from the MNISTtraining_data over\\n30 epochs, with a mini-batch size of 10, and a learning rate of η= 3.0,\\n>>> net.SGD(training_data , 30, 10, 3.0, test_data=test_data)\\nNote that if you’re running the code as you read along, it will take some time to execute –\\nfor a typical machine (as of 2015) it will likely take a few minutes to run. I suggest you set\\nthings running, continue to read, and periodically check the output from the code. If you’re\\nin a rush you can speed things up by decreasing the number of epochs, by decreasing the\\nnumber of hidden neurons, or by using only part of the training data. Note that production\\ncode would be much, much faster: these Python scripts are intended to help you understand\\nhow neural nets work, not to be high-performance code! And, of course, once we’ve trained\\na network it can be run very quickly indeed, on almost any computing platform. For example,\\nonce we’ve learned a good set of weights and biases for a network, it can easily be ported\\nto run in Javascript in a web browser, or as a native app on a mobile device. In any case,\\nhere is a partial transcript of the output of one training run of the neural network. The\\ntranscript shows the number of test images correctly recognized by the neural network after\\neach epoch of training. As you can see, after just a single epoch this has reached 9,129 out\\nof 10,000, and the number continues to grow,\\nEpoch 0: 9129 / 10000\\nEpoch 1: 9295 / 10000\\nEpoch 2: 9348 / 10000\\n...\\nEpoch 27: 9528 / 10000\\nEpoch 28: 9542 / 10000\\nEpoch 29: 9534 / 10000\\nThat is, the trained network gives us a classiﬁcation rate of about 95 percent – 95.42 percent\\nat its peak (“Epoch 28”)! That’s quite encouraging as a ﬁrst attempt. I should warn you,\\nhowever, that if you run the code then your results are not necessarily going to be quite the\\nsame as mine, since we’ll be initializing our network using (different) random weights and\\nbiases. To generate results in this chapter I’ve taken best-of-three runs.\\nLet’s rerun the above experiment, changing the number of hidden neurons to 100. As\\nwas the case earlier, if you’re running the code as you read along, you should be warned that\\nit takes quite a while to execute (on my machine this experiment takes tens of seconds for\\neach training epoch), so it’s wise to continue reading in parallel while the code executes.\\n>>> net = network.Network([784, 100, 10])\\n>>> net.SGD(training_data , 30, 10, 3.0, test_data=test_data)\\nSure enough, this improves the results to 96.59 percent. At least in this case, using more\\nhidden neurons helps us get better results8\\nOf course, to obtain these accuracies I had to make speciﬁc choices for the number of\\nepochs of training, the mini-batch size, and the learning rate,η. As I mentioned above, these\\nare known as hyper-parameters for our neural network, in order to distinguish them from\\nthe parameters (weights and biases) learnt by our learning algorithm. If we choose our\\n8Reader feedback indicates quite some variation in results for this experiment, and some training\\nruns give results quite a bit worse. Using the techniques introduced in chapter 3 will greatly reduce the\\nvariation in performance across different training runs for our networks.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a297ca3e-d065-48da-9b79-2802945b0e57', embedding=None, metadata={'page_label': '31', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.6. Implementing our network to classify digits\\n\\x0c\\x0c\\x0c 31\\nhyper-parameters poorly, we can get bad results. Suppose, for example, that we’d chosen\\nthe learning rate to be η= 0.001,\\n>>> net = network.Network([784, 100, 10])\\n>>> net.SGD(training_data , 30, 10, 0.001, test_data=test_data)\\nThe results are much less encouraging,\\nEpoch 0: 1139 / 10000\\nEpoch 1: 1136 / 10000\\nEpoch 2: 1135 / 10000\\n...\\nEpoch 27: 2101 / 10000\\nEpoch 28: 2123 / 10000\\nEpoch 29: 2142 / 10000\\nHowever, you can see that the performance of the network is getting slowly better over time.\\nThat suggests increasing the learning rate, say to η= 0.01. If we do that, we get better\\nresults, which suggests increasing the learning rate again. (If making a change improves\\nthings, try doing more!) If we do that several times over, we’ll end up with a learning\\nrate of something like η= 1.0 (and perhaps ﬁne tune to 3.0), which is close to our earlier\\nexperiments. So even though we initially made a poor choice of hyper-parameters, we at\\nleast got enough information to help us improve our choice of hyper-parameters. In general,\\ndebugging a neural network can be challenging. This is especially true when the initial\\nchoice of hyper-parameters produces results no better than random noise. Suppose we try\\nthe successful 30 hidden neuron network architecture from earlier, but with the learning\\nrate changed to η= 100.0:\\n>>> net = network.Network([784, 30, 10])\\n>>> net.SGD(training_data , 30, 10, 100.0, test_data=test_data)\\nAt this point we’ve actually gone too far, and the learning rate is too high:\\nEpoch 0: 1009 / 10000\\nEpoch 1: 1009 / 10000\\nEpoch 2: 1009 / 10000\\nEpoch 3: 1009 / 10000\\n...\\nEpoch 27: 982 / 10000\\nEpoch 28: 982 / 10000\\nEpoch 29: 982 / 10000\\nNow imagine that we were coming to this problem for the ﬁrst time. Of course, we know\\nfrom our earlier experiments that the right thing to do is to decrease the learning rate. But\\nif we were coming to this problem for the ﬁrst time then there wouldn’t be much in the\\noutput to guide us on what to do. We might worry not only about the learning rate, but\\nabout every other aspect of our neural network. We might wonder if we’ve initialized the\\nweights and biases in a way that makes it hard for the network to learn? Or maybe we don’t\\nhave enough training data to get meaningful learning? Perhaps we haven’t run for enough\\nepochs? Or maybe it’s impossible for a neural network with this architecture to learn to\\nrecognize handwritten digits? Maybe the learning rate is too low? Or, maybe, the learning\\nrate is too high? When you’re coming to a problem for the ﬁrst time, you’re not always sure.\\nThe lesson to take away from this is that debugging a neural network is not trivial, and,\\njust as for ordinary programming, there is an art to it. You need to learn that art of debugging\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ee4d5f2b-4d03-4650-b763-21e52350e1ee', embedding=None, metadata={'page_label': '32', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='32\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nin order to get good results from neural networks. More generally, we need to develop\\nheuristics for choosing good hyper-parameters and a good architecture. We’ll discuss all\\nthese at length through the book, including how I chose the hyper-parameters above.\\nExercise\\n• Try creating a network with just two layers – an input and an output layer, no hidden\\nlayer – with 784 and 10 neurons, respectively. Train the network using stochastic\\ngradient descent. What classiﬁcation accuracy can you achieve?\\nEarlier, I skipped over the details of how the MNIST data is loaded. It’s pretty straightforward.\\nFor completeness, here’s the code. The data structures used to store the MNIST data are\\ndescribed in the documentation strings – it’s straightforward stuff, tuples and lists of Numpy\\nndarray objects (think of them as vectors if you’re not familiar with ndarrays):\\n\"\"\"\\nmnist_loader\\n~~~~~~~~~~~~\\nA library to load the MNIST image data. For details of the data\\nstructures that are returned , see the doc strings for ‘‘load_data ‘‘\\nand ‘‘load_data_wrapper ‘‘. In practice , ‘‘load_data_wrapper ‘‘ is the\\nfunction usually called by our neural network code.\\n\"\"\"\\n#### Libraries\\n# Standard library\\nimport cPickle\\nimport gzip\\n# Third -party libraries\\nimport numpy as np\\ndef load_data():\\n\"\"\"Return the MNIST data as a tuple containing the training data , the\\nvalidation data , and the test data.\\nThe ‘‘training_data ‘‘ is returned as a tuple with two entries. The first entry\\ncontains the actual training images. This is a\\nnumpy ndarray with 50,000 entries. Each entry is, in turn , a numpy ndarray\\nwith 784 values , representing the 28 * 28 = 784\\npixels in a single MNIST image.\\nThe second entry in the ‘‘training_data ‘‘ tuple is a numpy ndarray containing\\n50,000 entries. Those entries are just the digit\\nvalues (0...9) for the corresponding images contained in the first entry of\\nthe tuple.\\nThe ‘‘validation_data ‘‘ and ‘‘test_data ‘‘ are similar , except each contains\\nonly 10,000 images.\\nThis is a nice data format , but for use in neural networks it’s helpful to\\nmodify the format of the ‘‘training_data ‘‘ a little.\\nThat’s done in the wrapper function ‘‘load_data_wrapper()‘‘, see below.\\n\"\"\"\\nf = gzip. open (’../data/mnist.pkl.gz’, ’rb’)\\ntraining_data , validation_data , test_data = cPickle.load(f)\\nf.close()\\nreturn (training_data , validation_data , test_data)\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2e92800e-76a9-47da-b073-efdef7edf0c3', embedding=None, metadata={'page_label': '33', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.6. Implementing our network to classify digits\\n\\x0c\\x0c\\x0c 33\\ndef load_data_wrapper():\\n\"\"\"Return a tuple containing ‘‘(training_data , validation_data ,\\ntest_data)‘‘. Based on ‘‘load_data ‘‘, but the format is more\\nconvenient for use in our implementation of neural networks.\\nIn particular , ‘‘training_data ‘‘ is a list containing 50,000\\n2-tuples ‘‘(x, y)‘‘. ‘‘x‘‘ is a 784-dimensional numpy.ndarray\\ncontaining the input image. ‘‘y‘‘ is a 10-dimensional\\nnumpy.ndarray representing the unit vector corresponding to the\\ncorrect digit for ‘‘x‘‘.\\n‘‘validation_data ‘‘ and ‘‘test_data ‘‘ are lists containing 10,000\\n2-tuples ‘‘(x, y)‘‘. In each case , ‘‘x‘‘ is a 784-dimensional\\nnumpy.ndarry containing the input image , and ‘‘y‘‘ is the\\ncorresponding classification , i.e., the digit values (integers)\\ncorresponding to ‘‘x‘‘.\\nObviously , this means we’re using slightly different formats for\\nthe training data and the validation / test data. These formats\\nturn out to be the most convenient for use in our neural network\\ncode.\"\"\"\\ntr_d , va_d , te_d = load_data()\\ntraining_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\\ntraining_results = [vectorized_result(y) for y in tr_d[1]]\\ntraining_data = zip (training_inputs , training_results)\\nvalidation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\\nvalidation_data = zip (validation_inputs , va_d[1])\\ntest_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\\ntest_data = zip (test_inputs , te_d[1])\\nreturn (training_data , validation_data , test_data)\\ndef vectorized_result(j):\\n\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\\nposition and zeroes elsewhere. This is used to convert a digit\\n(0...9) into a corresponding desired output from the neural\\nnetwork.\"\"\"\\ne = np.zeros((10, 1))\\ne[j] = 1.0\\nreturn e\\nI said above that our program gets pretty good results. What does that mean? Good compared\\nto what? It’s informative to have some simple (non-neural-network) baseline tests to compare\\nagainst, to understand what it means to perform well. The simplest baseline of all, of course,\\nis to randomly guess the digit. That’ll be right about ten percent of the time. We’re doing\\nmuch better than that!\\nWhat about a less trivial baseline? Let’s try an extremely simple idea: we’ll look at how\\ndark an image is. For instance, an image of a 2 will typically be quite a bit darker than an\\nimage of a 1, just because more pixels are blackened out, as the following examples illustrate:\\nThis suggests using the training data to compute average darknesses for each digit, 0,1,2,. . .,9.\\nWhen presented with a new image, we compute how dark the image is, and then guess that\\nit’s whichever digit has the closest average darkness. This is a simple procedure, and is easy\\nto code up, so I won’t explicitly write out the code – if you’re interested it’s in the GitHub\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='242adab4-a21a-4f5d-8f15-eafa174bd8a2', embedding=None, metadata={'page_label': '34', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='34\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\nrepository. But it’s a big improvement over random guessing, getting 2,225 of the 10,000\\ntest images correct, i.e., 22.25 percent accuracy .\\nIt’s not difﬁcult to ﬁnd other ideas which achieve accuracies in the 20 to 50 percent\\nrange. If you work a bit harder you can get up over 50 percent. But to get much higher\\naccuracies it helps to use established machine learning algorithms. Let’s try using one of the\\nbest known algorithms, the support vector machine or SVM. If you’re not familiar with SVMs,\\nnot to worry , we’re not going to need to understand the details of how SVMs work. Instead,\\nwe’ll use a Python library called scikit-learn, which provides a simple Python interface to a\\nfast C-based library for SVMs known as LIBSVM.\\nIf we run scikit-learn’s SVM classiﬁer using the default settings, then it gets 9,435 of\\n10,000 test images correct. (The code is available here.) That’s a big improvement over\\nour naive approach of classifying an image based on how dark it is. Indeed, it means that\\nthe SVM is performing roughly as well as our neural networks, just a little worse. In later\\nchapters we’ll introduce new techniques that enable us to improve our neural networks so\\nthat they perform much better than the SVM.\\nThat’s not the end of the story , however. The 9,435 of 10,000 result is for scikit-learn’s\\ndefault settings for SVMs. SVMs have a number of tunable parameters, and it’s possible to\\nsearch for parameters which improve this out-of-the-box performance. I won’t explicitly do\\nthis search, but instead refer you to this blog post by Andreas Müller if you’d like to know\\nmore. Mueller shows that with some work optimizing the SVM’s parameters it’s possible to\\nget the performance up above 98.5 percent accuracy . In other words, a well-tuned SVM only\\nmakes an error on about one digit in 70. That’s pretty good! Can neural networks do better?\\nIn fact, they can. At present, well-designed neural networks outperform every other\\ntechnique for solving MNIST , including SVMs. The current (2013) record is classifying 9,979\\nof 10,000 images correctly. This was done by Li Wan, Matthew Zeiler, Sixin Zhang, Yann\\nLeCun, and Rob Fergus. We’ll see most of the techniques they used later in the book. At\\nthat level the performance is close to human-equivalent, and is arguably better, since quite a\\nfew of the MNIST images are difﬁcult even for humans to recognize with conﬁdence, for\\nexample:\\nI trust you’ll agree that those are tough to classify! With images like these in the MNIST\\ndata set it’s remarkable that neural networks can accurately classify all but 21 of the 10,000\\ntest images. Usually, when programming we believe that solving a complicated problem\\nlike recognizing the MNIST digits requires a sophisticated algorithm. But even the neural\\nnetworks in the Wan et al paper just mentioned involve quite simple algorithms, variations\\non the algorithm we’ve seen in this chapter. All the complexity is learned, automatically,\\nfrom the training data. In some sense, the moral of both our results and those in more\\nsophisticated papers, is that for some problems:\\nsophisticated algorithm ≤simple learning algorithm + good training data.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2a57608a-bd51-4a29-9faa-162107837042', embedding=None, metadata={'page_label': '35', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.7. Toward deep learning\\n\\x0c\\x0c\\x0c 35\\nFigure 1.1: Credits: 1. Ester Inbar. 2. Unknown. 3. NASA, ESA, G. Illingworth, D. Magee, and P . Oesch (University\\nof California, Santa Cruz), R. Bouwens (Leiden University), and the HUDF09 Team.\\n1.7 Toward deep learning\\nWhile our neural network gives impressive performance, that performance is somewhat\\nmysterious. The weights and biases in the network were discovered automatically . And that\\nmeans we don’t immediately have an explanation of how the network does what it does.\\nCan we ﬁnd some way to understand the principles by which our network is classifying\\nhandwritten digits? And, given such principles, can we do better?\\nTo put these questions more starkly , suppose that a few decades hence neural networks\\nlead to artiﬁcial intelligence (AI). Will we understand how such intelligent networks work?\\nPerhaps the networks will be opaque to us, with weights and biases we don’t understand,\\nbecause they’ve been learned automatically . In the early days of AI research people hoped\\nthat the effort to build an AI would also help us understand the principles behind intelligence\\nand, maybe, the functioning of the human brain. But perhaps the outcome will be that we\\nend up understanding neither the brain nor how artiﬁcial intelligence works!\\nTo address these questions, let’s think back to the interpretation of artiﬁcial neurons that\\nI gave at the start of the chapter, as a means of weighing evidence. Suppose we want to\\ndetermine whether an image shows a human face or not:\\nWe could attack this problem the same way we attacked handwriting recognition – by\\nusing the pixels in the image as input to a neural network, with the output from the network\\na single neuron indicating either “Yes, it’s a face” or “No, it’s not a face”.\\nLet’s suppose we do this, but that we’re not using a learning algorithm. Instead, we’re\\ngoing to try to design a network by hand, choosing appropriate weights and biases. How\\nmight we go about it? Forgetting neural networks entirely for the moment, a heuristic we\\ncould use is to decompose the problem into sub-problems: does the image have an eye in\\nthe top left? Does it have an eye in the top right? Does it have a nose in the middle? Does it\\nhave a mouth in the bottom middle? Is there hair on top? And so on.\\nIf the answers to several of these questions are “yes”, or even just “probably yes”, then\\nwe’d conclude that the image is likely to be a face. Conversely , if the answers to most of the\\nquestions are “no”, then the image probably isn’t a face.\\nOf course, this is just a rough heuristic, and it suffers from many deﬁciencies. Maybe\\nthe person is bald, so they have no hair. Maybe we can only see part of the face, or the\\nface is at an angle, so some of the facial features are obscured. Still, the heuristic suggests\\nthat if we can solve the sub-problems using neural networks, then perhaps we can build a\\nneural network for face-detection, by combining the networks for the sub-problems. Here’s\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cdb599aa-240f-4cb5-a47d-bc3373bad3c9', embedding=None, metadata={'page_label': '36', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='36\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\na possible architecture, with rectangles denoting the sub-networks. Note that this isn’t\\nintended as a realistic approach to solving the face-detection problem; rather, it’s to help us\\nbuild intuition about how networks function. Here’s the architecture:\\nIt’s also plausible that the sub-networks can be decomposed. Suppose we’re considering the\\nquestion: “Is there an eye in the top left?” This can be decomposed into questions such as:\\n“Is there an eyebrow?”; “ Are there eyelashes?”; “Is there an iris?”; and so on. Of course, these\\nquestions should really include positional information, as well–“Is the eyebrow in the top\\nleft, and above the iris?”, that kind of thing – but let’s keep it simple. The network to answer\\nthe question “Is there an eye in the top left?” can now be decomposed:\\nThose questions too can be broken down, further and further through multiple layers.\\nUltimately, we’ll be working with sub-networks that answer questions so simple they can\\neasily be answered at the level of single pixels. Those questions might, for example, be\\nabout the presence or absence of very simple shapes at particular points in the image. Such\\nquestions can be answered by single neurons connected to the raw pixels in the image.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2885f046-f3c9-4693-b75e-2b799e44a8ca', embedding=None, metadata={'page_label': '37', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.7. Toward deep learning\\n\\x0c\\x0c\\x0c 37\\nThe end result is a network which breaks down a very complicated question – does this\\nimage show a face or not – into very simple questions answerable at the level of single pixels.\\nIt does this through a series of many layers, with early layers answering very simple and\\nspeciﬁc questions about the input image, and later layers building up a hierarchy of ever\\nmore complex and abstract concepts. Networks with this kind of many-layer structure – two\\nor more hidden layers – are called deep neural networks.\\nOf course, I haven’t said how to do this recursive decomposition into sub-networks. It\\ncertainly isn’t practical to hand-design the weights and biases in the network. Instead, we’d\\nlike to use learning algorithms so that the network can automatically learn the weights\\nand biases – and thus, the hierarchy of concepts – from training data. Researchers in the\\n1980s and 1990s tried using stochastic gradient descent and backpropagation to train deep\\nnetworks. Unfortunately , except for a few special architectures, they didn’t have much luck.\\nThe networks would learn, but very slowly , and in practice often too slowly to be useful.\\nSince 2006, a set of techniques has been developed that enable learning in deep neural\\nnets. These deep learning techniques are based on stochastic gradient descent and back-\\npropagation, but also introduce new ideas. These techniques have enabled much deeper\\n(and larger) networks to be trained – people now routinely train networks with 5 to 10\\nhidden layers. And, it turns out that these perform far better on many problems than shallow\\nneural networks, i.e., networks with just a single hidden layer. The reason, of course, is\\nthe ability of deep nets to build up a complex hierarchy of concepts. It’s a bit like the way\\nconventional programming languages use modular design and ideas about abstraction to\\nenable the creation of complex computer programs. Comparing a deep network to a shallow\\nnetwork is a bit like comparing a programming language with the ability to make function\\ncalls to a stripped down language with no ability to make such calls. Abstraction takes a\\ndifferent form in neural networks than it does in conventional programming, but it’s just as\\nimportant.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e47eeec3-1c68-4982-b6ac-99e089255b8e', embedding=None, metadata={'page_label': '38', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='38\\n\\x0c\\x0c\\x0c Using neural nets to recognize handwritten digits\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='92f33ada-0e92-41f2-931f-f77eb5ac9d7b', embedding=None, metadata={'page_label': '39', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 39\\n22222\\nHow the backpropagation\\nalgorithm works\\nIn the last chapter we saw how neural networks can learn their weights and biases using\\nthe gradient descent algorithm. There was, however, a gap in our explanation: we didn’t\\ndiscuss how to compute the gradient of the cost function. That’s quite a gap! In this\\nchapter I’ll explain a fast algorithm for computing such gradients, an algorithm known as\\nbackpropagation.\\nThe backpropagation algorithm was originally introduced in the 1970s, but its importance\\nwasn’t fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton,\\nand Ronald Williams. That paper describes several neural networks where backpropagation\\nworks far faster than earlier approaches to learning, making it possible to use neural nets to\\nsolve problems which had previously been insoluble. Today , the backpropagation algorithm\\nis the workhorse of learning in neural networks.\\nThis chapter is more mathematically involved than the rest of the book. If you’re not crazy\\nabout mathematics you may be tempted to skip the chapter, and to treat backpropagation as\\na black box whose details you’re willing to ignore. Why take the time to study those details?\\nThe reason, of course, is understanding. At the heart of backpropagation is an expression\\nfor the partial derivative ∂C/∂w of the cost function C with respect to any weight w (or bias\\nb) in the network. The expression tells us how quickly the cost changes when we change the\\nweights and biases. And while the expression is somewhat complex, it also has a beauty to it,\\nwith each element having a natural, intuitive interpretation. And so backpropagation isn’t\\njust a fast algorithm for learning. It actually gives us detailed insights into how changing the\\nweights and biases changes the overall behaviour of the network. That’s well worth studying\\nin detail.\\nWith that said, if you want to skim the chapter, or jump straight to the next chapter, that’s\\nﬁne. I’ve written the rest of the book to be accessible even if you treat backpropagation as a\\nblack box. There are, of course, points later in the book where I refer back to results from\\nthis chapter. But at those points you should still be able to understand the main conclusions,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8acafa10-f741-44df-90ff-aae0ff167ccc', embedding=None, metadata={'page_label': '40', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='40\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\neven if you don’t follow all the reasoning.\\n2.1 Warm up: a fast matrix-based approach to computing the\\noutput from a neural network\\nBefore discussing backpropagation, let’s warm up with a fast matrix-based algorithm to\\ncompute the output from a neural network. We actually already brieﬂy saw this algorithm\\nnear the end of the last chapter (section 1.6), but I described it quickly , so it’s worth revisiting\\nin detail. In particular, this is a good way of getting comfortable with the notation used in\\nbackpropagation, in a familiar context.\\nLet’s begin with a notation which lets us refer to weights in the network in an unam-\\nbiguous way . We’ll usewl\\njk to denote the weight for the connection from the k-th neuron in\\nthe (l −1)-th layer to the j-th neuron in the l-th layer. So, for example, the diagram below\\nshows the weight on a connection from the fourth neuron in the second layer to the second\\nneuron in the third layer of a network:\\nThis notation is cumbersome at ﬁrst, and it does take some work to master. But with a little\\neffort you’ll ﬁnd the notation becomes easy and natural. One quirk of the notation is the\\nordering of the j and k indices. You might think that it makes more sense to use j to refer\\nto the input neuron, and k to the output neuron, not vice versa, as is actually done. I’ll\\nexplain the reason for this quirk below. We use a similar notation for the network’s biases\\nand activations. Explicitly , we usebl\\nj for the bias of the j-th neuron in the l-th layer. And we\\nuse al\\nj for the activation of the j-th neuron in the l-th layer. The following diagram shows\\nexamples of these notations in use:\\nWith these notations, the activation al\\nj of the j-th neuron in the l-th layer is related to the\\nactivations in the (l −1)-th layer by the equation (compare Equation 1.4 and surrounding\\ndiscussion in the last chapter)\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='af8743ba-adc7-49e1-a889-4e62a8cfc777', embedding=None, metadata={'page_label': '41', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1. Warm up: a fast matrix-based approach to computing the output from a neural network\\n\\x0c\\x0c\\x0c 41\\nal\\nj = σ\\n\\x82∑\\nk\\nwl\\njk al−1\\nk + bl\\nj\\n\\x8c\\n, (2.1)\\nwhere the sum is over all neurons k in the (l −1)-th layer. To rewrite this expression in a\\nmatrix form we deﬁne a weight matrix wl for each layer, l. The entries of the weight matrix\\nwl are just the weights connecting to the l-th layer of neurons, that is, the entry in the j-th\\nrow and k-th column is wl\\njk . Similarly, for each layer l we deﬁne a bias vector, bl . You can\\nprobably guess how this works – the components of the bias vector are just the values bl\\nj,\\none component for each neuron in the l-th layer. And ﬁnally , we deﬁne an activation vector\\nal whose components are the activations al\\nj. The last ingredient we need to rewrite 2.1 in a\\nmatrix form is the idea of vectorizing a function such as σ. We met vectorization brieﬂy in\\nthe last chapter, but to recap, the idea is that we want to apply a function such as σto every\\nelement in a vector v. We use the obvious notation σ(v) to denote this kind of elementwise\\napplication of a function. That is, the components of σ(v) are just σ(v)j = σ(vj). As an\\nexample, if we have the function f (x) =x2 then the vectorized form of f has the effect\\nf\\n\\x82\\x96\\n2\\n3\\n\\x99\\x8c\\n=\\n\\x96\\nf (2)\\nf (3)\\n\\x99\\n=\\n\\x96\\n4\\n9\\n\\x99\\n, (2.2)\\nthat is, the vectorized f just squares every element of the vector.\\nWith these notations in mind, Equation 2.1 can be rewritten in the beautiful and compact\\nvectorized form\\nal = σ(wl al−1 + bl ). (2.3)\\nThis expression gives us a much more global way of thinking about how the activations in\\none layer relate to activations in the previous layer: we just apply the weight matrix to the\\nactivations, then add the bias vector, and ﬁnally apply the σfunction1. That global view is\\noften easier and more succinct (and involves fewer indices!) than the neuron-by-neuron\\nview we’ve taken to now. Think of it as a way of escaping index hell, while remaining precise\\nabout what’s going on. The expression is also useful in practice, because most matrix libraries\\nprovide fast ways of implementing matrix multiplication, vector addition, and vectorization.\\nIndeed, the code (see 1.6) in the last chapter made implicit use of this expression to compute\\nthe behaviour of the network.\\nWhen using Equation 2.3 to compute al , we compute the intermediate quantity zl ≡\\nwl al−1 + bl along the way . This quantity turns out to be useful enough to be worth naming:\\nwe call zl the weighted input to the neurons in layer l. We’ll make considerable use of\\nthe weighted input zl later in the chapter. Equation 2.3 is sometimes written in terms\\nof the weighted input, as al = σ(zl ). It’s also worth noting that zl has components zl\\nj =∑\\nk wl\\njk al−1\\nk + bl\\nj, that is, zl\\nj is just the weighted input to the activation function for neuron j\\nin layer l.\\n1By the way , it’s this expression that motivates the quirk in thewl\\njk notation mentioned earlier. If we\\nused j to index the input neuron, andk to index the output neuron, then we’d need to replace the weight\\nmatrix in Equation 2.3 by the transpose of the weight matrix. That’s a small change, but annoying, and\\nwe’d lose the easy simplicity of saying (and thinking) “apply the weight matrix to the activations”.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='79b38c22-2a46-498d-864f-fdd92a6effde', embedding=None, metadata={'page_label': '42', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='42\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\n2.2 The two assumptions we need about the cost function\\nThe goal of backpropagation is to compute the partial derivatives ∂C/∂w and ∂C/∂b of the\\ncost function C with respect to any weight w or bias b in the network. For backpropagation\\nto work we need to make two main assumptions about the form of the cost function. Before\\nstating those assumptions, though, it’s useful to have an example cost function in mind. We’ll\\nuse the quadratic cost function from last chapter (c.f. Equation 1.6). In the notation of the\\nlast section, the quadratic cost has the form\\nC = 1\\n2n\\n∑\\nx\\n\\r\\ry(x) −aL(x)\\n\\r\\r2\\n, (2.4)\\nwhere: n is the total number of training examples; the sum is over individual training\\nexamples, x; y = y(x) is the corresponding desired output; L denotes the number of layers\\nin the network; and aL = aL(x) is the vector of activations output from the network when x\\nis input.\\nOkay, so what assumptions do we need to make about our cost function, C, in order\\nthat backpropagation can be applied? The ﬁrst assumption we need is that the cost function\\ncan be written as an average C = 1\\nn\\n∑\\nx Cx over cost functions Cx for individual training\\nexamples, x. This is the case for the quadratic cost function, where the cost for a single\\ntraining example is Cx = 1\\n2 ∥y −aL∥2. This assumption will also hold true for all the other\\ncost functions we’ll meet in this book.\\nThe reason we need this assumption is because what backpropagation actually lets us\\ndo is compute the partial derivatives ∂Cx /∂w and ∂Cx /∂b for a single training example.\\nWe then recover ∂C/∂w and ∂C/∂b by averaging over training examples. In fact, with this\\nassumption in mind, we’ll suppose the training examplex has been ﬁxed, and drop the x\\nsubscript, writing the cost Cx as C. We’ll eventually put the x back in, but for now it’s a\\nnotational nuisance that is better left implicit.\\nThe second assumption we make about the cost is that it can be written as a function of\\nthe outputs from the neural network:\\nFor example, the quadratic cost function satisﬁes this requirement, since the quadratic cost\\nfor a single training example x may be written as\\nC = 1\\n2∥y −aL∥2 = 1\\n2\\n∑\\nj\\n(yj −aL\\nj )2, (2.5)\\nand thus is a function of the output activations. Of course, this cost function also depends\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5a562062-b916-4718-af6c-8be7bc1b38be', embedding=None, metadata={'page_label': '43', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3. The Hadamard product, s ⊙t\\n\\x0c\\x0c\\x0c 43\\non the desired output y, and you may wonder why we’re not regarding the cost also as\\na function of y. Remember, though, that the input training example x is ﬁxed, and so\\nthe output y is also a ﬁxed parameter. In particular, it’s not something we can modify by\\nchanging the weights and biases in any way , i.e., it’s not something which the neural network\\nlearns. And so it makes sense to regard C as a function of the output activations aL alone,\\nwith y merely a parameter that helps deﬁne that function.\\n2.3 The Hadamard product, s ⊙t\\nThe backpropagation algorithm is based on common linear algebraic operations – things\\nlike vector addition, multiplying a vector by a matrix, and so on. But one of the operations\\nis a little less commonly used. In particular, suppose s and t are two vectors of the same\\ndimension. Then we use s ⊙t to denote the elementwise product of the two vectors. Thus\\nthe components of s ⊙t are just (s ⊙t)j = sj tj. As an example,\\n\\x96\\n1\\n2\\n\\x99\\n⊙\\n\\x96\\n3\\n4\\n\\x99\\n=\\n\\x96\\n1 ∗3\\n2 ∗4\\n\\x99\\n=\\n\\x96\\n3\\n8\\n\\x99\\n. (2.6)\\nThis kind of elementwise multiplication is sometimes called the Hadamard product or Schur\\nproduct. We’ll refer to it as the Hadamard product. Good matrix libraries usually provide fast\\nimplementations of the Hadamard product, and that comes in handy when implementing\\nbackpropagation.\\n2.4 The four fundamental equations behind backpropagation\\nBackpropagation is about understanding how changing the weights and biases in a network\\nchanges the cost function. Ultimately , this means computing the partial derivatives∂C/∂wl\\njk\\nand ∂C/∂bl\\nj. But to compute those, we ﬁrst introduce an intermediate quantity ,δl\\nj, which\\nwe call the error in the j-th neuron in the l-th layer. Backpropagation will give us a procedure\\nto compute the error δl\\nj, and then will relate δl\\nj to ∂C/∂wl\\njk and ∂C/∂bl\\nj.\\nTo understand how the error is deﬁned, imagine there is a demon in our neural network:\\nThe demon sits at the j-th neuron in layer l. As the input to the neuron comes in, the demon\\nmesses with the neuron’s operation. It adds a little change ∆zl\\nj to the neuron’s weighted\\ninput, so that instead of outputting σ(zl\\nj ), the neuron instead outputs σ(zl\\nj + ∆zl\\nj ). This\\nchange propagates through later layers in the network, ﬁnally causing the overall cost to\\nchange by an amount ∂C\\n∂zl\\nj\\n∆zl\\nj .\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b2f23ece-bbf3-49c2-b7cb-a659e871c69a', embedding=None, metadata={'page_label': '44', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='44\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\nNow, this demon is a good demon, and is trying to help you improve the cost, i.e., they’re\\ntrying to ﬁnd a δzl\\nj which makes the cost smaller. Suppose ∂C/∂zl\\nj has a large value (either\\npositive or negative). Then the demon can lower the cost quite a bit by choosing ∆zl\\nj to have\\nthe opposite sign to ∂C/∂zl\\nj . By contrast, if ∂C/∂zl\\nj is close to zero, then the demon can’t\\nimprove the cost much at all by perturbing the weighted input zl\\nj . So far as the demon can\\ntell, the neuron is already pretty near optimal2. And so there’s a heuristic sense in which\\n∂C/∂zl\\nj is a measure of the error in the neuron.\\nMotivated by this story , we deﬁne the errorδl\\nj of neuron j in layer l by\\nδl\\nj ≡∂C\\n∂zl\\nj\\n. (2.7)\\nAs per our usual conventions, we use δl to denote the vector of errors associated with layer l.\\nBackpropagation will give us a way of computing δl for every layer, and then relating those\\nerrors to the quantities of real interest, ∂C/∂wl\\njk and ∂C/∂bl\\nj.\\nYou might wonder why the demon is changing the weighted inputzl\\nj . Surely it’d be more\\nnatural to imagine the demon changing the output activation al\\nj, with the result that we’d be\\nusing ∂C\\n∂al\\nj\\nas our measure of error. In fact, if you do this things work out quite similarly to\\nthe discussion below. But it turns out to make the presentation of backpropagation a little\\nmore algebraically complicated. So we’ll stick withδl\\nj = ∂C\\n∂zl\\nj\\nas our measure of error3.\\nPlan of attack: Backpropagation is based around four fundamental equations. Together,\\nthose equations give us a way of computing both the error δl and the gradient of the cost\\nfunction. I state the four equations below. Be warned, though: you shouldn’t expect to\\ninstantaneously assimilate the equations. Such an expectation will lead to disappointment.\\nIn fact, the backpropagation equations are so rich that understanding them well requires\\nconsiderable time and patience as you gradually delve deeper into the equations. The good\\nnews is that such patience is repaid many times over. And so the discussion in this section is\\nmerely a beginning, helping you on the way to a thorough understanding of the equations.\\nHere’s a preview of the ways we’ll delve more deeply into the equations later in the\\nchapter: I’ll give a short proof of the equations, which helps explain why they are true;\\nwe’ll restate the equations in algorithmic form as pseudocode, and see how the pseudocode\\ncan be implemented as real, running Python code; and, in the ﬁnal section of the chapter,\\nwe’ll develop an intuitive picture of what the backpropagation equations mean, and how\\nsomeone might discover them from scratch. Along the way we’ll return repeatedly to the\\nfour fundamental equations, and as you deepen your understanding those equations will\\ncome to seem comfortable and, perhaps, even beautiful and natural.\\nAn equation for the error in the output layer,δL: The components of δL are given by\\nδL\\nj = ∂C\\n∂aL\\nj\\nσ′(zL\\nj ). (BP1)\\n2This is only the case for small changes ∆zl\\nj, of course. We’ll assume that the demon is constrained\\nto make such small changes.\\n3In classiﬁcation problems like MNIST the term “error” is sometimes used to mean the classiﬁcation\\nfailure rate. E.g., if the neural net correctly classiﬁes 96.0 percent of the digits, then the error is 4.0\\npercent. Obviously, this has quite a different meaning from our δvectors. In practice, you shouldn’t\\nhave trouble telling which meaning is intended in any given usage.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e725963e-3e88-4968-a1b0-8ed298b63d05', embedding=None, metadata={'page_label': '45', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4. The four fundamental equations behind backpropagation\\n\\x0c\\x0c\\x0c 45\\nThis is a very natural expression. The ﬁrst term on the right, ∂C/∂aL\\nj , just measures how\\nfast the cost is changing as a function of the j-th output activation. If, for example, C doesn’t\\ndepend much on a particular output neuron, j, then δL\\nj will be small, which is what we’d\\nexpect. The second term on the right, σ′(zL\\nj ), measures how fast the activation function σis\\nchanging at zL\\nj .\\nNotice that everything in Eq. BP1 is easily computed. In particular, we compute zL\\nj while\\ncomputing the behaviour of the network, and it’s only a small additional overhead to compute\\nσ′(zL\\nj ). The exact form of ∂C/∂aL\\nj will, of course, depend on the form of the cost function.\\nHowever, provided the cost function is known there should be little trouble computing\\n∂C/∂aL\\nj . For example, if we’re using the quadratic cost function thenC = 1\\n2\\n∑\\nj(yj −aL\\nj )2,\\nand so ∂C/∂aL\\nj = (aL\\nj −yj), which obviously is easily computable.\\nEquation BP1 is a componentwise expression for δL. It’s a perfectly good expression, but\\nnot the matrix-based form we want for backpropagation. However, it’s easy to rewrite the\\nequation in a matrix-based form, as\\nδL = ∇a C ⊙σ′(zL). (BP1a)\\nHere, ∇a C is deﬁned to be a vector whose components are the partial derivatives ∂C/∂aL\\nj .\\nYou can think of ∇a C as expressing the rate of change of C with respect to the output\\nactivations. It’s easy to see that Equations BP1a and BP1 are equivalent, and for that reason\\nfrom now on we’ll use BP1 interchangeably to refer to both equations. As an example, in the\\ncase of the quadratic cost we have ∇a C = (aL −y), and so the fully matrix-based form of\\nBP1 becomes\\nδL = (aL −y) ⊙σ′(zL). (2.8)\\nAs you can see, everything in this expression has a nice vector form, and is easily computed\\nusing a library such as Numpy .\\nAn equation for the errorδl in terms of the error in the next layer, δl+1: In particular\\nδl = ((wl+1)T δl+1) ⊙σ′(zl ), (BP2)\\nwhere (wl+1)T is the transpose of the weight matrixwl+1 for the (l+1)-th layer. This equation\\nappears complicated, but each element has a nice interpretation. Suppose we know the\\nerror δl+1 at the (l+1)-th layer. When we apply the transpose weight matrix, (wl+1)T , we\\ncan think intuitively of this as moving the error backward through the network, giving us\\nsome sort of measure of the error at the output of the l-th layer. We then take the Hadamard\\nproduct ⊙σ′(zl ). This moves the error backward through the activation function in layer l,\\ngiving us the error δl in the weighted input to layer l.\\nBy combining (BP2) with (BP1) we can compute the errorδl for any layer in the network.\\nWe start by using (BP1) to compute δL, then apply Equation (BP2) to compute δL−1, then\\nEquation (BP2) again to compute δL−2, and so on, all the way back through the network.\\nAn equation for the rate of change of the cost with respect to any bias in the net-\\nwork: In particular:\\n∂C\\n∂bl\\nj\\n= δl\\nj. (BP3)\\nThat is, the error δl\\nj is exactly equal to the rate of change ∂C/∂bl\\nj. This is great news, since\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='04650349-92a0-49db-9327-d2150598d8d1', embedding=None, metadata={'page_label': '46', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='46\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\n(BP1) and (BP2) have already told us how to computeδl\\nj. We can rewrite (BP3) in shorthand\\nas\\n∂C\\n∂b = δ, (2.9)\\nwhere it is understood that δis being evaluated at the same neuron as the bias b.\\nAn equation for the rate of change of the cost with respect to any weight in the\\nnetwork: In particular:\\n∂C\\n∂wl\\njk\\n= al−1\\nk δl\\nj. (BP4)\\nThis tells us how to compute the partial derivatives ∂C/∂wl\\njk in terms of the quantities δl\\nand al−1, which we already know how to compute. The equation can be rewritten in a less\\nindex-heavy notation as\\n∂C\\n∂w = ainδout, (2.10)\\nwhere it’s understood thatain is the activation of the neuron input to the weight w, and δout\\nis the error of the neuron output from the weight w. Zooming in to look at just the weight w,\\nand the two neurons connected by that weight, we can depict this as:\\nA nice consequence of Equation 2.10 is that when the activation ain is small, ain ≈0, the\\ngradient term ∂C/∂w will also tend to be small. In this case, we’ll say the weight learns\\nslowly, meaning that it’s not changing much during gradient descent. In other words, one\\nconsequence of (BP4) is that weights output from low-activation neurons learn slowly .\\nThere are other insights along these lines which can be obtained from (BP1)–(BP4). Let’s\\nstart by looking at the output layer. Consider the termσ′(zL\\nj ) in (BP1). Recall from the graph\\nof the sigmoid function in the last chapter that the σfunction becomes very ﬂat when σ(zL\\nj )\\nis approximately 0 or 1. When this occurs we will have σ′(zL\\nj ) ≈0. And so the lesson is that\\na weight in the ﬁnal layer will learn slowly if the output neuron is either low activation (≈0)\\nor high activation (≈1). In this case it’s common to say the output neuron has saturated\\nand, as a result, the weight has stopped learning (or is learning slowly). Similar remarks\\nhold also for the biases of output neuron.\\nWe can obtain similar insights for earlier layers. In particular, note the σ′(zl ) term in\\n(BP2). This means that δl\\nj is likely to get small if the neuron is near saturation. And this, in\\nturn, means that any weights input to a saturated neuron will learn slowly4.\\nSumming up, we’ve learnt that a weight will learn slowly if either the input neuron is\\nlow-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.\\nNone of these observations is too greatly surprising. Still, they help improve our mental\\nmodel of what’s going on as a neural network learns. Furthermore, we can turn this type\\nof reasoning around. The four fundamental equations turn out to hold for any activation\\nfunction, not just the standard sigmoid function (that’s because, as we’ll see in a moment,\\n4This reasoning won’t hold if(wl+1)T δl+1 has large enough entries to compensate for the smallness\\nof σ′(zl\\nj). But I’m speaking of the general tendency .\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e4ae262b-442f-46e8-9805-2489d9f058ab', embedding=None, metadata={'page_label': '47', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4. The four fundamental equations behind backpropagation\\n\\x0c\\x0c\\x0c 47\\nthe proofs don’t use any special properties of σ). And so we can use these equations to\\ndesign activation functions which have particular desired learning properties. As an example\\nto give you the idea, suppose we were to choose a (non-sigmoid) activation function σso\\nthat σ′is always positive, and never gets close to zero. That would prevent the slow-down\\nof learning that occurs when ordinary sigmoid neurons saturate. Later in the book we’ll see\\nexamples where this kind of modiﬁcation is made to the activation function. Keeping the\\nfour equations BP1–BP4 in mind can help explain why such modiﬁcations are tried, and\\nwhat impact they can have.\\nProblem\\n• Alternate presentation of the equations of backpropagation: I’ve stated the equa-\\ntions of backpropagation (notably BP1 and BP2) using the Hadamard product. This\\npresentation may be disconcerting if you’re unused to the Hadamard product. There’s\\nan alternative approach, based on conventional matrix multiplication, which some\\nreaders may ﬁnd enlightening.\\n(1) Show that (BP1) may be rewritten as\\nδL = Σ′(zL)∇a C, (2.11)\\nwhere Σ′(zL) is a square matrix whose diagonal entries are the values σ′(zL\\nj ),\\nand whose off-diagonal entries are zero. Note that this matrix acts on ∇a C by\\nconventional matrix multiplication.\\n(2) Show that (BP2) may be rewritten as\\nδl = Σ′(zl )(wl+1)T δl+1. (2.12)\\n(3) By combining observations (1) and (2) show that\\nδl = Σ′(zl )(wl+1)T . . .Σ′(zL−1)(wL)T Σ′(zL)∇a C (2.13)\\nFor readers comfortable with matrix multiplication this equation may be easier\\nto understand than (BP1) and (BP2). The reason I’ve focused on (BP1) and\\n(BP2) is because that approach turns out to be faster to implement numerically .\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8a356aea-88f6-4d49-a20e-a9ded78c9b71', embedding=None, metadata={'page_label': '48', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='48\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\n2.5 Proof of the four fundamental equations (optional)\\nWe’ll now prove the four fundamental equations (BP1)–(BP4). All four are consequences of\\nthe chain rule from multivariable calculus. If you’re comfortable with the chain rule, then I\\nstrongly encourage you to attempt the derivation yourself before reading on.\\nLet’s begin with Equation (BP1), which gives an expression for the output error,δl . To\\nprove this equation, recall that by deﬁnition\\nδL\\nj = ∂C\\n∂zL\\nj\\n. (2.14)\\nApplying the chain rule, we can re-express the partial derivative above in terms of partial\\nderivatives with respect to the output activations,\\nδL\\nj =\\n∑\\nk\\n∂C\\n∂aL\\nk\\n∂aL\\nk\\n∂zL\\nj\\n, (2.15)\\nwhere the sum is over all neurons k in the output layer. Of course, the output activation aL\\nk\\nof the k-th neuron depends only on the weighted input zL\\nj for the j-th neuron when k = j.\\nAnd so ∂aL\\nk /∂zL\\nj vanishes when k ̸= j. As a result we can simplify the previous equation to\\nδL\\nj = ∂C\\n∂aL\\nj\\n∂aL\\nj\\n∂zL\\nj\\n. (2.16)\\nRecalling that aL\\nj = σ(zL\\nj ) the second term on the right can be written as σ′(zL\\nj ), and the\\nequation becomes\\nδL\\nj = ∂C\\n∂aL\\nj\\nσ′(zL\\nj ), (2.17)\\nwhich is just (BP1), in component form. Next, we’ll prove (BP2), which gives an equation\\nfor the error δl in terms of the error in the next layer, δl+1. To do this, we want to rewrite\\nδl\\nj = ∂C/∂zl\\nj in terms of δl+1\\nk = ∂C/∂zl+1\\nk . We can do this using the chain rule,\\nδl\\nj = ∂C\\n∂zl\\nj\\n=\\n∑\\nk\\n∂C\\n∂zl+1\\nk\\n∂zl+1\\nk\\n∂zl\\nj\\n=\\n∑\\nk\\n∂zl+1\\nk\\n∂zl\\nj\\nδl+1\\nk , (2.18)\\nwhere in the last line we have interchanged the two terms on the right-hand side, and\\nsubstituted the deﬁnition of δl+1\\nk . To evaluate the ﬁrst term on the last line, note that\\nzl+1\\nk =\\n∑\\nj\\nwl+1\\nk j al\\nj + bl+1\\nk =\\n∑\\nj\\nwl+1\\nk j σ(zl\\nj ) +bl+1\\nk . (2.19)\\nDifferentiating, we obtain\\n∂zl+1\\nk\\n∂zl\\nj\\n= wl+1\\nk j σ′(zl\\nj ). (2.20)\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='fe1a3e2e-b18e-4573-bad1-defe87c1f175', embedding=None, metadata={'page_label': '49', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.6. The backpropagation algorithm\\n\\x0c\\x0c\\x0c 49\\nSubstituting back into (2.18) we obtain\\nδl\\nj =\\n∑\\nk\\nwl+1\\nk j δl+1\\nk σ′(zl\\nj ). (2.21)\\nThis is just (BP2) written in component form.\\nThe ﬁnal two equations we want to prove are (BP3) and (BP4). These also follow from\\nthe chain rule, in a manner similar to the proofs of the two equations above. I leave them to\\nyou as an exercise.\\nExercise\\n• Prove Equations (BP3) and (BP4).\\nThat completes the proof of the four fundamental equations of backpropagation. The proof\\nmay seem complicated. But it’s really just the outcome of carefully applying the chain rule.\\nA little less succinctly , we can think of backpropagation as a way of computing the gradient\\nof the cost function by systematically applying the chain rule from multi-variable calculus.\\nThat’s all there really is to backpropagation – the rest is details.\\n2.6 The backpropagation algorithm\\nThe backpropagation equations provide us with a way of computing the gradient of the cost\\nfunction. Let’s explicitly write this out in the form of an algorithm:\\n1. Input x: Set the corresponding activation a1 for the input layer.\\n2. Feedforward: For each l = 2, 3, . . . ,L compute zl = wl al−1 + bl and al = σ(zl ).\\n3. Output error δL: Compute the vector δL = ∇a C ⊙σ′(zL).\\n4. Backpropagate the error: For eachl = L−1, L−2, . . . ,2 compute δl = ((wl+1)T δl+1)⊙\\nσ′(zl ).\\n5. Output: The gradient of the cost function is given by ∂C\\n∂wl\\njk\\n= al−1\\nk δl\\nj and ∂C\\n∂bl\\nj\\n= δl\\nj.\\nExamining the algorithm you can see why it’s calledbackpropagation. We compute the error\\nvectors δl backward, starting from the ﬁnal layer. It may seem peculiar that we’re going\\nthrough the network backward. But if you think about the proof of backpropagation, the\\nbackward movement is a consequence of the fact that the cost is a function of outputs from\\nthe network. To understand how the cost varies with earlier weights and biases we need\\nto repeatedly apply the chain rule, working backward through the layers to obtain usable\\nexpressions.\\nExercises\\n• Backpropagation with a single modiﬁed neuronSuppose we modify a single neuron\\nin a feedforward network so that the output from the neuron is given by f (\\n∑\\nj wj xj +\\nb), where f is some function other than the sigmoid. How should we modify the\\nbackpropagation algorithm in this case?\\n• Backpropagation with linear neurons Suppose we replace the usual non-linear\\nσfunction with σ(z) = z throughout the network. Rewrite the backpropagation\\nalgorithm for this case.\\nAs I’ve described it above, the backpropagation algorithm computes the gradient of the\\ncost function for a single training example, C = Cx . In practice, it’s common to combine\\nbackpropagation with a learning algorithm such as stochastic gradient descent, in which we\\ncompute the gradient for many training examples. In particular, given a mini-batch of m\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='55395c9e-d39f-4c48-a855-06d7dd0fb9a1', embedding=None, metadata={'page_label': '50', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='50\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\ntraining examples, the following algorithm applies a gradient descent learning step based on\\nthat mini-batch:\\n1. Input a set of training examples\\n2. For each training example x: Set the corresponding input activation ax,1, and perform\\nthe following steps:\\n• Feedforward: For each l = 2, 3, . . . ,L compute zx,l = wl ax,l−1 + bl and ax,l =\\nσ(zx,l ).\\n• Output error δx,L: Compute the vector δx,L = ∇a Cx ⊙σ′(zx,L).\\n• Backpropagate the error: For each l = L −1, L −2, . . . ,2 compute δx,l =\\n((wl+1)T δx,l+1) ⊙σ′(zx,l ).\\n3. Gradient descent: For each l = L, L −1, . . . ,2 update the weights according to the rule\\nwl →wl −η\\nm\\n∑\\nx δx,l (ax,l−1)T , and the biases according to the rulebl →bl −η\\nm\\n∑\\nx δx,l .\\nOf course, to implement stochastic gradient descent in practice you also need an outer loop\\ngenerating mini-batches of training examples, and an outer loop stepping through multiple\\nepochs of training. I’ve omitted those for simplicity .\\n2.7 The code for backpropagation\\nHaving understood backpropagation in the abstract, we can now understand the code used\\nin the last chapter to implement backpropagation. Recall from that chapter that the code was\\ncontained in the update_mini_batch and backprop methods of the Network class. The\\ncode for these methods is a direct translation of the algorithm described above. In particular,\\nthe update_mini_batch method updates the Network’s weights and biases by computing\\nthe gradient for the current mini_batch of training examples:\\nclass Network( object ):\\n...\\ndef update_mini_batch(self , mini_batch , eta):\\n\"\"\"Update the network’s weights and biases by applying\\ngradient descent using backpropagation to a single mini batch.\\nThe \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\\nis the learning rate.\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\nfor x, y in mini_batch:\\ndelta_nabla_b , delta_nabla_w = self.backprop(x, y)\\nnabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)]\\nnabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)]\\nself.weights = [w-(eta/ len (mini_batch))*nw\\nfor w, nw in zip (self.weights , nabla_w)]\\nself.biases = [b-(eta/ len (mini_batch))*nb\\nfor b, nb in zip (self.biases , nabla_b)]\\nMost of the work is done by the line delta_nabla_b, delta_nabla_w = self.backprop\\n(x, y) which uses the backprop method to ﬁgure out the partial derivatives ∂Cx /∂bl\\nj and\\n∂Cx /∂wl\\njk . The backprop method follows the algorithm in the last section closely . There is\\none small change – we use a slightly different approach to indexing the layers. This change\\nis made to take advantage of a feature of Python, namely the use of negative list indices\\nto count backward from the end of a list, so, e.g., l[-3] is the third last entry in a list l.\\nThe code for backprop is below, together with a few helper functions, which are used to\\ncompute the σfunction, the derivative σ′, and the derivative of the cost function. With these\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d9c8034a-6dd2-44bf-aa9f-c135c8c667b5', embedding=None, metadata={'page_label': '51', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.7. The code for backpropagation\\n\\x0c\\x0c\\x0c 51\\ninclusions you should be able to understand the code in a self-contained way . If something’s\\ntripping you up, you may ﬁnd it helpful to consult the original description (and complete\\nlisting) of the code.\\nclass Network( object ):\\n...\\ndef backprop(self , x, y):\\n\"\"\"Return a tuple \"(nabla_b , nabla_w)\" representing the\\ngradient for the cost function C_x. \"nabla_b\" and\\n\"nabla_w\" are layer -by-layer lists of numpy arrays , similar\\nto \"self.biases\" and \"self.weights\".\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\n# feedforward\\nactivation = x\\nactivations = [x] # list to store all the activations , layer by layer\\nzs = [] # list to store all the z vectors , layer by layer\\nfor b, w in zip (self.biases , self.weights):\\nz = np.dot(w, activation)+b\\nzs.append(z)\\nactivation = sigmoid(z)\\nactivations.append(activation)\\n# backward pass\\ndelta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\\nnabla_b[-1] = delta\\nnabla_w[-1] = np.dot(delta , activations[-2].transpose())\\n# Note that the variable l in the loop below is used a little\\n# differently to the notation in Chapter 2 of the book. Here ,\\n# l = 1 means the last layer of neurons , l = 2 is the\\n# second -last layer , and so on. It’s a renumbering of the\\n# scheme in the book , used here to take advantage of the fact\\n# that Python can use negative indices in lists.\\nfor l in xrange (2, self.num_layers):\\nz = zs[-l]\\nsp = sigmoid_prime(z)\\ndelta = np.dot(self.weights[-l+1]. transpose(), delta) * sp\\nnabla_b[-l] = delta\\nnabla_w[-l] = np.dot(delta , activations[-l-1]. transpose())\\nreturn (nabla_b , nabla_w)\\n...\\ndef cost_derivative(self , output_activations , y):\\n\"\"\"Return the vector of partial derivatives \\\\partial{} C_x /\\n\\\\partial{} a for the output activations.\"\"\"\\nreturn (output_activations -y)\\ndef sigmoid(z):\\n\"\"\"The sigmoid function.\"\"\"\\nreturn 1.0/(1.0+np.exp(-z))\\ndef sigmoid_prime(z):\\n\"\"\"Derivative of the sigmoid function.\"\"\"\\nreturn sigmoid(z)*(1-sigmoid(z))\\nProblem\\n• Fully matrix-based approach to backpropagation over a mini-batch Our imple-\\nmentation of stochastic gradient descent loops over training examples in a mini-batch.\\nIt’s possible to modify the backpropagation algorithm so that it computes the gradients\\nfor all training examples in a mini-batch simultaneously. The idea is that instead of\\nbeginning with a single input vector, x, we can begin with a matrix X = [x1 x2 . . .xm]\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8e517f14-0f75-4820-88a0-4504b389cc5d', embedding=None, metadata={'page_label': '52', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='52\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\nwhose columns are the vectors in the mini-batch. We forward-propagate by multiply-\\ning by the weight matrices, adding a suitable matrix for the bias terms, and applying\\nthe sigmoid function everywhere. We backpropagate along similar lines. Explicitly\\nwrite out pseudocode for this approach to the backpropagation algorithm. Modify\\nnetwork.py so that it uses this fully matrix-based approach. The advantage of this\\napproach is that it takes full advantage of modern libraries for linear algebra. As a\\nresult it can be quite a bit faster than looping over the mini-batch. (On my laptop,\\nfor example, the speedup is about a factor of two when run on MNIST classiﬁcation\\nproblems like those we considered in the last chapter.) In practice, all serious libraries\\nfor backpropagation use this fully matrix-based approach or some variant.\\n2.8 In what sense is backpropagation a fast algorithm?\\nIn what sense is backpropagation a fast algorithm? To answer this question, let’s consider\\nanother approach to computing the gradient. Imagine it’s the early days of neural networks\\nresearch. Maybe it’s the 1950s or 1960s, and you’re the ﬁrst person in the world to think of\\nusing gradient descent to learn! But to make the idea work you need a way of computing\\nthe gradient of the cost function. You think back to your knowledge of calculus, and decide\\nto see if you can use the chain rule to compute the gradient. But after playing around a bit,\\nthe algebra looks complicated, and you get discouraged. So you try to ﬁnd another approach.\\nYou decide to regard the cost as a function of the weights C = C(w) alone (we’ll get back to\\nthe biases in a moment). You number the weights w1, w2, . . ., and want to compute ∂C/∂wj\\nfor some particular weight wj. An obvious way of doing that is to use the approximation\\n∂C\\n∂wj\\n≈\\nC(w + εej) −C(w)\\nε , (2.22)\\nwhere ε> 0 is a small positive number, and ej is the unit vector in the j-th direction. In\\nother words, we can estimate ∂C/∂wj by computing the cost C for two slightly different\\nvalues of wj, and then applying Equation 2.22. The same idea will let us compute the partial\\nderivatives ∂C/∂b with respect to the biases.\\nThis approach looks very promising. It’s simple conceptually, and extremely easy to\\nimplement, using just a few lines of code. Certainly , it looks much more promising than the\\nidea of using the chain rule to compute the gradient!\\nUnfortunately , while this approach appears promising, when you implement the code it\\nturns out to be extremely slow. To understand why, imagine we have a million weights in\\nour network. Then for each distinct weight wj we need to compute C(w + εej) in order to\\ncompute ∂C/∂wj. That means that to compute the gradient we need to compute the cost\\nfunction a million different times, requiring a million forward passes through the network\\n(per training example). We need to compute C(w) as well, so that’s a total of a million and\\none passes through the network.\\nWhat’s clever about backpropagation is that it enables us to simultaneously compute all\\nthe partial derivatives ∂C/∂wj using just one forward pass through the network, followed\\nby one backward pass through the network. Roughly speaking, the computational cost\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d8cabc61-fe87-483b-bc22-b6ce111c8ec9', embedding=None, metadata={'page_label': '53', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.9. Backpropagation: the big picture\\n\\x0c\\x0c\\x0c 53\\nof the backward pass is about the same as the forward pass 5. And so the total cost of\\nbackpropagation is roughly the same as making just two forward passes through the network.\\nCompare that to the million and one forward passes we needed for the approach based on\\n(2.22)! And so even though backpropagation appears superﬁcially more complex than the\\napproach based on (2.22), it’s actually much, much faster.\\nThis speedup was ﬁrst fully appreciated in 1986, and it greatly expanded the range of\\nproblems that neural networks could solve. That, in turn, caused a rush of people using\\nneural networks. Of course, backpropagation is not a panacea. Even in the late 1980s people\\nran up against limits, especially when attempting to use backpropagation to train deep neural\\nnetworks, i.e., networks with many hidden layers. Later in the book we’ll see how modern\\ncomputers and some clever new ideas now make it possible to use backpropagation to train\\nsuch deep neural networks.\\n2.9 Backpropagation: the big picture\\nAs I’ve explained it, backpropagation presents two mysteries. First, what’s the algorithm\\nreally doing? We’ve developed a picture of the error being backpropagated from the output.\\nBut can we go any deeper, and build up more intuition about what is going on when we\\ndo all these matrix and vector multiplications? The second mystery is how someone could\\never have discovered backpropagation in the ﬁrst place? It’s one thing to follow the steps in\\nan algorithm, or even to follow the proof that the algorithm works. But that doesn’t mean\\nyou understand the problem so well that you could have discovered the algorithm in the\\nﬁrst place. Is there a plausible line of reasoning that could have led you to discover the\\nbackpropagation algorithm? In this section I’ll address both these mysteries.\\nTo improve our intuition about what the algorithm is doing, let’s imagine that we’ve\\nmade a small change ∆wl\\njk to some weight in the network, wl\\njk :\\nThat change in weight will cause a change in the output activation from the corresponding\\nneuron:\\n5This should be plausible, but it requires some analysis to make a careful statement. It’s plausible\\nbecause the dominant computational cost in the forward pass is multiplying by the weight matrices,\\nwhile in the backward pass it’s multiplying by the transposes of the weight matrices. These operations\\nobviously have similar computational cost.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='32d2912d-3cfc-43f9-8128-593f7352d412', embedding=None, metadata={'page_label': '54', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='54\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\nThat, in turn, will cause a change in all the activations in the next layer:\\nThose changes will in turn cause changes in the next layer, and then the next, and so on all\\nthe way through to causing a change in the ﬁnal layer, and then in the cost function:\\nThe change ∆C in the cost is related to the change ∆wl\\njk in the weight by the equation\\n∆C ≈ ∂C\\n∂wl\\njk\\n∆wl\\njk . (2.23)\\nThis suggests that a possible approach to computing ∂C/∂wl\\njk is to carefully track how a\\nsmall change in wl\\njk propagates to cause a small change in C. If we can do that, being careful\\nto express everything along the way in terms of easily computable quantities, then we should\\nbe able to compute ∂C/∂wl\\njk .\\nLet’s try to carry this out. The change∆wl\\njk causes a small change ∆al\\nj in the activation\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bb7b3530-54ab-488c-9841-08c63e26b2e5', embedding=None, metadata={'page_label': '55', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.9. Backpropagation: the big picture\\n\\x0c\\x0c\\x0c 55\\nof the j-th neuron in the l-th layer. This change is given by\\n∆al\\nj ≈\\n∂al\\nj\\n∂wl\\njk\\n∆wl\\njk . (2.24)\\nThe change in activation ∆al\\nj will cause changes in all the activations in the next layer, i.e.,\\nthe (l+1)-th layer. We’ll concentrate on the way just a single one of those activations is\\naffected, say al+1\\nq ,\\nIn fact, it’ll cause the following change:\\n∆al+1\\nq ≈\\n∂al+1\\nq\\n∂al\\nj\\n∆al\\nj. (2.25)\\nSubstituting in the expression from Equation 2.24, we get:\\n∆al+1\\nq ≈\\n∂al+1\\nq\\n∂al\\nj\\n∂al\\nj\\n∂wl\\njk\\n∆wl\\njk . (2.26)\\nOf course, the change ∆al+1\\nq will, in turn, cause changes in the activations in the next layer.\\nIn fact, we can imagine a path all the way through the network from wl\\njk to C, with each\\nchange in activation causing a change in the next activation, and, ﬁnally , a change in the cost\\nat the output. If the path goes through activations al\\nj, al+1\\nq , ··· , aL−1\\nn , aL\\nm then the resulting\\nexpression is\\n∆C ≈ ∂C\\n∂aL\\nm\\n∂aL\\nm\\n∂aL−1\\nn\\n∂aL−1\\nn\\n∂aL−2\\np\\n. . .\\n∂al+1\\nq\\n∂al\\nj\\n∂al\\nj\\n∂wl\\njk\\n∆wl\\njk , (2.27)\\nthat is, we’ve picked up a∂a/∂a type term for each additional neuron we’ve passed through,\\nas well as the ∂C/∂aL\\nm term at the end. This represents the change in C due to changes in\\nthe activations along this particular path through the network. Of course, there’s many paths\\nby which a change in wl\\njk can propagate to affect the cost, and we’ve been considering just a\\nsingle path. To compute the total change in C it is plausible that we should sum over all the\\npossible paths between the weight and the ﬁnal cost, i.e.,\\n∆C ≈\\n∑\\nmnp...q\\n∂C\\n∂aL\\nm\\n∂aL\\nm\\n∂aL−1\\nn\\n∂aL−1\\nn\\n∂aL−2\\np\\n. . .\\n∂al+1\\nq\\n∂al\\nj\\n∂al\\nj\\n∂wl\\njk\\n∆wl\\njk , (2.28)\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ab4acc79-156f-4fb3-86ae-fdde2550947b', embedding=None, metadata={'page_label': '56', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='56\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\nwhere we’ve summed over all possible choices for the intermediate neurons along the path.\\nComparing with (2.23) we see that\\n∂C\\n∂wl\\njk\\n=\\n∑\\nmnp...q\\n∂C\\n∂aL\\nm\\n∂aL\\nm\\n∂aL−1\\nn\\n∂aL−1\\nn\\n∂aL−2\\np\\n. . .\\n∂al+1\\nq\\n∂al\\nj\\n∂al\\nj\\n∂wl\\njk\\n. (2.29)\\nNow, Equation 2.29 looks complicated. However, it has a nice intuitive interpretation. We’re\\ncomputing the rate of change ofC with respect to a weight in the network. What the equation\\ntells us is that every edge between two neurons in the network is associated with a rate\\nfactor which is just the partial derivative of one neuron’s activation with respect to the other\\nneuron’s activation. The edge from the ﬁrst weight to the ﬁrst neuron has a rate factor\\n∂al\\nj/∂wl\\njk . The rate factor for a path is just the product of the rate factors along the path.\\nAnd the total rate of change ∂C/∂wl\\njk is just the sum of the rate factors of all paths from the\\ninitial weight to the ﬁnal cost. This procedure is illustrated here, for a single path:\\nWhat I’ve been providing up to now is a heuristic argument, a way of thinking about what’s\\ngoing on when you perturb a weight in a network. Let me sketch out a line of thinking you\\ncould use to further develop this argument. First, you could derive explicit expressions for\\nall the individual partial derivatives in Equation 2.29. That’s easy to do with a bit of calculus.\\nHaving done that, you could then try to ﬁgure out how to write all the sums over indices as\\nmatrix multiplications. This turns out to be tedious, and requires some persistence, but not\\nextraordinary insight. After doing all this, and then simplifying as much as possible, what\\nyou discover is that you end up with exactly the backpropagation algorithm! And so you can\\nthink of the backpropagation algorithm as providing a way of computing the sum over the\\nrate factor for all these paths. Or, to put it slightly differently , the backpropagation algorithm\\nis a clever way of keeping track of small perturbations to the weights (and biases) as they\\npropagate through the network, reach the output, and then affect the cost.\\nNow, I’m not going to work through all this here. It’s messy and requires considerable\\ncare to work through all the details. If you’re up for a challenge, you may enjoy attempting it.\\nAnd even if not, I hope this line of thinking gives you some insight into what backpropagation\\nis accomplishing.\\nWhat about the other mystery – how backpropagation could have been discovered in\\nthe ﬁrst place? In fact, if you follow the approach I just sketched you will discover a proof\\nof backpropagation. Unfortunately, the proof is quite a bit longer and more complicated\\nthan the one I described earlier in this chapter. So how was that short (but more mysterious)\\nproof discovered? What you ﬁnd when you write out all the details of the long proof is\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0eb0447e-5208-482e-bea6-72644aa979c9', embedding=None, metadata={'page_label': '57', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.9. Backpropagation: the big picture\\n\\x0c\\x0c\\x0c 57\\nthat, after the fact, there are several obvious simpliﬁcations staring you in the face. You\\nmake those simpliﬁcations, get a shorter proof, and write that out. And then several more\\nobvious simpliﬁcations jump out at you. So you repeat again. The result after a few iterations\\nis the proof we saw earlier 6 – short, but somewhat obscure, because all the signposts to\\nits construction have been removed! I am, of course, asking you to trust me on this, but\\nthere really is no great mystery to the origin of the earlier proof. It’s just a lot of hard work\\nsimplifying the proof I’ve sketched in this section.\\n6There is one clever step required. In Equation 2.29 the intermediate variables are activations like\\nal+1\\nq . The clever idea is to switch to using weighted inputs, like zl+1\\nq , as the intermediate variables. If\\nyou don’t have this idea, and instead continue using the activationsal+1\\nq , the proof you obtain turns out\\nto be slightly more complex than the proof given earlier in the chapter.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='fc9847a3-a48a-4f30-b040-69c232814902', embedding=None, metadata={'page_label': '58', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='58\\n\\x0c\\x0c\\x0c How the backpropagation algorithm works\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0ac81de6-0fb4-4029-bd2e-efc60c6b7010', embedding=None, metadata={'page_label': '59', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 59\\n33333\\nImproving the way neural\\nnetworks learn\\nWhen a golf player is ﬁrst learning to play golf, they usually spend most of their time\\ndeveloping a basic swing. Only gradually do they develop other shots, learning to chip, draw\\nand fade the ball, building on and modifying their basic swing. In a similar way , up to now\\nwe’ve focused on understanding the backpropagation algorithm. It’s our “basic swing”, the\\nfoundation for learning in most work on neural networks. In this chapter I explain a suite of\\ntechniques which can be used to improve on our vanilla implementation of backpropagation,\\nand so improve the way our networks learn.\\nThe techniques we’ll develop in this chapter include: a better choice of cost function,\\nknown as the cross-entropy cost function; four so-called “regularization” methods (L1 and\\nL2 regularization, dropout, and artiﬁcial expansion of the training data), which make our\\nnetworks better at generalizing beyond the training data; a better method for initializing\\nthe weights in the network; and a set of heuristics to help choose good hyper-parameters\\nfor the network. I’ll also overview several other techniques in less depth. The discussions\\nare largely independent of one another, and so you may jump ahead if you wish. We’ll also\\nimplement many of the techniques in running code, and use them to improve the results\\nobtained on the handwriting classiﬁcation problem studied in Chapter 1.\\nOf course, we’re only covering a few of the many, many techniques which have been\\ndeveloped for use in neural nets. The philosophy is that the best entree to the plethora of\\navailable techniques is in-depth study of a few of the most important. Mastering those impor-\\ntant techniques is not just useful in its own right, but will also deepen your understanding of\\nwhat problems can arise when you use neural networks. That will leave you well prepared\\nto quickly pick up other techniques, as you need them.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='40954faf-fa90-4fdb-87b1-64f237422dff', embedding=None, metadata={'page_label': '60', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='60\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n3.1 The cross-entropy cost function\\nMost of us ﬁnd it unpleasant to be wrong. Soon after beginning to learn the piano I gave my\\nﬁrst performance before an audience. I was nervous, and began playing the piece an octave\\ntoo low. I got confused, and couldn’t continue until someone pointed out my error. I was\\nvery embarrassed. Yet while unpleasant, we also learn quickly when we’re decisively wrong.\\nYou can bet that the next time I played before an audience I played in the correct octave! By\\ncontrast, we learn more slowly when our errors are less well-deﬁned.\\nIdeally, we hope and expect that our neural networks will learn fast from their errors.\\nIs this what happens in practice? To answer this question, let’s look at a toy example. The\\nexample involves a neuron with just one input:\\nWe’ll train this neuron to do something ridiculously easy: take the input 1 to the output 0.\\nOf course, this is such a trivial task that we could easily ﬁgure out an appropriate weight and\\nbias by hand, without using a learning algorithm. However, it turns out to be illuminating to\\nuse gradient descent to attempt to learn a weight and bias. So let’s take a look at how the\\nneuron learns.\\nTo make things deﬁnite, I’ll pick the initial weight to be 0.6 and the initial bias to be\\n0.9. These are generic choices used as a place to begin learning, I wasn’t picking them to\\nbe special in any way . The initial output from the neuron is 0.82, so quite a bit of learning\\nwill be needed before our neuron gets near the desired output, 0.0. The learning rate is\\nη=0.15, which turns out to be slow enough that we can follow what’s happening, but fast\\nenough that we can get substantial learning in just a few seconds. The cost is the quadratic\\ncost function, C, introduced back in Chapter 1. I’ll remind you of the exact form of the cost\\nfunction shortly , so there’s no need to go and dig up the deﬁnition.\\nAs you can see, the neuron rapidly learns a weight and bias that drives down the cost, and\\ngives an output from the neuron of about 0.09. That’s not quite the desired output, 0.0, but\\nit is pretty good. Suppose, however, that we instead choose both the starting weight and the\\nstarting bias to be 2.0. In this case the initial output is 0.98, which is very badly wrong. Let’s\\nlook at how the neuron learns to output 0 in this case.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='10696821-44ab-4fa0-97d6-89110c78ba52', embedding=None, metadata={'page_label': '61', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 61\\nAlthough this example uses the same learning rate ( η= 0.15), we can see that learning\\nstarts out much more slowly. Indeed, for the ﬁrst 150 or so learning epochs, the weights\\nand biases don’t change much at all. Then the learning kicks in and, much as in our ﬁrst\\nexample, the neuron’s output rapidly moves closer to 0.0.\\nThis behavior is strange when contrasted to human learning. As I said at the beginning\\nof this section, we often learn fastest when we’re badly wrong about something. But we’ve\\njust seen that our artiﬁcial neuron has a lot of difﬁculty learning when it’s badly wrong – far\\nmore difﬁculty than when it’s just a little wrong. What’s more, it turns out that this behavior\\noccurs not just in this toy model, but in more general networks. Why is learning so slow?\\nAnd can we ﬁnd a way of avoiding this slowdown?\\nTo understand the origin of the problem, consider that our neuron learns by changing the\\nweight and bias at a rate determined by the partial derivatives of the cost function, ∂C/∂w\\nand ∂C/∂b. So saying “learning is slow” is really the same as saying that those partial\\nderivatives are small. The challenge is to understand why they are small. To understand\\nthat, let’s compute the partial derivatives. Recall that we’re using the quadratic cost function,\\nwhich, from Equation 1.6, is given by\\nC = (y −a)2\\n2 , (3.1)\\nwhere a is the neuron’s output when the training input x = 1 is used, and y = 0 is the\\ncorresponding desired output. To write this more explicitly in terms of the weight and bias,\\nrecall that a = σ(z), where z = wx + b. Using the chain rule to differentiate with respect to\\nthe weight and bias we get\\n∂C\\n∂w = ( a −y)σ′(z)x = aσ′(z) (3.2)\\n∂C\\n∂b = ( a −y)σ′(z) =aσ′(z), (3.3)\\nwhere I have substituted x = 1 and y = 0. To understand the behavior of these expressions,\\nlet’s look more closely at theσ′(z) term on the right-hand side. Recall the shape of the σ\\nfunction:\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ef261d0e-66a3-4f05-a969-2f308c4390d1', embedding=None, metadata={'page_label': '62', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='62\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n−6 −4 −2 0 2 4 6\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSigmoid function\\nWe can see from this graph that when the neuron’s output is close to 1, the curve gets very\\nﬂat, and so σ′(z) gets very small. Equations 3.2 and 3.3 then tell us that∂C/∂w and ∂C/∂b\\nget very small. This is the origin of the learning slowdown. What’s more, as we shall see a\\nlittle later, the learning slowdown occurs for essentially the same reason in more general\\nneural networks, not just the toy example we’ve been playing with.\\n3.1.1 Introducing the cross-entropy cost function\\nHow can we address the learning slowdown? It turns out that we can solve the problem by\\nreplacing the quadratic cost with a different cost function, known as the cross-entropy . To\\nunderstand the cross-entropy , let’s move a little away from our super-simple toy model. We’ll\\nsuppose instead that we’re trying to train a neuron with several input variables,x1, x2, . . .,\\ncorresponding weights w1, w2, . . ., and a bias,b:\\nThe output from the neuron is, of course, a = σ(z), where z =\\n∑\\nj wj bj + b is the weighted\\nsum of the inputs. We deﬁne the cross-entropy cost function for this neuron by\\nC = −1\\nn\\n∑\\nx\\n[y ln a + (1 −y) ln(1 −a)] , (3.4)\\nwhere n is the total number of items of training data, the sum is over all training inputs, x,\\nand y is the corresponding desired output.\\nIt’s not obvious that the expression (3.4) ﬁxes the learning slowdown problem. In fact,\\nfrankly , it’s not even obvious that it makes sense to call this a cost function! Before addressing\\nthe learning slowdown, let’s see in what sense the cross-entropy can be interpreted as a cost\\nfunction.\\nTwo properties in particular make it reasonable to interpret the cross-entropy as a cost\\nfunction. First, it’s non-negative, that is,C >0. To see this, notice that: (a) all the individual\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0ad0d5ca-67e2-47f7-8192-ecab3628ca4a', embedding=None, metadata={'page_label': '63', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 63\\nterms in the sum in (3.4) are negative, since both logarithms are of numbers in the range 0\\nto 1; and (b) there is a minus sign out the front of the sum.\\nSecond, if the neuron’s actual output is close to the desired output for all training inputs,\\nx, then the cross-entropy will be close to zero1. To see this, suppose for example that y = 0\\nand a ≈0 for some input x. This is a case when the neuron is doing a good job on that input.\\nWe see that the ﬁrst term in the expression (57) for the cost vanishes, since y = 0, while the\\nsecond term is just −ln(1 −a) ≈0. A similar analysis holds when y = 1 and a ≈1. And so\\nthe contribution to the cost will be low provided the actual output is close to the desired\\noutput.\\nSumming up, the cross-entropy is positive, and tends toward zero as the neuron gets\\nbetter at computing the desired output,y, for all training inputs,x. These are both properties\\nwe’d intuitively expect for a cost function. Indeed, both properties are also satisﬁed by the\\nquadratic cost. So that’s good news for the cross-entropy . But the cross-entropy cost function\\nhas the beneﬁt that, unlike the quadratic cost, it avoids the problem of learning slowing\\ndown. To see this, let’s compute the partial derivative of the cross-entropy cost with respect\\nto the weights. We substitute a = σ(z) into (3.4), and apply the chain rule twice, obtaining:\\n∂C\\n∂wj\\n= −1\\nn\\n∑\\nx\\n\\x81 y\\nσ(z) − 1 −y\\n1 −σ(z)\\n\\x8b ∂σ\\n∂wj\\n= −1\\nn\\n∑\\nx\\n\\x81 y\\nσ(z) − 1 −y\\n1 −σ(z)\\n\\x8b\\nσ′(z)xj. (3.5)\\nPutting everything over a common denominator and simplifying this becomes:\\n∂C\\n∂wj\\n= 1\\nn\\n∑\\nx\\nσ′(z)xj\\nσ(z)(1 −σ(z)) (σ(z) −y). (3.6)\\nUsing the deﬁnition of the sigmoid function, σ(z) =1/(1 + e−z), and a little algebra we can\\nshow that σ′(z) =σ(z)(1 −σ(z)). I’ll ask you to verify this in an exercise below, but for\\nnow let’s accept it as given. We see that theσ′(z) and σ(z)(1 −σ(z)) terms cancel in the\\nequation just above, and it simpliﬁes to become:\\n∂C\\n∂wj\\n= 1\\nn\\n∑\\nx\\nxj(σ(z) −y). (3.7)\\nThis is a beautiful expression. It tells us that the rate at which the weight learns is controlled\\nby σ(z) −y, i.e., by the error in the output. The larger the error, the faster the neuron will\\nlearn. This is just what we’d intuitively expect. In particular, it avoids the learning slowdown\\ncaused by the σ′(z) term in the analogous equation for the quadratic cost, Equation(3.2).\\nWhen we use the cross-entropy, the σ′(z) term gets canceled out, and we no longer need\\nworry about it being small. This cancellation is the special miracle ensured by the cross-\\nentropy cost function. Actually , it’s not really a miracle. As we’ll see later, the cross-entropy\\nwas specially chosen to have just this property .\\nIn a similar way , we can compute the partial derivative for the bias. I won’t go through\\n1To prove this I will need to assume that the desired outputs y are all either 0 or 1. This is usually\\nthe case when solving classiﬁcation problems, for example, or when computing Boolean functions. To\\nunderstand what happens when we don’t make this assumption, see the exercises at the end of this\\nsection.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6c945849-6e4e-4d92-8712-ee8afe0e4de9', embedding=None, metadata={'page_label': '64', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='64\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nall the details again, but you can easily verify that\\n∂C\\n∂b = 1\\nn\\n∑\\nx\\n(σ(z) −y). (3.8)\\nAgain, this avoids the learning slowdown caused by theσ′(z) term in the analogous equation\\nfor the quadratic cost, Equation (3.3).\\nExercise\\n• Verify that σ′(z) =σ(z)(1 −σ(z))\\nLet’s return to the toy example we played with earlier, and explore what happens when we\\nuse the cross-entropy instead of the quadratic cost. To re-orient ourselves, we’ll begin with\\nthe case where the quadratic cost did just ﬁne, with starting weight 0.6 and starting bias 0.9:\\nUnsurprisingly, the neuron learns perfectly well in this instance, just as it did earlier. And\\nnow let’s look at the case where our neuron got stuck before, with the weight and bias both\\nstarting at 2.0:\\nSuccess! This time the neuron learned quickly , just as we hoped. If you observe closely you\\ncan see that the slope of the cost curve was much steeper initially than the initial ﬂat region\\non the corresponding curve for the quadratic cost. It’s that steepness which the cross-entropy\\nbuys us, preventing us from getting stuck just when we’d expect our neuron to learn fastest,\\ni.e., when the neuron starts out badly wrong.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='32e0cb70-03c6-4bcc-8872-55caae037112', embedding=None, metadata={'page_label': '65', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 65\\nI didn’t say what learning rate was used in the examples just illustrated. Earlier, with the\\nquadratic cost, we used η= 0.15. Should we have used the same learning rate in the new\\nexamples? In fact, with the change in cost function it’s not possible to say precisely what it\\nmeans to use the “same” learning rate; it’s an apples and oranges comparison. For both cost\\nfunctions I simply experimented to ﬁnd a learning rate that made it possible to see what is\\ngoing on. If you’re still curious, despite my disavowal, here’s the lowdown: I usedη= 0.005\\nin the examples just given.\\nYou might object that the change in learning rate makes the graphs above meaningless.\\nWho cares how fast the neuron learns, when our choice of learning rate was arbitrary to\\nbegin with?! That objection misses the point. The point of the graphs isn’t about the absolute\\nspeed of learning. It’s about how the speed of learning changes. In particular, when we\\nuse the quadratic cost learning is slower when the neuron is unambiguously wrong than\\nit is later on, as the neuron gets closer to the correct output; while with the cross-entropy\\nlearning is faster when the neuron is unambiguously wrong. Those statements don’t depend\\non how the learning rate is set.\\nWe’ve been studying the cross-entropy for a single neuron. However, it’s easy to generalize\\nthe cross-entropy to many-neuron multi-layer networks. In particular, suppose y = y1, y2, . . .\\nare the desired values at the output neurons, i.e., the neurons in the ﬁnal layer, while\\naL\\n1 , aL\\n2 , . . . are the actual output values. Then we deﬁne the cross-entropy by\\n∑\\nj\\n\\x94\\nyj ln aL\\nj + (1 −yj) ln(1 −aL\\nj )\\n\\x97\\n. (3.9)\\nThis is the same as our earlier expression, Equation (3.4), except now we’ve got the\\n∑\\nj\\nsumming over all the output neurons. I won’t explicitly work through a derivation, but it\\nshould be plausible that using the expression (3.9) avoids a learning slowdown in many-\\nneuron networks. If you’re interested, you can work through the derivation in the problem\\nbelow.\\nIncidentally, I’m using the term “cross-entropy” in a way that has confused some early\\nreaders, since it superﬁcially appears to conﬂict with other sources. In particular, it’s common\\nto deﬁne the cross-entropy for two probability distributions, pj and qj, as\\n∑\\nj pj ln qj. This\\ndeﬁnition may be connected to (3.4), if we treat a single sigmoid neuron as outputting a\\nprobability distribution consisting of the neuron’s activationa and its complement 1 −a.\\nHowever, when we have many sigmoid neurons in the ﬁnal layer, the vector aL\\nj of activa-\\ntions don’t usually form a probability distribution. As a result, a deﬁnition like\\n∑\\nj pj ln qj\\ndoesn’t even make sense, since we’re not working with probability distributions. Instead,\\nyou can think of (3.9) as a summed set of per-neuron cross-entropies, with the activation\\nof each neuron being interpreted as part of a two-element probability distribution2. In this\\nsense, (3.9) is a generalization of the cross-entropy for probability distributions.\\nWhen should we use the cross-entropy instead of the quadratic cost? In fact, the cross-\\nentropy is nearly always the better choice, provided the output neurons are sigmoid neurons.\\nTo see why , consider that when we’re setting up the network we usually initialize the weights\\nand biases using some sort of randomization. It may happen that those initial choices result\\nin the network being decisively wrong for some training input – that is, an output neuron\\nwill have saturated near 1, when it should be 0, or vice versa. If we’re using the quadratic\\ncost that will slow down learning. It won’t stop learning completely , since the weights will\\n2Of course, in our networks there are no probabilistic elements, so they’re not really probabilities.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e814578d-6c97-4625-ac91-baa3563fd759', embedding=None, metadata={'page_label': '66', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='66\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\ncontinue learning from other training inputs, but it’s obviously undesirable.\\nExercises\\n• One gotcha with the cross-entropy is that it can be difﬁcult at ﬁrst to remember the\\nrespective roles of the ys and the as. It’s easy to get confused about whether the right\\nform is\\n−[y ln a + (1 −y) ln(1 −a)].\\nWhat happens to the second of these expressions when y = 0 or 1? Does this problem\\nafﬂict the ﬁrst expression? Why or why not?\\n• In the single-neuron discussion at the start of this section, I argued that the cross-\\nentropy is small if σ(z) ≈ y for all training inputs. The argument relied on y\\nbeing equal to either 0 or 1. This is usually true in classiﬁcation problems, but for\\nother problems (e.g., regression problems) y can sometimes take values intermediate\\nbetween 0 and 1. Show that the cross-entropy is still minimized when σ(z) =y for\\nall training inputs. When this is the case the cross-entropy has the value:\\nC = −1\\nn\\n∑\\nx\\n[y ln y + (1 −y) ln(1 −y)]. (3.10)\\nThe quantity −[y ln y + (1 −y) ln(1 −y)] is sometimes known as the binary entropy.\\nProblems\\n• Many-layer multi-neuron networks In the notation introduced in the last chapter,\\nshow that for the quadratic cost the partial derivative with respect to weights in the\\noutput layer is\\n∂C\\n∂wL\\njk\\n= 1\\nn\\n∑\\nx\\naL−1\\nk (aL\\nj −yj)σ′(zL\\nj ). (3.11)\\nThe term σ′(zL\\nj ) causes a learning slowdown whenever an output neuron saturates\\non the wrong value. Show that for the cross-entropy cost the output error δL for a\\nsingle training example x is given by\\nδL = aL −y. (3.12)\\nUse this expression to show that the partial derivative with respect to the weights in\\nthe output layer is given by\\n∂C\\n∂wL\\njk\\n= 1\\nn\\n∑\\nx\\naL−1\\nk (aL\\nj −yj). (3.13)\\nThe σ′(zL\\nj ) term has vanished, and so the cross-entropy avoids the problem of learning\\nslowdown, not just when used with a single neuron, as we saw earlier, but also in\\nmany-layer multi-neuron networks. A simple variation on this analysis holds also for\\nthe biases. If this is not obvious to you, then you should work through that analysis\\nas well.\\n• Using the quadratic cost when we have linear neurons in the output layer Sup-\\npose that we have a many-layer multi-neuron network. Suppose all the neurons in\\nthe ﬁnal layer are linear neurons, meaning that the sigmoid activation function is not\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='739a127e-11dd-4244-ae8b-67395b70b31d', embedding=None, metadata={'page_label': '67', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 67\\napplied, and the outputs are simply aL\\nj = zL\\nj . Show that if we use the quadratic cost\\nfunction then the output error δL for a single training example x is given by\\nδL = aL −y. (3.14)\\nSimilarly to the previous problem, use this expression to show that the partial deriva-\\ntives with respect to the weights and biases in the output layer are given by\\n∂C\\n∂wL\\njk\\n= 1\\nn\\n∑\\nx\\naL−1\\nk (aL\\nj −yj) (3.15)\\n∂C\\n∂bL\\nj\\n= 1\\nn\\n∑\\nx\\n(aL\\nj −yj). (3.16)\\nThis shows that if the output neurons are linear neurons then the quadratic cost will\\nnot give rise to any problems with a learning slowdown. In this case the quadratic\\ncost is, in fact, an appropriate cost function to use.\\n3.1.2 Using the cross-entropy to classify MNIST digits\\nThe cross-entropy is easy to implement as part of a program which learns using gradient\\ndescent and backpropagation. We’ll do that later in the chapter, developing an improved\\nversion of our earlier program for classifying the MNIST handwritten digits, network.py.\\nThe new program is called network2.py, and incorporates not just the cross-entropy, but\\nalso several other techniques developed in this chapter3. For now, let’s look at how well our\\nnew program classiﬁes MNIST digits. As was the case in Chapter 1, we’ll use a network with\\n30 hidden neurons, and we’ll use a mini-batch size of 10. We set the learning rate toη= 0.54\\nand we train for 30 epochs. The interface to network2.py is slightly different than network.\\npy, but it should still be clear what is going on. You can, by the way , get documentation about\\nnetwork2.py’s interface by using commands such ashelp(network2.Network.SGD) in a\\nPython shell.\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data , 30, 10, 0.5, evaluation_data=test_data ,\\nmonitor_evaluation_accuracy=True)\\n3The code is available on GitHub.\\n4In Chapter 1 we used the quadratic cost and a learning rate of η= 3.0. As discussed above, it’s not\\npossible to say precisely what it means to use the “same” learning rate when the cost function is changed.\\nFor both cost functions I experimented to ﬁnd a learning rate that provides near-optimal performance,\\ngiven the other hyper-parameter choices.\\nThere is, incidentally , a very rough general heuristic for relating the learning rate for the cross-entropy\\nand the quadratic cost. As we saw earlier, the gradient terms for the quadratic cost have an extra\\nσ′= σ(1 −σ) term in them. Suppose we average this over values for σ,\\n∫1\\n0 dσσ(1 −σ) =1/6. We\\nsee that (very roughly) the quadratic cost learns an average of 6 times slower, for the same learning\\nrate. This suggests that a reasonable starting point is to divide the learning rate for the quadratic cost\\nby 6. Of course, this argument is far from rigorous, and shouldn’t be taken too seriously. Still, it can\\nsometimes be a useful starting point.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6327fa34-d01b-4c74-88d1-b984f213a108', embedding=None, metadata={'page_label': '68', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='68\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nNote, by the way , that thenet.large_weight_initializer() command is used to initial-\\nize the weights and biases in the same way as described in Chapter 1. We need to run this\\ncommand because later in this chapter we’ll change the default weight initialization in our\\nnetworks. The result from running the above sequence of commands is a network with 95.49\\npercent accuracy . This is pretty close to the result we obtained in Chapter 1, 95.42 percent,\\nusing the quadratic cost.\\nLet’s look also at the case where we use 100 hidden neurons, the cross-entropy, and\\notherwise keep the parameters the same. In this case we obtain an accuracy of 96.82 percent.\\nThat’s a substantial improvement over the results from Chapter 1, where we obtained a\\nclassiﬁcation accuracy of 96.59 percent, using the quadratic cost. That may look like a small\\nchange, but consider that the error rate has dropped from 3.41 percent to 3.18 percent.\\nThat is, we’ve eliminated about one in fourteen of the original errors. That’s quite a handy\\nimprovement.\\nIt’s encouraging that the cross-entropy cost gives us similar or better results than the\\nquadratic cost. However, these results don’t conclusively prove that the cross-entropy is a\\nbetter choice. The reason is that I’ve put only a little effort into choosing hyper-parameters\\nsuch as learning rate, mini-batch size, and so on. For the improvement to be really convincing\\nwe’d need to do a thorough job optimizing such hyper-parameters. Still, the results are\\nencouraging, and reinforce our earlier theoretical argument that the cross-entropy is a better\\nchoice than the quadratic cost.\\nThis, by the way, is part of a general pattern that we’ll see through this chapter and,\\nindeed, through much of the rest of the book. We’ll develop a new technique, we’ll try it out,\\nand we’ll get “improved” results. It is, of course, nice that we see such improvements. But\\nthe interpretation of such improvements is always problematic. They’re only truly convincing\\nif we see an improvement after putting tremendous effort into optimizing all the other\\nhyper-parameters. That’s a great deal of work, requiring lots of computing power, and we’re\\nnot usually going to do such an exhaustive investigation. Instead, we’ll proceed on the basis\\nof informal tests like those done above. Still, you should keep in mind that such tests fall\\nshort of deﬁnitive proof, and remain alert to signs that the arguments are breaking down.\\nBy now, we’ve discussed the cross-entropy at great length. Why go to so much effort\\nwhen it gives only a small improvement to our MNIST results? Later in the chapter we’ll\\nsee other techniques – notably, regularization – which give much bigger improvements.\\nSo why so much focus on cross-entropy? Part of the reason is that the cross-entropy is a\\nwidely-used cost function, and so is worth understanding well. But the more important\\nreason is that neuron saturation is an important problem in neural nets, a problem we’ll\\nreturn to repeatedly throughout the book. And so I’ve discussed the cross-entropy at length\\nbecause it’s a good laboratory to begin understanding neuron saturation and how it may be\\naddressed.\\n3.1.3 What does the cross-entropy mean? Where does it come from?\\nOur discussion of the cross-entropy has focused on algebraic analysis and practical implemen-\\ntation. That’s useful, but it leaves unanswered broader conceptual questions, like: what does\\nthe cross-entropy mean? Is there some intuitive way of thinking about the cross-entropy?\\nAnd how could we have dreamed up the cross-entropy in the ﬁrst place?\\nLet’s begin with the last of these questions: what could have motivated us to think up the\\ncross-entropy in the ﬁrst place? Suppose we’d discovered the learning slowdown described\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='750e7ac7-2f98-40a2-93d3-74eb9b0a7eba', embedding=None, metadata={'page_label': '69', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 69\\nearlier, and understood that the origin was the σ′(z) terms in Equations (3.2) and (3.3).\\nAfter staring at those equations for a bit, we might wonder if it’s possible to choose a cost\\nfunction so that theσ′(z) term disappeared. In that case, the cost C = Cx for a single training\\nexample x would satisfy\\n∂C\\n∂wj\\n= xj(a −y) (3.17)\\n∂C\\n∂b = ( a −y). (3.18)\\nIf we could choose the cost function to make these equations true, then they would capture\\nin a simple way the intuition that the greater the initial error, the faster the neuron learns.\\nThey’d also eliminate the problem of a learning slowdown. In fact, starting from these\\nequations we’ll now show that it’s possible to derive the form of the cross-entropy , simply by\\nfollowing our mathematical noses. To see this, note that from the chain rule we have\\n∂C\\n∂b = ∂C\\n∂a σ′(z). (3.19)\\nUsing σ′(z) =σ(z)(1 −σ(z)) =a(1 −a) the last equation becomes\\n∂C\\n∂b = ∂C\\n∂a a(1 −a). (3.20)\\nComparing to Equation 3.18 we obtain\\n∂C\\n∂a = a −y\\na(1 −a) . (3.21)\\nIntegrating this expression with respect to a gives\\nC = −[y ln a + (1 −y) ln(1 −a)] +constant, (3.22)\\nfor some constant of integration. This is the contribution to the cost from a single training\\nexample, x. To get the full cost function we must average over training examples, obtaining\\nC = −1\\nn\\n∑\\nx\\n[y ln a + (1 −y) ln(1 −a)] +constant, (3.23)\\nwhere the constant here is the average of the individual constants for each training exam-\\nple. And so we see that Equations (3.17) and (3.18) uniquely determine the form of the\\ncross-entropy, up to an overall constant term. The cross-entropy isn’t something that was\\nmiraculously pulled out of thin air. Rather, it’s something that we could have discovered in a\\nsimple and natural way .\\nWhat about the intuitive meaning of the cross-entropy? How should we think about it?\\nExplaining this in depth would take us further aﬁeld than I want to go. However, it is worth\\nmentioning that there is a standard way of interpreting the cross-entropy that comes from the\\nﬁeld of information theory . Roughly speaking, the idea is that the cross-entropy is a measure\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='63277e45-1859-4867-bbe2-bf23957098da', embedding=None, metadata={'page_label': '70', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='70\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nof surprise. In particular, our neuron is trying to compute the function x →y = y(x). But\\ninstead it computes the function x →a = a(x). Suppose we think of a as our neuron’s\\nestimated probability that y is 1, and 1−a is the estimated probability that the right value for\\ny is 0. Then the cross-entropy measures how “surprised” we are, on average, when we learn\\nthe true value for y. We get low surprise if the output is what we expect, and high surprise\\nif the output is unexpected. Of course, I haven’t said exactly what “surprise” means, and so\\nthis perhaps seems like empty verbiage. But in fact there is a precise information-theoretic\\nway of saying what is meant by surprise. Unfortunately, I don’t know of a good, short,\\nself-contained discussion of this subject that’s available online. But if you want to dig deeper,\\nthen Wikipedia contains a brief summary that will get you started down the right track. And\\nthe details can be ﬁlled in by working through the materials about the Kraft inequality in\\nchapter 5 of the book about information theory by Cover and Thomas.\\nProblem\\n• We’ve discussed at length the learning slowdown that can occur when output neurons\\nsaturate, in networks using the quadratic cost to train. Another factor that may inhibit\\nlearning is the presence of the xj term in Equation (3.7). Because of this term, when\\nan input xj is near to zero, the corresponding weight wj will learn slowly. Explain\\nwhy it is not possible to eliminate the xj term through a clever choice of cost function.\\n3.1.4 Softmax\\nIn this chapter we’ll mostly use the cross-entropy cost to address the problem of learning\\nslowdown. However, I want to brieﬂy describe another approach to the problem, based on\\nwhat are called softmax layers of neurons. We’re not actually going to use softmax layers in\\nthe remainder of the chapter, so if you’re in a great hurry, you can skip to the next section.\\nHowever, softmax is still worth understanding, in part because it’s intrinsically interesting,\\nand in part because we’ll use softmax layers in Chapter 6, in our discussion of deep neural\\nnetworks.\\nThe idea of softmax is to deﬁne a new type of output layer for our neural networks.\\nIt begins in the same way as with a sigmoid layer, by forming the weighted inputs 5 zL\\nj =∑\\nk wL\\njk aL−1\\nk + bL\\nj . However, we don’t apply the sigmoid function to get the output. Instead, in\\na softmax layer we apply the so-called softmax function to the zL\\nj . According to this function,\\nthe activation aL\\nj of the j-th output neuron is\\naL\\nj = ex L\\nj\\n∑\\nk ezL\\nk\\n(3.24)\\nwhere in the denominator we sum over all the output neurons.\\nIf you’re not familiar with the softmax function, Equation (3.24) may look pretty opaque.\\nIt’s certainly not obvious why we’d want to use this function. And it’s also not obvious that this\\nwill help us address the learning slowdown problem. To better understand Equation (3.24),\\nsuppose we have a network with four output neurons, and four corresponding weighted\\ninputs, which we’ll denotezL\\n1 , zL\\n2 , zL\\n3 , and zL\\n4 . Figure 3.1 shows a graph of the corresponding\\noutput activations for different inputs 6. As you increase zL\\n4 , you’ll see an increase in the\\n5In describing the softmax we’ll make frequent use of notation introduced in the last chapter. You\\nmay wish to revisit that chapter if you need to refresh your memory about the meaning of the notation.\\n6This paragraph is an adaptation of an animation from online version of the book.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d00c9154-2638-4748-b5e1-ea3442e07e01', embedding=None, metadata={'page_label': '71', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1. The cross-entropy cost function\\n\\x0c\\x0c\\x0c 71\\n−4 −2 0 2 40\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nz4\\naj, j ∈(1 . . . 4)\\na1(z1 = −1) a2(z2 = 0)\\na3(z3 = 1) a4\\nFigure 3.1: Equation 3.24 for different ﬁxed values of zL\\n1,2,3 and variable zL\\n4 . L index avoided for clarity .\\ncorresponding output activation, aL\\n4 , and a decrease in the other output activations. Similarly ,\\nif you decrease zL\\n4 then aL\\n4 will decrease, and all the other output activations will increase. In\\nfact, if you look closely , you’ll see that in both cases the total change in the other activations\\nexactly compensates for the change in aL\\n4 . The reason is that the output activations are\\nguaranteed to always sum up to 1, as we can prove using Equation (3.24) and a little algebra:\\n∑\\nj\\naL\\nj =\\n∑\\nj ezL\\nj\\n∑\\nk ezL\\nk\\n= 1. (3.25)\\nAs a result, if aL\\n4 increases, then the other output activations must decrease by the same total\\namount, to ensure the sum over all activations remains 1. And, of course, similar statements\\nhold for all the other activations.\\nEquation (3.24) also implies that the output activations are all positive, since the ex-\\nponential function is positive. Combining this with the observation in the last paragraph,\\nwe see that the output from the softmax layer is a set of positive numbers which sum up\\nto 1. In other words, the output from the softmax layer can be thought of as a probability\\ndistribution.\\nThe fact that a softmax layer outputs a probability distribution is rather pleasing. In many\\nproblems it’s convenient to be able to interpret the output activation aL\\nj as the network’s\\nestimate of the probability that the correct output is j. So, for instance, in the MNIST\\nclassiﬁcation problem, we can interpret aL\\nj as the network’s estimated probability that the\\ncorrect digit classiﬁcation is j.\\nBy contrast, if the output layer was a sigmoid layer, then we certainly couldn’t assume\\nthat the activations formed a probability distribution. I won’t explicitly prove it, but it should\\nbe plausible that the activations from a sigmoid layer won’t in general form a probability\\ndistribution. And so with a sigmoid output layer we don’t have such a simple interpretation\\nof the output activations.\\nExercise\\n• Construct an example showing explicitly that in a network with a sigmoid output layer,\\nthe output activations aL\\nj won’t always sum to 1.\\nWe’re starting to build up some feel for the softmax function and the way softmax layers\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b43c46e9-6e5b-4dea-97d3-f73690a8bd41', embedding=None, metadata={'page_label': '72', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='72\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nbehave. Just to review where we’re at: the exponentials in Equation (3.24) ensure that\\nall the output activations are positive. And the sum in the denominator of Equation (3.24)\\nensures that the softmax outputs sum to 1. So that particular form no longer appears so\\nmysterious: rather, it is a natural way to ensure that the output activations form a probability\\ndistribution. You can think of softmax as a way of rescaling the zL\\nj , and then squishing them\\ntogether to form a probability distribution.\\nExercises\\n• Monotonicity of softmaxShow that ∂aL\\nj /∂zL\\nk is positive if j = k and negative if j ̸= k.\\nAs a consequence, increasing zL\\nj is guaranteed to increase the corresponding output\\nactivation, aL\\nj , and will decrease all the other output activations. We already saw this\\nempirically with the sliders, but this is a rigorous proof.\\n• Non-locality of softmax A nice thing about sigmoid layers is that the output aL\\nj is a\\nfunction of the corresponding weighted input, aL\\nj = σ(zL\\nj ). Explain why this is not\\nthe case for a softmax layer: any particular output activation aL\\nj depends on all the\\nweighted inputs.\\nProblem\\n• Inverting the softmax layerSuppose we have a neural network with a softmax output\\nlayer, and the activationsaL\\nj are known. Show that the corresponding weighted inputs\\nhave the form zL\\nj = ln aL\\nj + C, for some constant C that is independent of j.\\nThe learning slowdown problem: We’ve now built up considerable familiarity with softmax\\nlayers of neurons. But we haven’t yet seen how a softmax layer lets us address the learning\\nslowdown problem. To understand that, let’s deﬁne thelog-likelihood cost function. We’ll\\nuse x to denote a training input to the network, and y to denote the corresponding desired\\noutput. Then the log-likelihood cost associated to this training input is\\nC ≡−ln aL\\nj (3.26)\\nSo, for instance, if we’re training with MNIST images, and input an image of a 7, then\\nthe log-likelihood cost is −ln aL\\n7 . To see that this makes intuitive sense, consider the case\\nwhen the network is doing a good job, that is, it is conﬁdent the input is a 7. In that case\\nit will estimate a value for the corresponding probability aL\\n7 which is close to 1, and so the\\ncost −ln aL\\n7 will be small. By contrast, when the network isn’t doing such a good job, the\\nprobability aL\\n7 will be smaller, and the cost −ln aL\\n7 will be larger. So the log-likelihood cost\\nbehaves as we’d expect a cost function to behave.\\nWhat about the learning slowdown problem? To analyze that, recall that the key to the\\nlearning slowdown is the behaviour of the quantities ∂C/∂wL\\njk and ∂C/∂bL\\nj . I won’t go\\nthrough the derivation explicitly – I’ll ask you to do in the problems, below – but with a little\\nalgebra you can show that7\\n∂C\\n∂bL\\nj\\n= aL\\nj −yj (3.27)\\n∂C\\n∂wL\\njk\\n= aL−1\\nk (aL\\nj −yj) (3.28)\\n7Note that I’m abusing notation here, usingy in a slightly different way to last paragraph. In the last\\nparagraph we used y to denote the desired output from the network – e.g., output a “7” if an image of a\\n7 was input. But in the equations which follow I’m usingy to denote the vector of output activations\\nwhich corresponds to 7, that is, a vector which is all 0s, except for a 1 in the 7th location.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='86ad0ac8-8be7-4e5f-8997-8787b84684a8', embedding=None, metadata={'page_label': '73', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 73\\nThese equations are the same as the analogous expressions obtained in our earlier analysis\\nof the cross-entropy. Compare, for example, Equation (3.28) to Equation (3.13). It’s the\\nsame equation, albeit in the latter I’ve averaged over training instances. And, just as in the\\nearlier analysis, these expressions ensure that we will not encounter a learning slowdown.\\nIn fact, it’s useful to think of a softmax output layer with log-likelihood cost as being quite\\nsimilar to a sigmoid output layer with cross-entropy cost.\\nGiven this similarity, should you use a sigmoid output layer and cross-entropy, or a\\nsoftmax output layer and log-likelihood? In fact, in many situations both approaches work\\nwell. Through the remainder of this chapter we’ll use a sigmoid output layer, with the\\ncross-entropy cost. Later, in Chapter 6, we’ll sometimes use a softmax output layer, with\\nlog-likelihood cost. The reason for the switch is to make some of our later networks more\\nsimilar to networks found in certain inﬂuential academic papers. As a more general point\\nof principle, softmax plus log-likelihood is worth using whenever you want to interpret the\\noutput activations as probabilities. That’s not always a concern, but can be useful with\\nclassiﬁcation problems (like MNIST) involving disjoint classes.\\nProblems\\n• Derive Equations (3.27) and (3.28).\\n• Where does the “softmax” name come from? Suppose we change the softmax\\nfunction so the output activations are given by\\naL\\nj = ecz L\\nj\\n∑\\nk ecz L\\nk\\n, (3.29)\\nwhere c is a positive constant. Note that c = 1 corresponds to the standard softmax\\nfunction. But if we use a different value of c we get a different function, which\\nis nonetheless qualitatively rather similar to the softmax. In particular, show that\\nthe output activations form a probability distribution, just as for the usual softmax.\\nSuppose we allow c to become large, i.e., c →∞. What is the limiting value for the\\noutput activations aL\\nj ? After solving this problem it should be clear to you why we\\nthink of the c = 1 function as a “softened” version of the maximum function. This is\\nthe origin of the term “softmax”.\\n• Backpropagation with softmax and the log-likelihood cost In the last chapter we\\nderived the backpropagation algorithm for a network containing sigmoid layers. To\\napply the algorithm to a network with a softmax layer we need to ﬁgure out an\\nexpression for the errorδL\\nj ≡∂C/∂zL\\nj in the ﬁnal layer. Show that a suitable expression\\nis:\\nδL\\nj = aL\\nj −yj. (3.30)\\nUsing this expression we can apply the backpropagation algorithm to a network using\\na softmax output layer and the log-likelihood cost.\\n3.2 Overﬁtting and regularization\\nThe Nobel prize winning physicist Enrico Fermi was once asked his opinion of a mathematical\\nmodel some colleagues had proposed as the solution to an important unsolved physics\\nproblem. The model gave excellent agreement with experiment, but Fermi was skeptical. He\\nasked how many free parameters could be set in the model. “Four” was the answer. Fermi\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0d837329-ff6d-41d9-a164-537a3e627100', embedding=None, metadata={'page_label': '74', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='74\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nreplied8 : “I remember my friend Johnny von Neumann used to say , with four parameters I\\ncan ﬁt an elephant, and with ﬁve I can make him wiggle his trunk.”.\\nThe point, of course, is that models with a large number of free parameters can describe\\nan amazingly wide range of phenomena. Even if such a model agrees well with the available\\ndata, that doesn’t make it a good model. It may just mean there’s enough freedom in the\\nmodel that it can describe almost any data set of the given size, without capturing any\\ngenuine insights into the underlying phenomenon. When that happens the model will work\\nwell for the existing data, but will fail to generalize to new situations. The true test of a\\nmodel is its ability to make predictions in situations it hasn’t been exposed to before.\\nFermi and von Neumann were suspicious of models with four parameters. Our 30\\nhidden neuron network for classifying MNIST digits has nearly 24,000 parameters! That’s\\na lot of parameters. Our 100 hidden neuron network has nearly 80,000 parameters, and\\nstate-of-the-art deep neural nets sometimes contain millions or even billions of parameters.\\nShould we trust the results?\\nLet’s sharpen this problem up by constructing a situation where our network does a\\nbad job generalizing to new situations. We’ll use our 30 hidden neuron network, with its\\n23,860 parameters. But we won’t train the network using all 50,000 MNIST training images.\\nInstead, we’ll use just the ﬁrst 1,000 training images. Using that restricted set will make\\nthe problem with generalization much more evident. We’ll train in a similar way to before,\\nusing the cross-entropy cost function, with a learning rate of η= 0.5 and a mini-batch size\\nof 10. However, we’ll train for 400 epochs, a somewhat larger number than before, because\\nwe’re not using as many training examples. Let’s usenetwork2 to look at the way the cost\\nfunction changes:\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data [:1000], 400, 10, 0.5, evaluation_data=test_data ,\\nmonitor_evaluation_accuracy=True , monitor_training_cost=True)\\nUsing the results we can plot the way the cost changes as the network learns9 :\\n8The quote comes from a charming article by Freeman Dyson, who is one of the people who proposed\\nthe ﬂawed model. A four-parameter elephant may be found here.\\n9This and the next four graphs were generated by the program overﬁtting.py.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='34dcc83a-0dac-4db0-9cb4-8f5a7d077cc9', embedding=None, metadata={'page_label': '75', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 75\\nThis looks encouraging, showing a smooth decrease in the cost, just as we expect. Note that\\nI’ve only shown training epochs 200 through 399. This gives us a nice up-close view of the\\nlater stages of learning, which, as we’ll see, turns out to be where the interesting action is.\\nLet’s now look at how the classiﬁcation accuracy on the test data changes over time:\\nAgain, I’ve zoomed in quite a bit. In the ﬁrst 200 epochs (not shown) the accuracy rises to\\njust under 82 percent. The learning then gradually slows down. Finally, at around epoch\\n280 the classiﬁcation accuracy pretty much stops improving. Later epochs merely see small\\nstochastic ﬂuctuations near the value of the accuracy at epoch 280. Contrast this with the\\nearlier graph, where the cost associated to the training data continues to smoothly drop.\\nIf we just look at that cost, it appears that our model is still getting “better”. But the test\\naccuracy results show the improvement is an illusion. Just like the model that Fermi disliked,\\nwhat our network learns after epoch 280 no longer generalizes to the test data. And so it’s\\nnot useful learning. We say the network is overﬁtting or overtraining beyond epoch 280.\\nYou might wonder if the problem here is that I’m looking at the cost on the training\\ndata, as opposed to the classiﬁcation accuracy on the test data. In other words, maybe the\\nproblem is that we’re making an apples and oranges comparison. What would happen if we\\ncompared the cost on the training data with the cost on the test data, so we’re comparing\\nsimilar measures? Or perhaps we could compare the classiﬁcation accuracy on both the\\ntraining data and the test data? In fact, essentially the same phenomenon shows up no\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5233df87-f99c-478f-aa15-8601799d8add', embedding=None, metadata={'page_label': '76', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='76\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nmatter how we do the comparison. The details do change, however. For instance, let’s look\\nat the cost on the test data:\\nWe can see that the cost on the test data improves until around epoch 15, but after that it\\nactually starts to get worse, even though the cost on the training data is continuing to get\\nbetter. This is another sign that our model is overﬁtting. It poses a puzzle, though, which is\\nwhether we should regard epoch 15 or epoch 280 as the point at which overﬁtting is coming\\nto dominate learning? From a practical point of view, what we really care about is improving\\nclassiﬁcation accuracy on the test data, while the cost on the test data is no more than a\\nproxy for classiﬁcation accuracy. And so it makes most sense to regard epoch 280 as the\\npoint beyond which overﬁtting is dominating learning in our neural network.\\nAnother sign of overﬁtting may be seen in the classiﬁcation accuracy on the training\\ndata:\\nThe accuracy rises all the way up to 100 percent. That is, our network correctly classiﬁes all\\n1,000 training images! Meanwhile, our test accuracy tops out at just 82.27 percent. So our\\nnetwork really is learning about peculiarities of the training set, not just recognizing digits\\nin general. It’s almost as though our network is merely memorizing the training set, without\\nunderstanding digits well enough to generalize to the test set.\\nOverﬁtting is a major problem in neural networks. This is especially true in modern\\nnetworks, which often have very large numbers of weights and biases. To train effectively,\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='41420f33-2dd3-49dc-980a-3d0666be29ba', embedding=None, metadata={'page_label': '77', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 77\\nwe need a way of detecting when overﬁtting is going on, so we don’t overtrain. And we’d\\nlike to have techniques for reducing the effects of overﬁtting.\\nThe obvious way to detect overﬁtting is to use the approach above, keeping track of\\naccuracy on the test data as our network trains. If we see that the accuracy on the test\\ndata is no longer improving, then we should stop training. Of course, strictly speaking, this\\nis not necessarily a sign of overﬁtting. It might be that accuracy on the test data and the\\ntraining data both stop improving at the same time. Still, adopting this strategy will prevent\\noverﬁtting.\\nIn fact, we’ll use a variation on this strategy. Recall that when we load in the MNIST\\ndata we load in three data sets:\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\nUp to now we’ve been using the training_data and test_data, and ignoring the val-\\nidation_data. The validation_data contains 10,000 images of digits, images which\\nare different from the 50,000 images in the MNIST training set, and the 10,000 images in\\nthe MNIST test set. Instead of using the test_data to prevent overﬁtting, we will use the\\nvalidation_data. To do this, we’ll use much the same strategy as was described above for\\nthe test_data. That is, we’ll compute the classiﬁcation accuracy on thevalidation_data\\nat the end of each epoch. Once the classiﬁcation accuracy on the validation_data has\\nsaturated, we stop training. This strategy is called early stopping. Of course, in practice we\\nwon’t immediately know when the accuracy has saturated. Instead, we continue training\\nuntil we’re conﬁdent that the accuracy has saturated10.\\nWhy use the validation_data to prevent overﬁtting, rather than the test_data? In\\nfact, this is part of a more general strategy , which is to use thevalidation_data to evaluate\\ndifferent trial choices of hyper-parameters such as the number of epochs to train for, the\\nlearning rate, the best network architecture, and so on. We use such evaluations to ﬁnd\\nand set good values for the hyper-parameters. Indeed, although I haven’t mentioned it until\\nnow, that is, in part, how I arrived at the hyper-parameter choices made earlier in this book.\\n(More on this later.)\\nOf course, that doesn’t in any way answer the question of why we’re using thevalida-\\ntion_data to prevent overﬁtting, rather than the test_data. Instead, it replaces it with\\na more general question, which is why we’re using thevalidation_data rather than the\\ntest_data to set good hyper-parameters? To understand why , consider that when setting\\nhyper-parameters we’re likely to try many different choices for the hyper-parameters. If we\\nset the hyper-parameters based on evaluations of the test_data it’s possible we’ll end up\\noverﬁtting our hyper-parameters to the test_data. That is, we may end up ﬁnding hyper-\\nparameters which ﬁt particular peculiarities of the test_data, but where the performance\\nof the network won’t generalize to other data sets. We guard against that by ﬁguring out the\\nhyper-parameters using the validation_data. Then, once we’ve got the hyper-parameters\\nwe want, we do a ﬁnal evaluation of accuracy using thetest_data. That gives us conﬁdence\\nthat our results on the test_data are a true measure of how well our neural network gener-\\nalizes. To put it another way , you can think of the validation data as a type of training data\\n10It requires some judgment to determine when to stop. In my earlier graphs I identiﬁed epoch 280 as\\nthe place at which accuracy saturated. It’s possible that was too pessimistic. Neural networks sometimes\\nplateau for a while in training, before continuing to improve. I wouldn’t be surprised if more learning\\ncould have occurred even after epoch 400, although the magnitude of any further improvement would\\nlikely be small. So it’s possible to adopt more or less aggressive strategies for early stopping.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e706a400-4747-47f8-a8b8-1be6a0937e3f', embedding=None, metadata={'page_label': '78', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='78\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nthat helps us learn good hyper-parameters. This approach to ﬁnding good hyper-parameters\\nis sometimes known as the hold out method, since the validation_data is kept apart or\\n“held out” from thetraining_data.\\nNow, in practice, even after evaluating performance on the test_data we may change\\nour minds and want to try another approach – perhaps a different network architecture –\\nwhich will involve ﬁnding a new set of hyper-parameters. If we do this, isn’t there a danger\\nwe’ll end up overﬁtting to thetest_data as well? Do we need a potentially inﬁnite regress\\nof data sets, so we can be conﬁdent our results will generalize? Addressing this concern fully\\nis a deep and difﬁcult problem. But for our practical purposes, we’re not going to worry too\\nmuch about this question. Instead, we’ll plunge ahead, using the basic hold out method,\\nbased on the training_data, validation_data, and test_data, as described above.\\nWe’ve been looking so far at overﬁtting when we’re just using 1,000 training images.\\nWhat happens when we use the full training set of 50,000 images? We’ll keep all the other\\nparameters the same (30 hidden neurons, learning rate 0.5, mini-batch size of 10), but\\ntrain using all 50,000 images for 30 epochs. Here’s a graph showing the results for the\\nclassiﬁcation accuracy on both the training data and the test data. Note that I’ve used the\\ntest data here, rather than the validation data, in order to make the results more directly\\ncomparable with the earlier graphs.\\nAs you can see, the accuracy on the test and training data remain much closer together than\\nwhen we were using 1,000 training examples. In particular, the best classiﬁcation accuracy\\nof 97.86 percent on the training data is only 2.53 percent higher than the 95.33 percent on\\nthe test data. That’s compared to the 17.73 percent gap we had earlier! Overﬁtting is still\\ngoing on, but it’s been greatly reduced. Our network is generalizing much better from the\\ntraining data to the test data. In general, one of the best ways of reducing overﬁtting is to\\nincrease the size of the training data. With enough training data it is difﬁcult for even a very\\nlarge network to overﬁt. Unfortunately , training data can be expensive or difﬁcult to acquire,\\nso this is not always a practical option.\\n3.2.1 Regularization\\nIncreasing the amount of training data is one way of reducing overﬁtting. Are there other\\nways we can reduce the extent to which overﬁtting occurs? One possible approach is to\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='83444f51-24a3-4436-8249-8e793703b0e9', embedding=None, metadata={'page_label': '79', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 79\\nreduce the size of our network. However, large networks have the potential to be more\\npowerful than small networks, and so this is an option we’d only adopt reluctantly .\\nFortunately , there are other techniques which can reduce overﬁtting, even when we have\\na ﬁxed network and ﬁxed training data. These are known as regularization techniques. In\\nthis section I describe one of the most commonly used regularization techniques, a technique\\nsometimes known as weight decay or L2 regularization. The idea of L2 regularization is to\\nadd an extra term to the cost function, a term called the regularization term. Here’s the\\nregularized cross-entropy:\\nC = −1\\nn\\n∑\\nx j\\n\\x94\\nyj ln aL\\nj + (1 −yj) ln(1 −aL\\nj )\\n\\x97\\n+ λ\\n2n\\n∑\\nw\\nw2. (3.31)\\nThe ﬁrst term is just the usual expression for the cross-entropy. But we’ve added a second\\nterm, namely the sum of the squares of all the weights in the network. This is scaled by a\\nfactor λ/2n, where λ> 0 is known as the regularization parameter, and n is, as usual, the\\nsize of our training set. I’ll discuss later how λis chosen. It’s also worth noting that the\\nregularization term doesn’t include the biases. I’ll also come back to that below.\\nOf course, it’s possible to regularize other cost functions, such as the quadratic cost. This\\ncan be done in a similar way:\\nC = 1\\n2n\\n∑\\nx\\n∥y −aL∥2 + λ\\n2n\\n∑\\nw\\nw2. (3.32)\\nIn both cases we can write the regularized cost function as\\nC = C0 + λ\\n2n\\n∑\\nw\\nw2, (3.33)\\nwhere C0 is the original, unregularized cost function.\\nIntuitively , the effect of regularization is to make it so the network prefers to learn small\\nweights, all other things being equal. Large weights will only be allowed if they considerably\\nimprove the ﬁrst part of the cost function. Put another way, regularization can be viewed\\nas a way of compromising between ﬁnding small weights and minimizing the original cost\\nfunction. The relative importance of the two elements of the compromise depends on the\\nvalue of λ: when λis small we prefer to minimize the original cost function, but when λis\\nlarge we prefer small weights.\\nNow, it’s really not at all obvious why making this kind of compromise should help reduce\\noverﬁtting! But it turns out that it does. We’ll address the question of why it helps in the\\nnext section. But ﬁrst, let’s work through an example showing that regularization really does\\nreduce overﬁtting.\\nTo construct such an example, we ﬁrst need to ﬁgure out how to apply our stochastic\\ngradient descent learning algorithm in a regularized neural network. In particular, we need\\nto know how to compute the partial derivatives ∂C/∂w and ∂C/∂b for all the weights and\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e6d0e02a-92b7-4139-bca9-0c49eae523d4', embedding=None, metadata={'page_label': '80', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='80\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nbiases in the network. Taking the partial derivatives of Equation (3.33) gives\\n∂C\\n∂w = ∂C0\\n∂w + λ\\nn w (3.34)\\n∂C\\n∂b = ∂C0\\n∂b . (3.35)\\nThe ∂C0/∂w and ∂C0/∂b terms can be computed using backpropagation, as described in\\nthe last chapter. And so we see that it’s easy to compute the gradient of the regularized cost\\nfunction: just use backpropagation, as usual, and then add λ\\nn w to the partial derivative of all\\nthe weight terms. The partial derivatives with respect to the biases are unchanged, and so\\nthe gradient descent learning rule for the biases doesn’t change from the usual rule:\\nb →b −η∂C0\\n∂b . (3.36)\\nThe learning rule for the weights becomes:\\nw → w −η∂C0\\n∂w −ηλ\\nn w =\\n\\x81\\n1 −ηλ\\nn\\n\\x8b\\nw −η∂C0\\n∂w . (3.37)\\nThis is exactly the same as the usual gradient descent learning rule, except we ﬁrst rescale\\nthe weight w by a factor 1 −ηλ\\nn . This rescaling is sometimes referred to as weight decay,\\nsince it makes the weights smaller. At ﬁrst glance it looks as though this means the weights\\nare being driven unstoppably toward zero. But that’s not right, since the other term may\\nlead the weights to increase, if so doing causes a decrease in the unregularized cost function.\\nOkay , that’s how gradient descent works. What about stochastic gradient descent? Well,\\njust as in unregularized stochastic gradient descent, we can estimate ∂C0/∂w by averaging\\nover a mini-batch of m training examples. Thus the regularized learning rule for stochastic\\ngradient descent becomes (c.f. Equation (1.20))\\nw →\\n\\x81\\n1 −ηλ\\nn\\n\\x8b\\nw −η\\nm\\n∑\\nx\\n∂Cx\\n∂w , (3.38)\\nwhere the sum is over training examples x in the mini-batch, and Cx is the (unregularized)\\ncost for each training example. This is exactly the same as the usual rule for stochastic\\ngradient descent, except for the 1 −ηλ/n weight decay factor. Finally , and for completeness,\\nlet me state the regularized learning rule for the biases. This is, of course, exactly the same\\nas in the unregularized case (c.f. Equation 1.21),\\nb →b −η\\nm\\n∑\\nx\\n∂Cx\\n∂b , (3.39)\\nwhere the sum is over training examples x in the mini-batch.\\nLet’s see how regularization changes the performance of our neural network. We’ll use\\na network with 30 hidden neurons, a mini-batch size of 10, a learning rate of 0.5, and\\nthe cross-entropy cost function. However, this time we’ll use a regularization parameter\\nof λ= 0.1. Note that in the code, we use the variable name lmbda, because lambda is a\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b6e34377-65cf-4acd-9711-bef290cc12a8', embedding=None, metadata={'page_label': '81', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 81\\nreserved word in Python, with an unrelated meaning. I’ve also used thetest_data again,\\nnot the validation_data. Strictly speaking, we should use the validation_data, for all\\nthe reasons we discussed earlier. But I decided to use the test_data because it makes\\nthe results more directly comparable with our earlier, unregularized results. You can easily\\nchange the code to use the validation_data instead, and you’ll ﬁnd that it gives similar\\nresults.\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data [:1000], 400, 10, 0.5, evaluation_data=test_data , lmbda\\n= 0.1, monitor_evaluation_cost=True , monitor_evaluation_accuracy=True ,\\nmonitor_training_cost=True , monitor_training_accuracy=True)\\nThe cost on the training data decreases over the whole time, much as it did in the earlier,\\nunregularized case11:\\nBut this time the accuracy on the test_data continues to increase for the entire 400 epochs:\\n11This and the next two graphs were produced with the program overfitting.py.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7b21b740-747d-4ba6-842e-2da456063821', embedding=None, metadata={'page_label': '82', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='82\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nClearly, the use of regularization has suppressed overﬁtting. What’s more, the accuracy is\\nconsiderably higher, with a peak classiﬁcation accuracy of 87.1 percent, compared to the peak\\nof 82.27 percent obtained in the unregularized case. Indeed, we could almost certainly get\\nconsiderably better results by continuing to train past 400 epochs. It seems that, empirically ,\\nregularization is causing our network to generalize better, and considerably reducing the\\neffects of overﬁtting.\\nWhat happens if we move out of the artiﬁcial environment of just having 1,000 training\\nimages, and return to the full 50,000 image training set? Of course, we’ve seen already that\\noverﬁtting is much less of a problem with the full 50,000 images. Does regularization help\\nany further? Let’s keep the hyper-parameters the same as before – 30 epochs, learning rate\\n0.5, mini-batch size of 10. However, we need to modify the regularization parameter. The\\nreason is because the size n of the training set has changed from n=1,000 to n=50,000, and\\nthis changes the weight decay factor 1 −ηλ/n. If we continued to use λ= 0.1 that would\\nmean much less weight decay , and thus much less of a regularization effect. We compensate\\nby changing to λ= 5.0.\\nOkay , let’s train our network, stopping ﬁrst to re-initialize the weights:\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data , 30, 10, 0.5, evaluation_data=test_data , lmbda = 5.0,\\n... monitor_evaluation_accuracy=True , monitor_training_accuracy=True)\\nWe obtain the results:\\nThere’s lots of good news here. First, our classiﬁcation accuracy on the test data is up, from\\n95.49 percent when running unregularized, to 96.49 percent. That’s a big improvement.\\nSecond, we can see that the gap between results on the training and test data is much\\nnarrower than before, running at under a percent. That’s still a signiﬁcant gap, but we’ve\\nobviously made substantial progress reducing overﬁtting.\\nFinally , let’s see what test classiﬁcation accuracy we get when we use 100 hidden neurons\\nand a regularization parameter ofλ= 5.0. I won’t go through a detailed analysis of overﬁtting\\nhere, this is purely for fun, just to see how high an accuracy we can get when we use our\\nnew tricks: the cross-entropy cost function and L2 regularization.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='57548fdd-98b2-4e1b-9e76-48a787a63b08', embedding=None, metadata={'page_label': '83', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 83\\n>>> net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data , 30, 10, 0.5, lmbda=5.0, evaluation_data=\\nvalidation_data ,\\n... monitor_evaluation_accuracy=True)\\nThe ﬁnal result is a classiﬁcation accuracy of 97.92 percent on the validation data. That’s a\\nbig jump from the 30 hidden neuron case. In fact, tuning just a little more, to run for 60\\nepochs at η= 0.1 and λ= 5.0 we break the 98 percent barrier, achieving 98.04 percent\\nclassiﬁcation accuracy on the validation data. Not bad for what turns out to be 152 lines of\\ncode!\\nI’ve described regularization as a way to reduce overﬁtting and to increase classiﬁcation\\naccuracies. In fact, that’s not the only beneﬁt. Empirically, when doing multiple runs of\\nour MNIST networks, but with different (random) weight initializations, I’ve found that the\\nunregularized runs will occasionally get “stuck”, apparently caught in local minima of the\\ncost function. The result is that different runs sometimes provide quite different results. By\\ncontrast, the regularized runs have provided much more easily replicable results.\\nWhy is this going on? Heuristically , if the cost function is unregularized, then the length\\nof the weight vector is likely to grow, all other things being equal. Over time this can lead\\nto the weight vector being very large indeed. This can cause the weight vector to get stuck\\npointing in more or less the same direction, since changes due to gradient descent only make\\ntiny changes to the direction, when the length is long. I believe this phenomenon is making\\nit hard for our learning algorithm to properly explore the weight space, and consequently\\nharder to ﬁnd good minima of the cost function.\\n3.2.2 Why does regularization help reduce overﬁtting?\\nWe’ve seen empirically that regularization helps reduce overﬁtting. That’s encouraging but,\\nunfortunately, it’s not obvious why regularization helps! A standard story people tell to\\nexplain what’s going on is along the following lines: smaller weights are, in some sense,\\nlower complexity , and so provide a simpler and more powerful explanation for the data, and\\nshould thus be preferred. That’s a pretty terse story , though, and contains several elements\\nthat perhaps seem dubious or mystifying. Let’s unpack the story and examine it critically . To\\ndo that, let’s suppose we have a simple data set for which we wish to build a model:\\n0 1 2 3 4 50\\n5\\n10\\nx\\ny\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8a9416e4-41b6-4c9d-95df-54353d5d49bf', embedding=None, metadata={'page_label': '84', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='84\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nImplicitly, we’re studying some real-world phenomenon here, withx and y representing\\nreal-world data. Our goal is to build a model which lets us predict y as a function of x. We\\ncould try using neural networks to build such a model, but I’m going to do something even\\nsimpler: I’ll try to model y as a polynomial in x. I’m doing this instead of using neural nets\\nbecause using polynomials will make things particularly transparent. Once we’ve understood\\nthe polynomial case, we’ll translate to neural networks. Now, there are ten points in the graph\\nabove, which means we can ﬁnd a unique 9-th-order polynomial y = a0 x9 + a1 x8 + . . .+ a9\\nwhich ﬁts the data exactly . Here’s the graph of that polynomial12 :\\n0 1 2 3 4 50\\n5\\n10\\nx\\ny\\nThat provides an exact ﬁt. But we can also get a good ﬁt using the linear model y = 2x:\\n0 1 2 3 4 50\\n5\\n10\\nx\\ny\\nWhich of these is the better model? Which is more likely to be true? And which model is more\\nlikely to generalize well to other examples of the same underlying real-world phenomenon?\\nThese are difﬁcult questions. In fact, we can’t determine with certainty the answer to\\nany of the above questions, without much more information about the underlying real-world\\nphenomenon. But let’s consider two possibilities: (1) the 9th order polynomial is, in fact,\\nthe model which truly describes the real-world phenomenon, and the model will therefore\\ngeneralize perfectly; (2) the correct model is y = 2x, but there’s a little additional noise due\\nto, say , measurement error, and that’s why the model isn’t an exact ﬁt.\\n12I won’t show the coefﬁcients explicitly , although they are easy to ﬁnd using a routine such as Numpy’s\\npolyfit.You can view the exact form of the polynomial in the source code for the graph if you’re curious.\\nIt’s the functionp(x) deﬁned starting on line 14 of the program which produces the graph.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d0e3e37-4436-445d-ab68-3ca5e440a310', embedding=None, metadata={'page_label': '85', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 85\\nIt’s nota priori possible to say which of these two possibilities is correct. (Or, indeed, if\\nsome third possibility holds). Logically , either could be true. And it’s not a trivial difference.\\nIt’s true that on the data provided there’s only a small difference between the two models.\\nBut suppose we want to predict the value of y corresponding to some large value of x, much\\nlarger than any shown on the graph above. If we try to do that there will be a dramatic\\ndifference between the predictions of the two models, as the 9th order polynomial model\\ncomes to be dominated by the x9 term, while the linear model remains, well, linear.\\nOne point of view is to say that in science we should go with the simpler explanation,\\nunless compelled not to. When we ﬁnd a simple model that seems to explain many data\\npoints we are tempted to shout “Eureka!” After all, it seems unlikely that a simple explanation\\nshould occur merely by coincidence. Rather, we suspect that the model must be expressing\\nsome underlying truth about the phenomenon. In the case at hand, the modely = 2x +noise\\nseems much simpler than y = a0 x9 + a1 x8 + . . .. It would be surprising if that simplicity had\\noccurred by chance, and so we suspect that y = 2x + noise expresses some underlying truth.\\nIn this point of view, the 9th order model is really just learning the effects of local noise. And\\nso while the 9th order model works perfectly for these particular data points, the model will\\nfail to generalize to other data points, and the noisy linear model will have greater predictive\\npower.\\nLet’s see what this point of view means for neural networks. Suppose our network mostly\\nhas small weights, as will tend to happen in a regularized network. The smallness of the\\nweights means that the behaviour of the network won’t change too much if we change a few\\nrandom inputs here and there. That makes it difﬁcult for a regularized network to learn the\\neffects of local noise in the data. Think of it as a way of making it so single pieces of evidence\\ndon’t matter too much to the output of the network. Instead, a regularized network learns\\nto respond to types of evidence which are seen often across the training set. By contrast, a\\nnetwork with large weights may change its behaviour quite a bit in response to small changes\\nin the input. And so an unregularized network can use large weights to learn a complex\\nmodel that carries a lot of information about the noise in the training data. In a nutshell,\\nregularized networks are constrained to build relatively simple models based on patterns\\nseen often in the training data, and are resistant to learning peculiarities of the noise in the\\ntraining data. The hope is that this will force our networks to do real learning about the\\nphenomenon at hand, and to generalize better from what they learn.\\nWith that said, this idea of preferring simpler explanation should make you nervous.\\nPeople sometimes refer to this idea as “Occam’s Razor”, and will zealously apply it as though\\nit has the status of some general scientiﬁc principle. But, of course, it’s not a general scientiﬁc\\nprinciple. There is no a priori logical reason to prefer simple explanations over more complex\\nexplanations. Indeed, sometimes the more complex explanation turns out to be correct.\\nLet me describe two examples where more complex explanations have turned out to be\\ncorrect. In the 1940s the physicist Marcel Schein announced the discovery of a new particle\\nof nature. The company he worked for, General Electric, was ecstatic, and publicized the\\ndiscovery widely. But the physicist Hans Bethe was skeptical. Bethe visited Schein, and\\nlooked at the plates showing the tracks of Schein’s new particle. Schein showed Bethe plate\\nafter plate, but on each plate Bethe identiﬁed some problem that suggested the data should\\nbe discarded. Finally, Schein showed Bethe a plate that looked good. Bethe said it might\\njust be a statistical ﬂuke. Schein: “Yes, but the chance that this would be statistics, even\\naccording to your own formula, is one in ﬁve.” Bethe: “But we have already looked at ﬁve\\nplates.” Finally, Schein said: “But on my plates, each one of the good plates, each one of\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5133fb5f-65ff-45d6-a127-73ed144ba1f0', embedding=None, metadata={'page_label': '86', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='86\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nthe good pictures, you explain by a different theory, whereas I have one hypothesis that\\nexplains all the plates, that they are [the new particle].” Bethe replied: “The sole difference\\nbetween your and my explanations is that yours is wrong and all of mine are right. Your\\nsingle explanation is wrong, and all of my multiple explanations are right.” Subsequent work\\nconﬁrmed that Nature agreed with Bethe, and Schein’s particle is no more13.\\nAs a second example, in 1859 the astronomer Urbain Le Verrier observed that the orbit\\nof the planet Mercury doesn’t have quite the shape that Newton’s theory of gravitation says it\\nshould have. It was a tiny , tiny deviation from Newton’s theory , and several of the explanations\\nproferred at the time boiled down to saying that Newton’s theory was more or less right, but\\nneeded a tiny alteration. In 1916, Einstein showed that the deviation could be explained very\\nwell using his general theory of relativity , a theory radically different to Newtonian gravitation,\\nand based on much more complex mathematics. Despite that additional complexity , today it’s\\naccepted that Einstein’s explanation is correct, and Newtonian gravity , even in its modiﬁed\\nforms, is wrong. This is in part because we now know that Einstein’s theory explains many\\nother phenomena which Newton’s theory has difﬁculty with. Furthermore, and even more\\nimpressively , Einstein’s theory accurately predicts several phenomena which aren’t predicted\\nby Newtonian gravity at all. But these impressive qualities weren’t entirely obvious in the\\nearly days. If one had judged merely on the grounds of simplicity , then some modiﬁed form\\nof Newton’s theory would arguably have been more attractive.\\nThere are three morals to draw from these stories. First, it can be quite a subtle business\\ndeciding which of two explanations is truly “simpler”. Second, even if we can make such a\\njudgment, simplicity is a guide that must be used with great caution! Third, the true test of\\na model is not simplicity , but rather how well it does in predicting new phenomena, in new\\nregimes of behaviour.\\nWith that said, and keeping the need for caution in mind, it’s an empirical fact that\\nregularized neural networks usually generalize better than unregularized networks. And so\\nthrough the remainder of the book we will make frequent use of regularization. I’ve included\\nthe stories above merely to help convey why no-one has yet developed an entirely convincing\\ntheoretical explanation for why regularization helps networks generalize. Indeed, researchers\\ncontinue to write papers where they try different approaches to regularization, compare\\nthem to see which works better, and attempt to understand why different approaches work\\nbetter or worse. And so you can view regularization as something of a kludge. While it often\\nhelps, we don’t have an entirely satisfactory systematic understanding of what’s going on,\\nmerely incomplete heuristics and rules of thumb.\\nThere’s a deeper set of issues here, issues which go to the heart of science. It’s the\\nquestion of how we generalize. Regularization may give us a computational magic wand\\nthat helps our networks generalize better, but it doesn’t give us a principled understanding\\nof how generalization works, nor of what the best approach is14.\\nThis is particularly galling because in everyday life, we humans generalize phenomenally\\nwell. Shown just a few images of an elephant a child will quickly learn to recognize other\\nelephants. Of course, they may occasionally make mistakes, perhaps confusing a rhinoceros\\nfor an elephant, but in general this process works remarkably accurately. So we have a\\n13The story is related by the physicist Richard Feynman in an interview with the historian Charles\\nWeiner.\\n14These issues go back to the problem of induction, famously discussed by the Scottish philosopher\\nDavid Hume in “An Enquiry Concerning Human Understanding” (1748). The problem of induction has\\nbeen given a modern machine learning form in the no-free lunch theorem of David Wolpert and William\\nMacready (1997).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8112e2bd-cc5d-4615-b72d-698b47589ab6', embedding=None, metadata={'page_label': '87', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 87\\nsystem – the human brain – with a huge number of free parameters. And after being shown\\njust one or a few training images that system learns to generalize to other images. Our\\nbrains are, in some sense, regularizing amazingly well! How do we do it? At this point we\\ndon’t know. I expect that in years to come we will develop more powerful techniques for\\nregularization in artiﬁcial neural networks, techniques that will ultimately enable neural\\nnets to generalize well even from small data sets.\\nIn fact, our networks already generalize better than one might a priori expect. A network\\nwith 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our\\ntraining data. It’s like trying to ﬁt an 80,000th degree polynomial to 50,000 data points. By\\nall rights, our network should overﬁt terribly. And yet, as we saw earlier, such a network\\nactually does a pretty good job generalizing. Why is that the case? It’s not well understood.\\nIt has been conjectured15 that “the dynamics of gradient descent learning in multilayer nets\\nhas a ‘self-regularization’ effect”. This is exceptionally fortunate, but it’s also somewhat\\ndisquieting that we don’t understand why it’s the case. In the meantime, we will adopt the\\npragmatic approach and use regularization whenever we can. Our neural networks will be\\nthe better for it.\\nLet me conclude this section by returning to a detail which I left unexplained earlier:\\nthe fact that L2 regularization doesn’t constrain the biases. Of course, it would be easy to\\nmodify the regularization procedure to regularize the biases. Empirically, doing this often\\ndoesn’t change the results very much, so to some extent it’s merely a convention whether to\\nregularize the biases or not. However, it’s worth noting that having a large bias doesn’t make\\na neuron sensitive to its inputs in the same way as having large weights. And so we don’t\\nneed to worry about large biases enabling our network to learn the noise in our training data.\\nAt the same time, allowing large biases gives our networks more ﬂexibility in behaviour – in\\nparticular, large biases make it easier for neurons to saturate, which is sometimes desirable.\\nFor these reasons we don’t usually include bias terms when regularizing.\\n3.2.3 Other techniques for regularization\\nThere are many regularization techniques other than L2 regularization. In fact, so many\\ntechniques have been developed that I can’t possibly summarize them all. In this section I\\nbrieﬂy describe three other approaches to reducing overﬁtting: L1 regularization, dropout,\\nand artiﬁcially increasing the training set size. We won’t go into nearly as much depth\\nstudying these techniques as we did earlier. Instead, the purpose is to get familiar with\\nthe main ideas, and to appreciate something of the diversity of regularization techniques\\navailable.\\nL1 regularization: In this approach we modify the unregularized cost function by adding\\nthe sum of the absolute values of the weights:\\nC = C0 + λ\\nn\\n∑\\nw\\n|w|. (3.40)\\nIntuitively , this is similar to L2 regularization, penalizing large weights, and tending to make\\nthe network prefer small weights. Of course, the L1 regularization term isn’t the same as the\\nL2 regularization term, and so we shouldn’t expect to get exactly the same behaviour. Let’s\\n15In Gradient-Based Learning Applied to Document Recognition, by Yann LeCun, Léon Bottou, Yoshua\\nBengio, and Patrick Haffner (1998).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e5a42eef-1d29-49fe-968d-615f3ed155b7', embedding=None, metadata={'page_label': '88', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='88\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\ntry to understand how the behaviour of a network trained using L1 regularization differs\\nfrom a network trained using L2 regularization.\\nTo do that, we’ll look at the partial derivatives of the cost function. Differentiating (95)\\nwe obtain:\\n∂C\\n∂w = ∂C0\\n∂w + λ\\nn sgn(w), (3.41)\\nwhere sgn(w) is the sign of w, that is, +1 if w is positive, and −1 if w is negative. Using this\\nexpression, we can easily modify backpropagation to do stochastic gradient descent using L1\\nregularization. The resulting update rule for an L1 regularized network is\\nw →w′= w −ηλ\\nn sgn(w) −η∂C0\\n∂w , (3.42)\\nwhere, as per usual, we can estimate ∂C0/∂w using a mini-batch average, if we wish.\\nCompare that to the update rule for L2 regularization (c.f. Equation (3.38)),\\nw →w′= w\\n\\x81\\n1 −ηλ\\nn\\n\\x8b\\n−η∂C0\\n∂w . (3.43)\\nIn both expressions the effect of regularization is to shrink the weights. This accords with our\\nintuition that both kinds of regularization penalize large weights. But the way the weights\\nshrink is different. In L1 regularization, the weights shrink by a constant amount toward 0. In\\nL2 regularization, the weights shrink by an amount which is proportional to w. And so when\\na particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much\\nless than L2 regularization does. By contrast, when |w|is small, L1 regularization shrinks\\nthe weight much more than L2 regularization. The net result is that L1 regularization tends\\nto concentrate the weight of the network in a relatively small number of high-importance\\nconnections, while the other weights are driven toward zero.\\nI’ve glossed over an issue in the above discussion, which is that the partial derivative\\n∂C/∂w isn’t deﬁned whenw = 0. The reason is that the function |w|has a sharp “corner”\\nat w = 0, and so isn’t differentiable at that point. That’s okay, though. What we’ll do is\\njust apply the usual (unregularized) rule for stochastic gradient descent when w = 0. That\\nshould be okay – intuitively , the effect of regularization is to shrink weights, and obviously it\\ncan’t shrink a weight which is already 0. To put it more precisely , we’ll use Equations (3.41)\\nand (3.42) with the convention that sgn(0) =0. That gives a nice, compact rule for doing\\nstochastic gradient descent with L1 regularization.\\nDropout: Dropout is a radically different technique for regularization. Unlike L1 and\\nL2 regularization, dropout doesn’t rely on modifying the cost function. Instead, in dropout\\nwe modify the network itself. Let me describe the basic mechanics of how dropout works,\\nbefore getting into why it works, and what the results are.\\nSuppose we’re trying to train a network:\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cee7daa6-caee-42d6-ad40-35e932736a39', embedding=None, metadata={'page_label': '89', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 89\\nIn particular, suppose we have a training input x and corresponding desired output y. Ordi-\\nnarily , we’d train by forward-propagatingx through the network, and then backpropagating\\nto determine the contribution to the gradient. With dropout, this process is modiﬁed. We\\nstart by randomly (and temporarily) deleting half the hidden neurons in the network, while\\nleaving the input and output neurons untouched. After doing this, we’ll end up with a\\nnetwork along the following lines. Note that the dropout neurons, i.e., the neurons which\\nhave been temporarily deleted, are still ghosted in:\\nWe forward-propagate the input x through the modiﬁed network, and then backpropagate\\nthe result, also through the modiﬁed network. After doing this over a mini-batch of examples,\\nwe update the appropriate weights and biases. We then repeat the process, ﬁrst restoring the\\ndropout neurons, then choosing a new random subset of hidden neurons to delete, estimating\\nthe gradient for a different mini-batch, and updating the weights and biases in the network.\\nBy repeating this process over and over, our network will learn a set of weights and\\nbiases. Of course, those weights and biases will have been learnt under conditions in which\\nhalf the hidden neurons were dropped out. When we actually run the full network that\\nmeans that twice as many hidden neurons will be active. To compensate for that, we halve\\nthe weights outgoing from the hidden neurons.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0265fe0d-cae0-477f-bfd9-df252ac8880d', embedding=None, metadata={'page_label': '90', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='90\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nThis dropout procedure may seem strange and ad hoc. Why would we expect it to help\\nwith regularization? To explain what’s going on, I’d like you to brieﬂy stop thinking about\\ndropout, and instead imagine training neural networks in the standard way (no dropout). In\\nparticular, imagine we train several different neural networks, all using the same training\\ndata. Of course, the networks may not start out identical, and as a result after training\\nthey may sometimes give different results. When that happens we could use some kind\\nof averaging or voting scheme to decide which output to accept. For instance, if we have\\ntrained ﬁve networks, and three of them are classifying a digit as a “3”, then it probably really\\nis a “3”. The other two networks are probably just making a mistake. This kind of averaging\\nscheme is often found to be a powerful (though expensive) way of reducing overﬁtting. The\\nreason is that the different networks may overﬁt in different ways, and averaging may help\\neliminate that kind of overﬁtting.\\nWhat’s this got to do with dropout? Heuristically, when we dropout different sets\\nof neurons, it’s rather like we’re training different neural networks. And so the dropout\\nprocedure is like averaging the effects of a very large number of different networks. The\\ndifferent networks will overﬁt in different ways, and so, hopefully , the net effect of dropout\\nwill be to reduce overﬁtting.\\nA related heuristic explanation for dropout is given in one of the earliest papers to use\\nthe technique16: “This technique reduces complex co-adaptations of neurons, since a neuron\\ncannot rely on the presence of particular other neurons. It is, therefore, forced to learn more\\nrobust features that are useful in conjunction with many different random subsets of the\\nother neurons.” In other words, if we think of our network as a model which is making\\npredictions, then we can think of dropout as a way of making sure that the model is robust\\nto the loss of any individual piece of evidence. In this, it’s somewhat similar to L1 and L2\\nregularization, which tend to reduce weights, and thus make the network more robust to\\nlosing any individual connection in the network.\\nOf course, the true measure of dropout is that it has been very successful in improving\\nthe performance of neural networks. The original paper17 introducing the technique applied\\nit to many different tasks. For us, it’s of particular interest that they applied dropout to\\nMNIST digit classiﬁcation, using a vanilla feedforward neural network along lines similar to\\nthose we’ve been considering. The paper noted that the best result anyone had achieved\\nup to that point using such an architecture was 98.4 percent classiﬁcation accuracy on the\\ntest set. They improved that to 98.7 percent accuracy using a combination of dropout and\\na modiﬁed form of L2 regularization. Similarly impressive results have been obtained for\\nmany other tasks, including problems in image and speech recognition, and natural language\\nprocessing. Dropout has been especially useful in training large, deep networks, where the\\nproblem of overﬁtting is often acute.\\nArtiﬁcially expanding the training data: We saw earlier that our MNIST classiﬁcation\\naccuracy dropped down to percentages in the mid-80s when we used only 1,000 training\\nimages. It’s not surprising that this is the case, since less training data means our network\\nwill be exposed to fewer variations in the way human beings write digits. Let’s try training\\nour 30 hidden neuron network with a variety of different training data set sizes, to see\\nhow performance varies. We train using a mini-batch size of 10, a learning rate η= 0.5,\\n16ImageNet Classiﬁcation with Deep Convolutional Neural Networks, by Alex Krizhevsky , Ilya Sutskever,\\nand Geoffrey Hinton (2012).\\n17Improving neural networks by preventing co-adaptation of feature detectors by Geoffrey Hinton,\\nNitish Srivastava, Alex Krizhevsky , Ilya Sutskever, and Ruslan Salakhutdinov (2012). Note that the paper\\ndiscusses a number of subtleties that I have glossed over in this brief introduction.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e0e4f0f8-348f-40dd-9411-a51e782ce5b8', embedding=None, metadata={'page_label': '91', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 91\\na regularization parameter λ= 5.0, and the cross-entropy cost function. We will train\\nfor 30 epochs when the full training data set is used, and scale up the number of epochs\\nproportionally when smaller training sets are used. To ensure the weight decay factor remains\\nthe same across training sets, we will use a regularization parameter of λ= 5.0 when the\\nfull training data set is used, and scale down λproportionally when smaller training sets are\\nused18.\\nAs you can see, the classiﬁcation accuracies improve considerably as we use more training\\ndata. Presumably this improvement would continue still further if more data was available.\\nOf course, looking at the graph above it does appear that we’re getting near saturation.\\nSuppose, however, that we redo the graph with the training set size plotted logarithmically:\\nIt seems clear that the graph is still going up toward the end. This suggests that if we used\\nvastly more training data – say , millions or even billions of handwriting samples, instead of\\njust 50,000 – then we’d likely get considerably better performance, even from this very small\\nnetwork.\\nObtaining more training data is a great idea. Unfortunately , it can be expensive, and so\\nis not always possible in practice. However, there’s another idea which can work nearly as\\nwell, and that’s to artiﬁcially expand the training data. Suppose, for example, that we take\\nan MNIST training image of a ﬁve,\\n18This and the next two graph are produced with the program more_data.py.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f96d7cb8-fc28-4f8a-90c6-6bb3f32243bd', embedding=None, metadata={'page_label': '92', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='92\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nand rotate it by a small amount, let’s say 15 degrees:\\nIt’s still recognizably the same digit. And yet at the pixel level it’s quite different to any image\\ncurrently in the MNIST training data. It’s conceivable that adding this image to the training\\ndata might help our network learn more about how to classify digits. What’s more, obviously\\nwe’re not limited to adding just this one image. We can expand our training data by making\\nmany small rotations of all the MNIST training images, and then using the expanded training\\ndata to improve our network’s performance.\\nThis idea is very powerful and has been widely used. Let’s look at some of the results\\nfrom a paper19 which applied several variations of the idea to MNIST . One of the neural\\nnetwork architectures they considered was along similar lines to what we’ve been using, a\\nfeedforward network with 800 hidden neurons and using the cross-entropy cost function.\\nRunning the network with the standard MNIST training data they achieved a classiﬁcation\\naccuracy of 98.4 percent on their test set. But then they expanded the training data, using\\nnot just rotations, as I described above, but also translating and skewing the images. By\\ntraining on the expanded data set they increased their network’s accuracy to 98.9 percent.\\nThey also experimented with what they called “elastic distortions”, a special type of image\\ndistortion intended to emulate the random oscillations found in hand muscles. By using the\\nelastic distortions to expand the data they achieved an even higher accuracy , 99.3 percent.\\nEffectively , they were broadening the experience of their network by exposing it to the sort\\nof variations that are found in real handwriting.\\nVariations on this idea can be used to improve performance on many learning tasks, not\\njust handwriting recognition. The general principle is to expand the training data by applying\\noperations that reﬂect real-world variation. It’s not difﬁcult to think of ways of doing this.\\nSuppose, for example, that you’re building a neural network to do speech recognition. We\\nhumans can recognize speech even in the presence of distortions such as background noise.\\nAnd so you can expand your data by adding background noise. We can also recognize speech\\nif it’s sped up or slowed down. So that’s another way we can expand the training data. These\\ntechniques are not always used – for instance, instead of expanding the training data by\\nadding noise, it may well be more efﬁcient to clean up the input to the network by ﬁrst\\napplying a noise reduction ﬁlter. Still, it’s worth keeping the idea of expanding the training\\ndata in mind, and looking for opportunities to apply it.\\nExercise\\n• As discussed above, one way of expanding the MNIST training data is to use small\\nrotations of training images. What’s a problem that might occur if we allow arbitrarily\\nlarge rotations of training images?\\nAn aside on big data and what it means to compare classiﬁcation accuracies:Let’s look\\nagain at how our neural network’s accuracy varies with training set size:\\n19Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis by Patrice\\nSimard, Dave Steinkraus, and John Platt (2003).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4f005603-4b05-41ff-9086-162e806d4395', embedding=None, metadata={'page_label': '93', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2. Overﬁtting and regularization\\n\\x0c\\x0c\\x0c 93\\nSuppose that instead of using a neural network we use some other machine learning technique\\nto classify digits. For instance, let’s try using the support vector machines (SVM) which we\\nmet brieﬂy back in Chapter 1. As was the case in Chapter 1, don’t worry if you’re not familiar\\nwith SVMs, we don’t need to understand their details. Instead, we’ll use the SVM supplied\\nby the scikit-learn library . Here’s how SVM performance varies as a function of training set\\nsize. I’ve plotted the neural net results as well, to make comparison easy20:\\nProbably the ﬁrst thing that strikes you about this graph is that our neural network outper-\\nforms the SVM for every training set size. That’s nice, although you shouldn’t read too much\\ninto it, since I just used the out-of-the-box settings from scikit-learn’s SVM, while we’ve done\\na fair bit of work improving our neural network. A more subtle but more interesting fact\\nabout the graph is that if we train our SVM using 50,000 images then it actually has better\\nperformance (94.48 percent accuracy) than our neural network does when trained using\\n5,000 images (93.24 percent accuracy). In other words, more training data can sometimes\\ncompensate for differences in the machine learning algorithm used.\\nSomething even more interesting can occur. Suppose we’re trying to solve a problem\\nusing two machine learning algorithms, algorithm A and algorithm B. It sometimes happens\\nthat algorithm A will outperform algorithm B with one set of training data, while algorithm\\n20This graph was produced with the program more_data.py (as were the last few graphs).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='47d46477-09ec-48dc-ae62-2b41757a4238', embedding=None, metadata={'page_label': '94', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='94\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nB will outperform algorithm A with a different set of training data. We don’t see that above –\\nit would require the two graphs to cross – but it does happen21. The correct response to the\\nquestion “Is algorithm A better than algorithm B?” is really: “What training data set are you\\nusing?”\\nAll this is a caution to keep in mind, both when doing development, and when reading\\nresearch papers. Many papers focus on ﬁnding new tricks to wring out improved performance\\non standard benchmark data sets. “Our whiz-bang technique gave us an improvement of X\\npercent on standard benchmark Y” is a canonical form of research claim. Such claims are\\noften genuinely interesting, but they must be understood as applying only in the context of\\nthe speciﬁc training data set used. Imagine an alternate history in which the people who\\noriginally created the benchmark data set had a larger research grant. They might have used\\nthe extra money to collect more training data. It’s entirely possible that the “improvement”\\ndue to the whiz-bang technique would disappear on a larger data set. In other words, the\\npurported improvement might be just an accident of history. The message to take away,\\nespecially in practical applications, is that what we want is both better algorithms and better\\ntraining data. It’s ﬁne to look for better algorithms, but make sure you’re not focusing on\\nbetter algorithms to the exclusion of easy wins getting more or better training data.\\nProblem\\n• (Research problem) How do our machine learning algorithms perform in the limit of\\nvery large data sets? For any given algorithm it’s natural to attempt to deﬁne a notion\\nof asymptotic performance in the limit of truly big data. A quick-and-dirty approach\\nto this problem is to simply try ﬁtting curves to graphs like those shown above, and\\nthen to extrapolate the ﬁtted curves out to inﬁnity. An objection to this approach\\nis that different approaches to curve ﬁtting will give different notions of asymptotic\\nperformance. Can you ﬁnd a principled justiﬁcation for ﬁtting to some particular class\\nof curves? If so, compare the asymptotic performance of several different machine\\nlearning algorithms.\\nSumming up: We’ve now completed our dive into overﬁtting and regularization. Of course,\\nwe’ll return again to the issue. As I’ve mentioned several times, overﬁtting is a major problem\\nin neural networks, especially as computers get more powerful, and we have the ability to\\ntrain larger networks. As a result there’s a pressing need to develop powerful regularization\\ntechniques to reduce overﬁtting, and this is an extremely active area of current work.\\n3.3 Weight initialization\\nWhen we create our neural networks, we have to make choices for the initial weights and\\nbiases. Up to now, we’ve been choosing them according to a prescription which I discussed\\nonly brieﬂy back in Chapter 1. Just to remind you, that prescription was to choose both the\\nweights and biases using independent Gaussian random variables, normalized to have mean\\n0 and standard deviation 1. While this approach has worked well, it was quite ad hoc, and\\nit’s worth revisiting to see if we can ﬁnd a better way of setting our initial weights and biases,\\nand perhaps help our neural networks learn faster.\\nIt turns out that we can do quite a bit better than initializing with normalized Gaussians.\\nTo see why, suppose we’re working with a network with a large number – say 1,000 – of\\ninput neurons. And let’s suppose we’ve used normalized Gaussians to initialize the weights\\n21Striking examples may be found in Scaling to very very large corpora for natural language disam-\\nbiguation, by Michele Banko and Eric Brill (2001).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='702d1eba-2ece-44b9-a94c-c001347b05b1', embedding=None, metadata={'page_label': '95', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3. Weight initialization\\n\\x0c\\x0c\\x0c 95\\nconnecting to the ﬁrst hidden layer. For now I’m going to concentrate speciﬁcally on the\\nweights connecting the input neurons to the ﬁrst neuron in the hidden layer, and ignore the\\nrest of the network:\\nWe’ll suppose for simplicity that we’re trying to train using a training inputx in which half\\nthe input neurons are on, i.e., set to 1, and half the input neurons are off, i.e., set to 0. The\\nargument which follows applies more generally , but you’ll get the gist from this special case.\\nLet’s consider the weighted sumz =\\n∑\\nj wj xj + b of inputs to our hidden neuron. 500 terms\\nin this sum vanish, because the corresponding input xj is zero. And so z is a sum over a total\\nof 501 normalized Gaussian random variables, accounting for the 500 weight terms and the\\n1 extra bias term. Thus z is itself distributed as a Gaussian with mean zero and standard\\ndeviation\\np\\n501 ≈22.4. That is, z has a very broad Gaussian distribution, not sharply peaked\\nat all:\\n−30 −20 −10 10 20 30\\n0.01\\n0.02\\nIn particular, we can see from this graph that it’s quite likely that |z|will be pretty large,\\ni.e., either z ≫1 or z ≪−1. If that’s the case then the outputσ(z) from the hidden neuron\\nwill be very close to either 1 or 0. That means our hidden neuron will have saturated.\\nAnd when that happens, as we know, making small changes in the weights will make only\\nabsolutely miniscule changes in the activation of our hidden neuron. That miniscule change\\nin the activation of the hidden neuron will, in turn, barely affect the rest of the neurons in\\nthe network at all, and we’ll see a correspondingly miniscule change in the cost function.\\nAs a result, those weights will only learn very slowly when we use the gradient descent\\nalgorithm22. It’s similar to the problem we discussed earlier in this chapter, in which output\\nneurons which saturated on the wrong value caused learning to slow down. We addressed\\n22We discussed this in more detail in Chapter 2, where we used the equations of backpropagation to\\nshow that weights input to saturated neurons learned slowly .\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='95c99668-4fa5-4d1f-a010-01461ba49c41', embedding=None, metadata={'page_label': '96', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='96\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nthat earlier problem with a clever choice of cost function. Unfortunately , while that helped\\nwith saturated output neurons, it does nothing at all for the problem with saturated hidden\\nneurons.\\nI’ve been talking about the weights input to the ﬁrst hidden layer. Of course, similar\\narguments apply also to later hidden layers: if the weights in later hidden layers are initialized\\nusing normalized Gaussians, then activations will often be very close to 0 or 1, and learning\\nwill proceed very slowly .\\nIs there some way we can choose better initializations for the weights and biases, so\\nthat we don’t get this kind of saturation, and so avoid a learning slowdown? Suppose we\\nhave a neuron with nin input weights. Then we shall initialize those weights as Gaussian\\nrandom variables with mean 0 and standard deviation 1/pnin. That is, we’ll squash the\\nGaussians down, making it less likely that our neuron will saturate. We’ll continue to choose\\nthe bias as a Gaussian with mean 0 and standard deviation 1, for reasons I’ll return to in a\\nmoment. With these choices, the weighted sum z =\\n∑\\nj wj xj + b will again be a Gaussian\\nrandom variable with mean 0, but it’ll be much more sharply peaked than it was before.\\nSuppose, as we did earlier, that 500 of the inputs are zero and 500 are 1. Then it’s easy to\\nshow (see the exercise below) that z has a Gaussian distribution with mean 0 and standard\\ndeviation\\np\\n3/2 = 1.22 . . .. This is much more sharply peaked than before, so much so that\\neven the graph below understates the situation, since I’ve had to rescale the vertical axis,\\nwhen compared to the earlier graph:\\n−30 −20 −10 10 20 30\\n0.4\\nSuch a neuron is much less likely to saturate, and correspondingly much less likely to have\\nproblems with a learning slowdown.\\nExercise\\n• Verify that the standard deviation ofz =\\n∑\\nj wj xj + b in the paragraph above is\\np\\n3/2.\\nIt may help to know that: (a) the variance of a sum of independent random variables\\nis the sum of the variances of the individual random variables; and (b) the variance is\\nthe square of the standard deviation.\\nI stated above that we’ll continue to initialize the biases as before, as Gaussian random\\nvariables with a mean of 0 and a standard deviation of 1. This is okay, because it doesn’t\\nmake it too much more likely that our neurons will saturate. In fact, it doesn’t much matter\\nhow we initialize the biases, provided we avoid the problem with saturation. Some people go\\nso far as to initialize all the biases to 0, and rely on gradient descent to learn appropriate biases.\\nBut since it’s unlikely to make much difference, we’ll continue with the same initialization\\nprocedure as before.\\nLet’s compare the results for both our old and new approaches to weight initialization,\\nusing the MNIST digit classiﬁcation task. As before, we’ll use 30 hidden neurons, a mini-batch\\nsize of 10, a regularization parameter λ= 5.0, and the cross-entropy cost function. We will\\ndecrease the learning rate slightly from η= 0.5 to 0.1, since that makes the results a little\\nmore easily visible in the graphs. We can train using the old method of weight initialization:\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='54b80701-255b-4267-be59-75d40133bc21', embedding=None, metadata={'page_label': '97', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3. Weight initialization\\n\\x0c\\x0c\\x0c 97\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.large_weight_initializer()\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda = 5.0, evaluation_data=\\nvalidation_data ,\\n... monitor_evaluation_accuracy=True)\\nWe can also train using the new approach to weight initialization. This is actually even easier,\\nsince network2’s default way of initializing the weights is using this new approach. That\\nmeans we can omit the net.large_weight_initializer() call above:\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda = 5.0, evaluation_data=\\nvalidation_data ,\\n... monitor_evaluation_accuracy=True)\\nPlotting the results23, we obtain:\\nIn both cases, we end up with a classiﬁcation accuracy somewhat over 96 percent. The ﬁnal\\nclassiﬁcation accuracy is almost exactly the same in the two cases. But the new initialization\\ntechnique brings us there much, much faster. At the end of the ﬁrst epoch of training the\\nold approach to weight initialization has a classiﬁcation accuracy under 87 percent, while\\nthe new approach is already almost 93 percent. What appears to be going on is that our\\nnew approach to weight initialization starts us off in a much better regime, which lets us get\\ngood results much more quickly . The same phenomenon is also seen if we plot results with\\n100 hidden neurons:\\n23The program used to generate this and the next graph is weight_initialization.py.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5b689868-5e30-421b-9d68-9a5ac16fa9ae', embedding=None, metadata={'page_label': '98', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='98\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nIn this case, the two curves don’t quite meet. However, my experiments suggest that with just\\na few more epochs of training (not shown) the accuracies become almost exactly the same.\\nSo on the basis of these experiments it looks as though the improved weight initialization\\nonly speeds up learning, it doesn’t change the ﬁnal performance of our networks. However, in\\nChapter 4 we’ll see examples of neural networks where the long-run behaviour is signiﬁcantly\\nbetter with the 1/pnin weight initialization. Thus it’s not only the speed of learning which is\\nimproved, it’s sometimes also the ﬁnal performance.\\nThe 1/pnin approach to weight initialization helps improve the way our neural nets\\nlearn. Other techniques for weight initialization have also been proposed, many building on\\nthis basic idea. I won’t review the other approaches here, since1/pnin works well enough for\\nour purposes. If you’re interested in looking further, I recommend looking at the discussion\\non pages 14 and 15 of a 2012 paper by Yoshua Bengio24, as well as the references therein.\\nProblem\\n• Connecting regularization and the improved method of weight initialization L2\\nregularization sometimes automatically gives us something similar to the new ap-\\nproach to weight initialization. Suppose we are using the old approach to weight\\ninitialization. Sketch a heuristic argument that: (1) supposing λis not too small,\\nthe ﬁrst epochs of training will be dominated almost entirely by weight decay; (2)\\nprovided ηλ≪n the weights will decay by a factor of exp(−ηλ/m) per epoch; and\\n(3) supposing λis not too large, the weight decay will tail off when the weights are\\ndown to a size around 1/pnin, where n is the total number of weights in the network.\\nArgue that these conditions are all satisﬁed in the examples graphed in this section.\\n3.4 Handwriting recognition revisited: the code\\nLet’s implement the ideas we’ve discussed in this chapter. We’ll develop a new program,\\nnetwork2.py, which is an improved version of the program network.py we developed in\\nChapter 1. If you haven’t looked atnetwork.py in a while then you may ﬁnd it helpful to\\nspend a few minutes quickly reading over the earlier discussion. It’s only 74 lines of code,\\nand is easily understood.\\n24Practical Recommendations for Gradient-Based Training of Deep Architectures, by Yoshua Bengio\\n(2012).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b9dadabb-4959-429a-a80b-3ef567865d40', embedding=None, metadata={'page_label': '99', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Handwriting recognition revisited: the code\\n\\x0c\\x0c\\x0c 99\\nAs was the case in network.py, the star of network2.py is the Network class, which\\nwe use to represent our neural networks. We initialize an instance of Network with a list of\\nsizes for the respective layers in the network, and a choice for the cost to use, defaulting to\\nthe cross-entropy:\\nclass Network( object ):\\ndef __init__(self , sizes , cost=CrossEntropyCost):\\nself.num_layers = len (sizes)\\nself.sizes = sizes\\nself.default_weight_initializer()\\nself.cost=cost\\nThe ﬁrst couple of lines of the __init__ method are the same as in network.py, and are\\npretty self-explanatory. But the next two lines are new, and we need to understand what\\nthey’re doing in detail.\\nLet’s start by examining thedefault_weight_initializer method. This makes use\\nof our new and improved approach to weight initialization. As we’ve seen, in that approach\\nthe weights input to a neuron are initialized as Gaussian random variables with mean 0 and\\nstandard deviation 1 divided by the square root of the number of connections input to the\\nneuron. Also in this method we’ll initialize the biases, using Gaussian random variables with\\nmean 0 and standard deviation 1. Here’s the code:\\ndef default_weight_initializer(self):\\nself.biases = [np.random.randn(y, 1) for y in self.sizes [1:]]\\nself.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip (self.sizes\\n[:-1], self.sizes [1:])]\\nTo understand the code, it may help to recall that np is the Numpy library for doing linear\\nalgebra. We’ll import Numpy at the beginning of our program. Also, notice that we don’t\\ninitialize any biases for the ﬁrst layer of neurons. We avoid doing this because the ﬁrst\\nlayer is an input layer, and so any biases would not be used. We did exactly the same thing\\nin network.py. Complementing the default_weight_initializer we’ll also include a\\nlarge_weight_initializer method. This method initializes the weights and biases using\\nthe old approach from Chapter 1, with both weights and biases initialized as Gaussian\\nrandom variables with mean 0 and standard deviation 1. The code is, of course, only a tiny\\nbit different from the default_weight_initializer:\\ndef large_weight_initializer(self):\\nself.biases = [np.random.randn(y, 1) for y in self.sizes [1:]]\\nself.weights = [np.random.randn(y, x) for x, y in zip (self.sizes[:-1], self.\\nsizes [1:])]\\nI’ve included thelarge_weight_initializer method mostly as a convenience to make\\nit easier to compare the results in this chapter to those in Chapter 1. I can’t think of many\\npractical situations where I would recommend using it!\\nThe second new thing in Network’s__init__ method is that we now initialize a cost\\nattribute. To understand how that works, let’s look at the class we use to represent the\\ncross-entropy cost25:\\nclass CrossEntropyCost( object ):\\n25If you’re not familiar with Python’s static methods you can ignore the@staticmethod decorators,\\nand just treat fn and delta as ordinary methods. If you’re curious about details, all@staticmethod\\ndoes is tell the Python interpreter that the method which follows doesn’t depend on the object in any\\nway . That’s why self isn’t passed as a parameter to thefn and delta methods.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='48232721-1669-4369-adc4-cfc0fff1fef1', embedding=None, metadata={'page_label': '100', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='100\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n@staticmethod\\ndef fn(a, y):\\nreturn np. sum (np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\\n@staticmethod\\ndef delta(z, a, y):\\nreturn (a-y)\\nLet’s break this down. The ﬁrst thing to observe is that even though the cross-entropy is,\\nmathematically speaking, a function, we’ve implemented it as a Python class, not a Python\\nfunction. Why have I made that choice? The reason is that the cost plays two different roles\\nin our network. The obvious role is that it’s a measure of how well an output activation,a,\\nmatches the desired output, y. This role is captured by the CrossEntropyCost.fn method.\\n(Note, by the way , that thenp.nan_to_num call inside CrossEntropyCost.fn ensures that\\nNumpy deals correctly with the log of numbers very close to zero.) But there’s also a second\\nway the cost function enters our network. Recall from Chapter 2 that when running the\\nbackpropagation algorithm we need to compute the network’s output error,δL. The form of\\nthe output error depends on the choice of cost function: different cost function, different\\nform for the output error. For the cross-entropy the output error is, as we saw in Equation\\n(3.12),\\nδL = aL −y. (3.44)\\nFor this reason we deﬁne a second method,CrossEntropyCost.delta, whose purpose is to\\ntell our network how to compute the output error. And then we bundle these two methods up\\ninto a single class containing everything our networks need to know about the cost function.\\nIn a similar way, network2.py also contains a class to represent the quadratic cost\\nfunction. This is included for comparison with the results of Chapter 1, since going forward\\nwe’ll mostly use the cross entropy . The code is just below. TheQuadraticCost.fn method\\nis a straightforward computation of the quadratic cost associated to the actual output, a,\\nand the desired output, y. The value returned by QuadraticCost.delta is based on the\\nexpression (2.8) for the output error for the quadratic cost, which we derived back in Chapter\\n2.\\nclass QuadraticCost( object ):\\n@staticmethod\\ndef fn(a, y):\\nreturn 0.5*np.linalg.norm(a-y)**2\\n@staticmethod\\ndef delta(z, a, y):\\nreturn (a-y) * sigmoid_prime(z)\\nWe’ve now understood the main differences betweennetwork2.py and network.py. It’s\\nall pretty simple stuff. There are a number of smaller changes, which I’ll discuss below,\\nincluding the implementation of L2 regularization. Before getting to that, let’s look at the\\ncomplete code for network2.py. You don’t need to read all the code in detail, but it is worth\\nunderstanding the broad structure, and in particular reading the documentation strings, so\\nyou understand what each piece of the program is doing. Of course, you’re also welcome to\\ndelve as deeply as you wish! If you get lost, you may wish to continue reading the prose\\nbelow, and return to the code later. Anyway , here’s the code:\\n\"\"\"network2.py\\n~~~~~~~~~~~~~~\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='74e5bb9f-0662-4e75-89a5-37116fba0e7b', embedding=None, metadata={'page_label': '101', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Handwriting recognition revisited: the code\\n\\x0c\\x0c\\x0c 101\\nAn improved version of network.py, implementing the stochastic\\ngradient descent learning algorithm for a feedforward neural network.\\nImprovements include the addition of the cross -entropy cost function ,\\nregularization , and better initialization of network weights. Note\\nthat I have focused on making the code simple , easily readable , and\\neasily modifiable. It is not optimized , and omits many desirable\\nfeatures.\\n\"\"\"\\n#### Libraries\\n# Standard library\\nimport json\\nimport random\\nimport sys\\n# Third -party libraries\\nimport numpy as np\\n#### Define the quadratic and cross -entropy cost functions\\nclass QuadraticCost( object ):\\n@staticmethod\\ndef fn(a, y):\\n\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output ‘‘y‘‘.\\n\"\"\"\\nreturn 0.5*np.linalg.norm(a-y)**2\\n@staticmethod\\ndef delta(z, a, y):\\n\"\"\"Return the error delta from the output layer.\"\"\"\\nreturn (a-y) * sigmoid_prime(z)\\nclass CrossEntropyCost( object ):\\n@staticmethod\\ndef fn(a, y):\\n\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output\\n‘‘y‘‘. Note that np.nan_to_num is used to ensure numerical\\nstability. In particular , if both ‘‘a‘‘ and ‘‘y‘‘ have a 1.0\\nin the same slot , then the expression (1-y)*np.log(1-a)\\nreturns nan. The np.nan_to_num ensures that that is converted\\nto the correct value (0.0).\\n\"\"\"\\nreturn np. sum (np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\\n@staticmethod\\ndef delta(z, a, y):\\n\"\"\"Return the error delta from the output layer. Note that the\\nparameter ‘‘z‘‘ is not used by the method. It is included in\\nthe method’s parameters in order to make the interface\\nconsistent with the delta method for other cost classes.\\n\"\"\"\\nreturn (a-y)\\n#### Main Network class\\nclass Network( object ):\\ndef __init__(self , sizes , cost=CrossEntropyCost):\\n\"\"\"The list ‘‘sizes ‘‘ contains the number of neurons in the respective\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8ee5d01e-20dc-43c9-8143-c4ea8cafceab', embedding=None, metadata={'page_label': '102', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='102\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nlayers of the network. For example , if the list was [2, 3, 1]\\nthen it would be a three -layer network , with the first layer\\ncontaining 2 neurons , the second layer 3 neurons , and the\\nthird layer 1 neuron. The biases and weights for the network\\nare initialized randomly , using\\n‘‘self.default_weight_initializer ‘‘ (see docstring for that\\nmethod).\\n\"\"\"\\nself.num_layers = len (sizes)\\nself.sizes = sizes\\nself.default_weight_initializer()\\nself.cost=cost\\ndef default_weight_initializer(self):\\n\"\"\" Initialize each weight using a Gaussian distribution with mean 0\\nand standard deviation 1 over the square root of the number of\\nweights connecting to the same neuron. Initialize the biases\\nusing a Gaussian distribution with mean 0 and standard\\ndeviation 1.\\nNote that the first layer is assumed to be an input layer , and\\nby convention we won’t set any biases for those neurons , since\\nbiases are only ever used in computing the outputs from later\\nlayers.\\n\"\"\"\\nself.biases = [np.random.randn(y, 1) for y in self.sizes [1:]]\\nself.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip (self.sizes\\n[:-1], self.sizes [1:])]\\ndef large_weight_initializer(self):\\n\"\"\" Initialize the weights using a Gaussian distribution with mean 0\\nand standard deviation 1. Initialize the biases using a\\nGaussian distribution with mean 0 and standard deviation 1.\\nNote that the first layer is assumed to be an input layer , and\\nby convention we won’t set any biases for those neurons , since\\nbiases are only ever used in computing the outputs from later\\nlayers.\\nThis weight and bias initializer uses the same approach as in\\nChapter 1, and is included for purposes of comparison. It\\nwill usually be better to use the default weight initializer\\ninstead.\\n\"\"\"\\nself.biases = [np.random.randn(y, 1) for y in self.sizes [1:]]\\nself.weights = [np.random.randn(y, x)\\nfor x, y in zip (self.sizes[:-1], self.sizes [1:])]\\ndef feedforward(self , a):\\n\"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\"\\nfor b, w in zip (self.biases , self.weights):\\na = sigmoid(np.dot(w, a)+b)\\nreturn a\\ndef SGD(self , training_data , epochs , mini_batch_size , eta , lmbda = 0.0,\\nevaluation_data=None , monitor_evaluation_cost=False ,\\nmonitor_evaluation_accuracy=False , monitor_training_cost=False ,\\nmonitor_training_accuracy=False):\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8ac3e2ef-8f21-4130-aaa7-c8889ad7282c', embedding=None, metadata={'page_label': '103', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Handwriting recognition revisited: the code\\n\\x0c\\x0c\\x0c 103\\n\"\"\"Train the neural network using mini -batch stochastic gradient\\ndescent. The ‘‘training_data ‘‘ is a list of tuples ‘‘(x, y)‘‘\\nrepresenting the training inputs and the desired outputs. The\\nother non -optional parameters are self -explanatory , as is the\\nregularization parameter ‘‘lmbda ‘‘. The method also accepts\\n‘‘evaluation_data ‘‘, usually either the validation or test\\ndata. We can monitor the cost and accuracy on either the\\nevaluation data or the training data , by setting the\\nappropriate flags. The method returns a tuple containing four\\nlists: the (per -epoch) costs on the evaluation data , the\\naccuracies on the evaluation data , the costs on the training\\ndata , and the accuracies on the training data. All values are\\nevaluated at the end of each training epoch. So, for example ,\\nif we train for 30 epochs , then the first element of the tuple\\nwill be a 30-element list containing the cost on the\\nevaluation data at the end of each epoch. Note that the lists\\nare empty if the corresponding flag is not set.\\n\"\"\"\\nif evaluation_data:\\nn_data = len (evaluation_data)\\nn = len (training_data)\\nevaluation_cost , evaluation_accuracy = [], []\\ntraining_cost , training_accuracy = [], []\\nfor j in xrange (epochs):\\nrandom.shuffle(training_data)\\nmini_batches = [\\ntraining_data[k:k+mini_batch_size]\\nfor k in xrange (0, n, mini_batch_size)]\\nfor mini_batch in mini_batches:\\nself.update_mini_batch(\\nmini_batch , eta , lmbda , len (training_data))\\nprint \"Epoch %s training complete\" % j\\nif monitor_training_cost:\\ncost = self.total_cost(training_data , lmbda)\\ntraining_cost.append(cost)\\nprint \"Cost on training data: {}\". format (cost)\\nif monitor_training_accuracy:\\naccuracy = self.accuracy(training_data , convert=True)\\ntraining_accuracy.append(accuracy)\\nprint \"Accuracy on training data: {} / {}\". format (accuracy , n)\\nif monitor_evaluation_cost:\\ncost = self.total_cost(evaluation_data , lmbda , convert=True)\\nevaluation_cost.append(cost)\\nprint \"Cost on evaluation data: {}\". format (cost)\\nif monitor_evaluation_accuracy:\\naccuracy = self.accuracy(evaluation_data)\\nevaluation_accuracy.append(accuracy)\\nprint \"Accuracy on evaluation data: {} / {}\". format (self.accuracy(\\nevaluation_data), n_data)\\nprint\\nreturn evaluation_cost , evaluation_accuracy , training_cost , training_accuracy\\ndef update_mini_batch(self , mini_batch , eta , lmbda , n):\\n\"\"\"Update the network’s weights and biases by applying gradient\\ndescent using backpropagation to a single mini batch. The\\n‘‘mini_batch ‘‘ is a list of tuples ‘‘(x, y)‘‘, ‘‘eta ‘‘ is the\\nlearning rate , ‘‘lmbda ‘‘ is the regularization parameter , and\\n‘‘n‘‘ is the total size of the training data set.\\n\"\"\"\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6449df83-1c0b-4c7b-8f78-fa7301a1351b', embedding=None, metadata={'page_label': '104', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='104\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\nfor x, y in mini_batch:\\ndelta_nabla_b , delta_nabla_w = self.backprop(x, y)\\nnabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)]\\nnabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)]\\nself.weights = [(1-eta*(lmbda/n))*w-(eta/ len (mini_batch))*nw\\nfor w, nw in zip (self.weights , nabla_w)]\\nself.biases = [b-(eta/ len (mini_batch))*nb\\nfor b, nb in zip (self.biases , nabla_b)]\\ndef backprop(self , x, y):\\n\"\"\"Return a tuple ‘‘(nabla_b , nabla_w)‘‘ representing the\\ngradient for the cost function C_x. ‘‘nabla_b ‘‘ and\\n‘‘nabla_w ‘‘ are layer -by-layer lists of numpy arrays , similar\\nto ‘‘self.biases ‘‘ and ‘‘self.weights ‘‘.\"\"\"\\nnabla_b = [np.zeros(b.shape) for b in self.biases]\\nnabla_w = [np.zeros(w.shape) for w in self.weights]\\n# feedforward\\nactivation = x\\nactivations = [x] # list to store all the activations , layer by layer\\nzs = [] # list to store all the z vectors , layer by layer\\nfor b, w in zip (self.biases , self.weights):\\nz = np.dot(w, activation)+b\\nzs.append(z)\\nactivation = sigmoid(z)\\nactivations.append(activation)\\n# backward pass\\ndelta = (self.cost).delta(zs[-1], activations[-1], y)\\nnabla_b[-1] = delta\\nnabla_w[-1] = np.dot(delta , activations[-2].transpose())\\n# Note that the variable l in the loop below is used a little\\n# differently to the notation in Chapter 2 of the book. Here ,\\n# l = 1 means the last layer of neurons , l = 2 is the\\n# second -last layer , and so on. It’s a renumbering of the\\n# scheme in the book , used here to take advantage of the fact\\n# that Python can use negative indices in lists.\\nfor l in xrange (2, self.num_layers):\\nz = zs[-l]\\nsp = sigmoid_prime(z)\\ndelta = np.dot(self.weights[-l+1]. transpose(), delta) * sp\\nnabla_b[-l] = delta\\nnabla_w[-l] = np.dot(delta , activations[-l-1]. transpose())\\nreturn (nabla_b , nabla_w)\\ndef accuracy(self , data , convert=False):\\n\"\"\"Return the number of inputs in ‘‘data ‘‘ for which the neural\\nnetwork outputs the correct result. The neural network’s\\noutput is assumed to be the index of whichever neuron in the\\nfinal layer has the highest activation.\\nThe flag ‘‘convert ‘‘ should be set to False if the data set is\\nvalidation or test data (the usual case), and to True if the\\ndata set is the training data. The need for this flag arises\\ndue to differences in the way the results ‘‘y‘‘ are\\nrepresented in the different data sets. In particular , it\\nflags whether we need to convert between the different\\nrepresentations. It may seem strange to use different\\nrepresentations for the different data sets. Why not use the\\nsame representation for all three data sets? It’s done for\\nefficiency reasons -- the program usually evaluates the cost\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0660ffdb-c851-4ab2-bb7d-261fb160f110', embedding=None, metadata={'page_label': '105', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4. Handwriting recognition revisited: the code\\n\\x0c\\x0c\\x0c 105\\non the training data and the accuracy on other data sets.\\nThese are different types of computations , and using different\\nrepresentations speeds things up. More details on the\\nrepresentations can be found in\\nmnist_loader.load_data_wrapper.\\n\"\"\"\\nif convert:\\nresults = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in\\ndata]\\nelse :\\nresults = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\\nreturn sum (int (x == y) for (x, y) in results)\\ndef total_cost(self , data , lmbda , convert=False):\\n\"\"\"Return the total cost for the data set ‘‘data ‘‘. The flag\\n‘‘convert ‘‘ should be set to False if the data set is the\\ntraining data (the usual case), and to True if the data set is\\nthe validation or test data. See comments on the similar (but\\nreversed) convention for the ‘‘accuracy ‘‘ method , above.\\n\"\"\"\\ncost = 0.0\\nfor x, y in data:\\na = self.feedforward(x)\\nif convert:\\ny = vectorized_result(y)\\ncost += self.cost.fn(a, y)/ len (data)\\ncost += 0.5*( lmbda/ len (data))* sum (np.linalg.norm(w)**2 for w in self.weights)\\nreturn cost\\ndef save(self , filename):\\n\"\"\"Save the neural network to the file ‘‘filename ‘‘.\"\"\"\\ndata = {\"sizes\": self.sizes ,\\n\"weights\": [w.tolist() for w in self.weights],\\n\"biases\": [b.tolist() for b in self.biases],\\n\"cost\": str (self.cost.__name__)}\\nf = open (filename , \"w\")\\njson.dump(data , f)\\nf.close()\\n#### Loading a Network\\ndef load(filename):\\n\"\"\"Load a neural network from the file ‘‘filename ‘‘. Returns an\\ninstance of Network.\\n\"\"\"\\nf = open (filename , \"r\")\\ndata = json.load(f)\\nf.close()\\ncost = getattr (sys.modules[__name__], data[\"cost\"])\\nnet = Network(data[\"sizes\"], cost=cost)\\nnet.weights = [np.array(w) for w in data[\"weights\"]]\\nnet.biases = [np.array(b) for b in data[\"biases\"]]\\nreturn net\\n#### Miscellaneous functions\\ndef vectorized_result(j):\\n\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j’th position\\nand zeroes elsewhere. This is used to convert a digit (0...9)\\ninto a corresponding desired output from the neural network.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0a2c992b-446f-44c4-8f93-ad9b80a8d446', embedding=None, metadata={'page_label': '106', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='106\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n\"\"\"\\ne = np.zeros((10, 1))\\ne[j] = 1.0\\nreturn e\\ndef sigmoid(z):\\n\"\"\"The sigmoid function.\"\"\"\\nreturn 1.0/(1.0+np.exp(-z))\\ndef sigmoid_prime(z):\\n\"\"\" Derivative of the sigmoid function.\"\"\"\\nreturn sigmoid(z)*(1-sigmoid(z))\\nOne of the more interesting changes in the code is to include L2 regularization. Although\\nthis is a major conceptual change, it’s so trivial to implement that it’s easy to miss in the\\ncode. For the most part it just involves passing the parameter lmbda to various methods,\\nnotably the Network.SGD method. The real work is done in a single line of the program, the\\nfourth-last line of the Network.update_mini_batch method. That’s where we modify the\\ngradient descent update rule to include weight decay . But although the modiﬁcation is tiny ,\\nit has a big impact on results!\\nThis is, by the way, common when implementing new techniques in neural networks.\\nWe’ve spent thousands of words discussing regularization. It’s conceptually quite subtle and\\ndifﬁcult to understand. And yet it was trivial to add to our program! It occurs surprisingly\\noften that sophisticated techniques can be implemented with small changes to code.\\nAnother small but important change to our code is the addition of several optional ﬂags\\nto the stochastic gradient descent method, Network.SGD. These ﬂags make it possible to\\nmonitor the cost and accuracy either on thetraining_data or on a set ofevaluation_data\\nwhich can be passed to Network.SGD. We’ve used these ﬂags often earlier in the chapter,\\nbut let me give an example of how it works, just to remind you:\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\\n>>> net.SGD(training_data , 30, 10, 0.5, lmbda = 5.0, evaluation_data=\\nvalidation_data ,\\n... monitor_evaluation_accuracy=True , monitor_evaluation_cost=True ,\\nmonitor_training_accuracy=True ,\\n... monitor_training_cost=True)\\nHere, we’re setting theevaluation_data to be the validation_data. But we could also\\nhave monitored performance on the test_data or any other data set. We also have four\\nﬂags telling us to monitor the cost and accuracy on both the evaluation_data and the\\ntraining_data. Those ﬂags are False by default, but they’ve been turned on here in order\\nto monitor our Network’s performance. Furthermore,network2.py’sNetwork.SGD method\\nreturns a four-element tuple representing the results of the monitoring. We can use this as\\nfollows:\\n>>> evaluation_cost , evaluation_accuracy , training_cost , training_accuracy = net\\n.SGD(training_data , 30, 10, 0.5, lmbda = 5.0, evaluation_data=\\nvalidation_data , monitor_evaluation_accuracy=True , monitor_evaluation_cost=\\nTrue , monitor_training_accuracy=True , monitor_training_cost=True)\\nSo, for example, evaluation_cost will be a 30-element list containing the cost on the\\nevaluation data at the end of each epoch. This sort of information is extremely useful in\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f542fdfa-c5eb-4982-9e86-57efb2666be3', embedding=None, metadata={'page_label': '107', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 107\\nunderstanding a network’s behaviour. It can, for example, be used to draw graphs showing\\nhow the network learns over time. Indeed, that’s exactly how I constructed all the graphs\\nearlier in the chapter. Note, however, that if any of the monitoring ﬂags are not set, then the\\ncorresponding element in the tuple will be the empty list.\\nOther additions to the code include a Network.save method, to save Network objects\\nto disk, and a function to load them back in again later. Note that the saving and loading\\nis done using JSON, not Python’s pickle or cPickle modules, which are the usual way we\\nsave and load objects to and from disk in Python. Using JSON requires more code than\\npickle or cPickle would. To understand why I’ve used JSON, imagine that at some time in\\nthe future we decided to change our Network class to allow neurons other than sigmoid\\nneurons. To implement that change we’d most likely change the attributes deﬁned in the\\nNetwork.__init__ method. If we’ve simply pickled the objects that would cause our load\\nfunction to fail. Using JSON to do the serialization explicitly makes it easy to ensure that old\\nNetworks will still load.\\nThere are many other minor changes in the code fornetwork2.py, but they’re all simple\\nvariations on network.py. The net result is to expand our 74-line program to a far more\\ncapable 152 lines.\\nProblems\\n• Modify the code above to implement L1 regularization, and use L1 regularization to\\nclassify MNIST digits using a 30 hidden neuron network. Can you ﬁnd a regularization\\nparameter that enables you to do better than running unregularized?\\n• Take a look at theNetwork.cost_derivative method in network.py. That method\\nwas written for the quadratic cost. How would you rewrite the method for the cross-\\nentropy cost? Can you think of a problem that might arise in the cross-entropy version?\\nIn network2.py we’ve eliminated theNetwork.cost_derivative method entirely ,\\ninstead incorporating its functionality into the CrossEntropyCost.delta method.\\nHow does this solve the problem you’ve just identiﬁed?\\n3.5 How to choose a neural network’s hyper-parameters?\\nUp until now I haven’t explained how I’ve been choosing values for hyper-parameters such\\nas the learning rate, η, the regularization parameter, λ, and so on. I’ve just been supplying\\nvalues which work pretty well. In practice, when you’re using neural nets to attack a problem,\\nit can be difﬁcult to ﬁnd good hyper-parameters. Imagine, for example, that we’ve just\\nbeen introduced to the MNIST problem, and have begun working on it, knowing nothing\\nat all about what hyper-parameters to use. Let’s suppose that by good fortune in our ﬁrst\\nexperiments we choose many of the hyper-parameters in the same way as was done earlier\\nthis chapter: 30 hidden neurons, a mini-batch size of 10, training for 30 epochs using\\nthe cross-entropy. But we choose a learning rate η= 10.0 and regularization parameter\\nλ= 1000.0. Here’s what I saw on one such run:\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = \\\\\\n... mnist_loader.load_data_wrapper()\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10])\\n>>> net.SGD(training_data , 30, 10, 10.0, lmbda = 1000.0,\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='39e2743a-fd1b-4db5-84e5-1b7ada7ac1d2', embedding=None, metadata={'page_label': '108', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='108\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n... evaluation_data=validation_data , monitor_evaluation_accuracy=True)\\nEpoch 0 training complete\\nAccuracy on evaluation data: 1030 / 10000\\nEpoch 1 training complete\\nAccuracy on evaluation data: 990 / 10000\\nEpoch 2 training complete\\nAccuracy on evaluation data: 1009 / 10000\\n...\\nEpoch 27 training complete\\nAccuracy on evaluation data: 1009 / 10000\\nEpoch 28 training complete\\nAccuracy on evaluation data: 983 / 10000\\nEpoch 29 training complete\\nAccuracy on evaluation data: 967 / 10000\\nOur classiﬁcation accuracies are no better than chance! Our network is acting as a random\\nnoise generator!\\n“Well, that’s easy to ﬁx,” you might say , “just decrease the learning rate and regularization\\nhyper-parameters”. Unfortunately , you don’t a priori know those are the hyper-parameters\\nyou need to adjust. Maybe the real problem is that our 30 hidden neuron network will never\\nwork well, no matter how the other hyper-parameters are chosen? Maybe we really need at\\nleast 100 hidden neurons? Or 300 hidden neurons? Or multiple hidden layers? Or a different\\napproach to encoding the output? Maybe our network is learning, but we need to train\\nfor more epochs? Maybe the mini-batches are too small? Maybe we’d do better switching\\nback to the quadratic cost function? Maybe we need to try a different approach to weight\\ninitialization? And so on, on and on and on. It’s easy to feel lost in hyper-parameter space.\\nThis can be particularly frustrating if your network is very large, or uses a lot of training\\ndata, since you may train for hours or days or weeks, only to get no result. If the situation\\npersists, it damages your conﬁdence. Maybe neural networks are the wrong approach to\\nyour problem? Maybe you should quit your job and take up beekeeping?\\nIn this section I explain some heuristics which can be used to set the hyper-parameters\\nin a neural network. The goal is to help you develop a workﬂow that enables you to do a\\npretty good job setting hyper-parameters. Of course, I won’t cover everything about hyper-\\nparameter optimization. That’s a huge subject, and it’s not, in any case, a problem that\\nis ever completely solved, nor is there universal agreement amongst practitioners on the\\nright strategies to use. There’s always one more trick you can try to eke out a bit more\\nperformance from your network. But the heuristics in this section should get you started.\\nBroad strategy: When using neural networks to attack a new problem the ﬁrst challenge\\nis to get any non-trivial learning, i.e., for the network to achieve results better than chance.\\nThis can be surprisingly difﬁcult, especially when confronting a new class of problem. Let’s\\nlook at some strategies you can use if you’re having this kind of trouble.\\nSuppose, for example, that you’re attacking MNIST for the ﬁrst time. You start out\\nenthusiastic, but are a little discouraged when your ﬁrst network fails completely , as in the\\nexample above. The way to go is to strip the problem down. Get rid of all the training\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='255abc1c-ebd4-4811-834c-e64edc527dbb', embedding=None, metadata={'page_label': '109', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 109\\nand validation images except images which are 0s or 1s. Then try to train a network to\\ndistinguish 0s from 1s. Not only is that an inherently easier problem than distinguishing all\\nten digits, it also reduces the amount of training data by 80 percent, speeding up training by\\na factor of 5. That enables much more rapid experimentation, and so gives you more rapid\\ninsight into how to build a good network.\\nYou can further speed up experimentation by stripping your network down to the simplest\\nnetwork likely to do meaningful learning. If you believe a [784, 10] network can likely\\ndo better-than-chance classiﬁcation of MNIST digits, then begin your experimentation with\\nsuch a network. It’ll be much faster than training a[784, 30, 10] network, and you can\\nbuild back up to the latter.\\nYou can get another speed up in experimentation by increasing the frequency of moni-\\ntoring. In network2.py we monitor performance at the end of each training epoch. With\\n50,000 images per epoch, that means waiting a little while – about ten seconds per epoch,\\non my laptop, when training a [784, 30, 10] network – before getting feedback on how well\\nthe network is learning. Of course, ten seconds isn’t very long, but if you want to trial dozens\\nof hyper-parameter choices it’s annoying, and if you want to trial hundreds or thousands\\nof choices it starts to get debilitating. We can get feedback more quickly by monitoring the\\nvalidation accuracy more often, say , after every 1,000 training images. Furthermore, instead\\nof using the full 10,000 image validation set to monitor performance, we can get a much\\nfaster estimate using just 100 validation images. All that matters is that the network sees\\nenough images to do real learning, and to get a pretty good rough estimate of performance.\\nOf course, our program network2.py doesn’t currently do this kind of monitoring. But as\\na kludge to achieve a similar effect for the purposes of illustration, we’ll strip down our\\ntraining data to just the ﬁrst 1,000 MNIST training images. Let’s try it and see what happens.\\n(To keep the code below simple I haven’t implemented the idea of using only 0 and 1 images.\\nOf course, that can be done with just a little more work.)\\n>>> net = network2.Network([784, 10])\\n>>> net.SGD(training_data [:1000], 30, 10, 10.0, lmbda = 1000.0, \\\\\\n... evaluation_data=validation_data [:100], \\\\\\n... monitor_evaluation_accuracy=True)\\nEpoch 0 training complete\\nAccuracy on evaluation data: 10 / 100\\nEpoch 1 training complete\\nAccuracy on evaluation data: 10 / 100\\nEpoch 2 training complete\\nAccuracy on evaluation data: 10 / 100\\n...\\nWe’re still getting pure noise! But there’s a big win: we’re now getting feedback in a fraction\\nof a second, rather than once every ten seconds or so. That means you can more quickly\\nexperiment with other choices of hyper-parameter, or even conduct experiments trialling\\nmany different choices of hyper-parameter nearly simultaneously .\\nIn the above example I left λas λ= 1000.0, as we used earlier. But since we changed\\nthe number of training examples we should really change λto keep the weight decay the\\nsame. That means changing λto 20.0. If we do that then this is what happens:\\n>>> net = network2.Network([784, 10])\\n>>> net.SGD(training_data [:1000], 30, 10, 10.0, lmbda = 20.0, \\\\\\n... evaluation_data=validation_data [:100], \\\\\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a6396902-2b4d-4742-a709-98a0932f5a03', embedding=None, metadata={'page_label': '110', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='110\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\n... monitor_evaluation_accuracy=True)\\nEpoch 0 training complete\\nAccuracy on evaluation data: 12 / 100\\nEpoch 1 training complete\\nAccuracy on evaluation data: 14 / 100\\nEpoch 2 training complete\\nAccuracy on evaluation data: 25 / 100\\nEpoch 3 training complete\\nAccuracy on evaluation data: 18 / 100\\n...\\nAhah! We have a signal. Not a terribly good signal, but a signal nonetheless. That’s something\\nwe can build on, modifying the hyper-parameters to try to get further improvement. Maybe\\nwe guess that our learning rate needs to be higher. (As you perhaps realize, that’s a silly\\nguess, for reasons we’ll discuss shortly , but please bear with me.) So to test our guess we try\\ndialing ηup to 100.0:\\n>>> net = network2.Network([784, 10])\\n>>> net.SGD(training_data [:1000], 30, 10, 100.0, lmbda = 20.0, \\\\\\n... evaluation_data=validation_data [:100], \\\\\\n... monitor_evaluation_accuracy=True)\\nEpoch 0 training complete\\nAccuracy on evaluation data: 10 / 100\\nEpoch 1 training complete\\nAccuracy on evaluation data: 10 / 100\\nEpoch 2 training complete\\nAccuracy on evaluation data: 10 / 100\\nEpoch 3 training complete\\nAccuracy on evaluation data: 10 / 100\\n...\\nThat’s no good! It suggests that our guess was wrong, and the problem wasn’t that the\\nlearning rate was too low. So instead we try dialing ηdown to η= 1.0:\\n>>> net = network2.Network([784, 10])\\n>>> net.SGD(training_data [:1000], 30, 10, 1.0, lmbda = 20.0, \\\\\\n... evaluation_data=validation_data [:100], \\\\\\n... monitor_evaluation_accuracy=True)\\nEpoch 0 training complete\\nAccuracy on evaluation data: 62 / 100\\nEpoch 1 training complete\\nAccuracy on evaluation data: 42 / 100\\nEpoch 2 training complete\\nAccuracy on evaluation data: 43 / 100\\nEpoch 3 training complete\\nAccuracy on evaluation data: 61 / 100\\n...\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d5d1483e-b691-44d8-9a54-e53ee7036861', embedding=None, metadata={'page_label': '111', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 111\\nThat’s better! And so we can continue, individually adjusting each hyper-parameter, gradually\\nimproving performance. Once we’ve explored to ﬁnd an improved value for η, then we\\nmove on to ﬁnd a good value for λ. Then experiment with a more complex architecture,\\nsay a network with 10 hidden neurons. Then adjust the values for ηand λagain. Then\\nincrease to 20 hidden neurons. And then adjust other hyper-parameters some more. And\\nso on, at each stage evaluating performance using our held-out validation data, and using\\nthose evaluations to ﬁnd better and better hyper-parameters. As we do so, it typically takes\\nlonger to witness the impact due to modiﬁcations of the hyper-parameters, and so we can\\ngradually decrease the frequency of monitoring.\\nThis all looks very promising as a broad strategy . However, I want to return to that initial\\nstage of ﬁnding hyper-parameters that enable a network to learn anything at all. In fact,\\neven the above discussion conveys too positive an outlook. It can be immensely frustrating\\nto work with a network that’s learning nothing. You can tweak hyper-parameters for days,\\nand still get no meaningful response. And so I’d like to re-emphasize that during the early\\nstages you should make sure you can get quick feedback from experiments. Intuitively, it\\nmay seem as though simplifying the problem and the architecture will merely slow you down.\\nIn fact, it speeds things up, since you much more quickly ﬁnd a network with a meaningful\\nsignal. Once you’ve got such a signal, you can often get rapid improvements by tweaking the\\nhyper-parameters. As with many things in life, getting started can be the hardest thing to do.\\nOkay, that’s the broad strategy. Let’s now look at some speciﬁc recommendations for\\nsetting hyper-parameters. I will focus on the learning rate,η, the L2 regularization parameter,\\nλ, and the mini-batch size. However, many of the remarks apply also to other hyper-\\nparameters, including those associated to network architecture, other forms of regularization,\\nand some hyper-parameters we’ll meet later in the book, such as the momentum co-efﬁcient.\\nLearning rate: Suppose we run three MNIST networks with three different learning rates,\\nη= 0.025, η= 0.25 and η= 2.5, respectively. We’ll set the other hyper-parameters as for\\nthe experiments in earlier sections, running over 30 epochs, with a mini-batch size of 10,\\nand with λ= 5.0. We’ll also return to using the full 50,000 training images. Here’s a graph\\nshowing the behaviour of the training cost as we train26:\\n26The graph was generated by multiple_eta.py.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4022e0ad-2ca4-4648-a168-0e62486ac20e', embedding=None, metadata={'page_label': '112', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='112\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nWith η= 0.025 the cost decreases smoothly until the ﬁnal epoch. With η= 0.25 the cost\\ninitially decreases, but after about 20 epochs it is near saturation, and thereafter most of the\\nchanges are merely small and apparently random oscillations. Finally , withη= 2.5 the cost\\nmakes large oscillations right from the start. To understand the reason for the oscillations,\\nrecall that stochastic gradient descent is supposed to step us gradually down into a valley of\\nthe cost function,\\nHowever, if ηis too large then the steps will be so large that they may actually overshoot the\\nminimum, causing the algorithm to climb up out of the valley instead. That’s likely27 what’s\\ncausing the cost to oscillate when η= 2.5. When we choose η=0.25 the initial steps do take\\nus toward a minimum of the cost function, and it’s only once we get near that minimum that\\nwe start to suffer from the overshooting problem. And when we choose η= 0.025 we don’t\\nsuffer from this problem at all during the ﬁrst 30 epochs. Of course, choosing ηso small\\ncreates another problem, namely, that it slows down stochastic gradient descent. An even\\nbetter approach would be to start with η= 0.25, train for 20 epochs, and then switch to\\nη= 0.025. We’ll discuss such variable learning rate schedules later. For now, though, let’s\\nstick to ﬁguring out how to ﬁnd a single good value for the learning rate, η.\\nWith this picture in mind, we can set ηas follows. First, we estimate the threshold\\nvalue for ηat which the cost on the training data immediately begins decreasing, instead of\\noscillating or increasing. This estimate doesn’t need to be too accurate. You can estimate\\nthe order of magnitude by starting with η=0.01. If the cost decreases during the ﬁrst few\\nepochs, then you should successively try η= 0.1, 1.0, . . .until you ﬁnd a value for ηwhere\\nthe cost oscillates or increases during the ﬁrst few epochs. Alternately , if the cost oscillates\\nor increases during the ﬁrst few epochs when η=0.01, then try η=0.001,0.0001,. . .until\\nyou ﬁnd a value for ηwhere the cost decreases during the ﬁrst few epochs. Following this\\nprocedure will give us an order of magnitude estimate for the threshold value of η. You\\nmay optionally reﬁne your estimate, to pick out the largest value of ηat which the cost\\ndecreases during the ﬁrst few epochs, say η=0.5 or η=0.2 (there’s no need for this to be\\nsuper-accurate). This gives us an estimate for the threshold value of η.\\nObviously, the actual value of ηthat you use should be no larger than the threshold\\nvalue. In fact, if the value of ηis to remain usable over many epochs then you likely want to\\n27This picture is helpful, but it’s intended as an intuition-building illustration of what may go on, not\\nas a complete, exhaustive explanation. Brieﬂy, a more complete explanation is as follows: gradient\\ndescent uses a ﬁrst-order approximation to the cost function as a guide to how to decrease the cost.\\nFor large η, higher-order terms in the cost function become more important, and may dominate the\\nbehaviour, causing gradient descent to break down. This is especially likely as we approach minima and\\nquasi-minima of the cost function, since near such points the gradient becomes small, making it easier\\nfor higher-order terms to dominate behaviour.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='719927e4-c5ff-4fff-8fcd-f79861f9238c', embedding=None, metadata={'page_label': '113', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 113\\nuse a value for ηthat is smaller, say , a factor of two below the threshold. Such a choice will\\ntypically allow you to train for many epochs, without causing too much of a slowdown in\\nlearning.\\nIn the case of the MNIST data, following this strategy leads to an estimate of 0.1 for the\\norder of magnitude of the threshold value of η. After some more reﬁnement, we obtain a\\nthreshold value η=0.5. Following the prescription above, this suggests using η=0.25 as our\\nvalue for the learning rate. In fact, I found that using η=0.5 worked well enough over 30\\nepochs that for the most part I didn’t worry about using a lower value ofη.\\nThis all seems quite straightforward. However, using the training cost to pick ηappears\\nto contradict what I said earlier in this section, namely, that we’d pick hyper-parameters\\nby evaluating performance using our held-out validation data. In fact, we’ll use validation\\naccuracy to pick the regularization hyper-parameter, the mini-batch size, and network\\nparameters such as the number of layers and hidden neurons, and so on. Why do things\\ndifferently for the learning rate? Frankly, this choice is my personal aesthetic preference,\\nand is perhaps somewhat idiosyncratic. The reasoning is that the other hyper-parameters\\nare intended to improve the ﬁnal classiﬁcation accuracy on the test set, and so it makes\\nsense to select them on the basis of validation accuracy . However, the learning rate is only\\nincidentally meant to impact the ﬁnal classiﬁcation accuracy . Its primary purpose is really to\\ncontrol the step size in gradient descent, and monitoring the training cost is the best way\\nto detect if the step size is too big. With that said, this is a personal aesthetic preference.\\nEarly on during learning the training cost usually only decreases if the validation accuracy\\nimproves, and so in practice it’s unlikely to make much difference which criterion you use.\\nUse early stopping to determine the number of training epochs: As we discussed\\nearlier in the chapter, early stopping means that at the end of each epoch we should compute\\nthe classiﬁcation accuracy on the validation data. When that stops improving, terminate.\\nThis makes setting the number of epochs very simple. In particular, it means that we don’t\\nneed to worry about explicitly ﬁguring out how the number of epochs depends on the other\\nhyper-parameters. Instead, that’s taken care of automatically . Furthermore, early stopping\\nalso automatically prevents us from overﬁtting. This is, of course, a good thing, although in\\nthe early stages of experimentation it can be helpful to turn off early stopping, so you can\\nsee any signs of overﬁtting, and use it to inform your approach to regularization.\\nTo implement early stopping we need to say more precisely what it means that the\\nclassiﬁcation accuracy has stopped improving. As we’ve seen, the accuracy can jump around\\nquite a bit, even when the overall trend is to improve. If we stop the ﬁrst time the accuracy\\ndecreases then we’ll almost certainly stop when there are more improvements to be had. A\\nbetter rule is to terminate if the best classiﬁcation accuracy doesn’t improve for quite some\\ntime. Suppose, for example, that we’re doing MNIST . Then we might elect to terminate if the\\nclassiﬁcation accuracy hasn’t improved during the last ten epochs. This ensures that we don’t\\nstop too soon, in response to bad luck in training, but also that we’re not waiting around\\nforever for an improvement that never comes.\\nThis no-improvement-in-ten rule is good for initial exploration of MNIST . However,\\nnetworks can sometimes plateau near a particular classiﬁcation accuracy for quite some time,\\nonly to then begin improving again. If you’re trying to get really good performance, the no-\\nimprovement-in-ten rule may be too aggressive about stopping. In that case, I suggest using\\nthe no-improvement-in-ten rule for initial experimentation, and gradually adopting more\\nlenient rules, as you better understand the way your network trains: no-improvement-in-\\ntwenty , no-improvement-in-ﬁfty , and so on. Of course, this introduces a new hyper-parameter\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='54b80285-87cc-4a97-8136-d05f00bdc4e1', embedding=None, metadata={'page_label': '114', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='114\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nto optimize! In practice, however, it’s usually easy to set this hyper-parameter to get pretty\\ngood results. Similarly, for problems other than MNIST , the no-improvement-in-ten rule\\nmay be much too aggressive or not nearly aggressive enough, depending on the details of\\nthe problem. However, with a little experimentation it’s usually easy to ﬁnd a pretty good\\nstrategy for early stopping.\\nWe haven’t used early stopping in our MNIST experiments to date. The reason is that\\nwe’ve been doing a lot of comparisons between different approaches to learning. For such\\ncomparisons it’s helpful to use the same number of epochs in each case. However, it’s well\\nworth modifying network2.py to implement early stopping:\\nProblem\\n• Modify network2.py so that it implements early stopping using a no-improvement-\\nin-n epochs strategy , wheren is a parameter that can be set.\\n• Can you think of a rule for early stopping other than no-improvement-in-n? Ideally , the\\nrule should compromise between getting high validation accuracies and not training\\ntoo long. Add your rule to network2.py, and run three experiments comparing the\\nvalidation accuracies and number of epochs of training to no-improvement-in-10.\\nLearning rate schedule: We’ve been holding the learning rateηconstant. However, it’s\\noften advantageous to vary the learning rate. Early on during the learning process it’s likely\\nthat the weights are badly wrong. And so it’s best to use a large learning rate that causes\\nthe weights to change quickly. Later, we can reduce the learning rate as we make more\\nﬁne-tuned adjustments to our weights.\\nHow should we set our learning rate schedule? Many approaches are possible. One\\nnatural approach is to use the same basic idea as early stopping. The idea is to hold the\\nlearning rate constant until the validation accuracy starts to get worse. Then decrease the\\nlearning rate by some amount, say a factor of two or ten. We repeat this many times, until,\\nsay , the learning rate is a factor of 1,024 (or 1,000) times lower than the initial value. Then\\nwe terminate.\\nA variable learning schedule can improve performance, but it also opens up a world of\\npossible choices for the learning schedule. Those choices can be a headache – you can spend\\nforever trying to optimize your learning schedule. For ﬁrst experiments my suggestion is to\\nuse a single, constant value for the learning rate. That’ll get you a good ﬁrst approximation.\\nLater, if you want to obtain the best performance from your network, it’s worth experimenting\\nwith a learning schedule, along the lines I’ve described28.\\nExercise\\n• Modify network2.py so that it implements a learning schedule that: halves the\\nlearning rate each time the validation accuracy satisﬁes the no-improvement-in-10\\nrule; and terminates when the learning rate has dropped to 1/128 of its original value.\\nThe regularization parameter, λ: I suggest starting initially with no regularization ( λ=\\n0.0), and determining a value for η, as above. Using that choice of η, we can then use the\\nvalidation data to select a good value for λ. Start by trialling λ= 1.029, and then increase or\\ndecrease by factors of 10, as needed to improve performance on the validation data. Once\\nyou’ve found a good order of magnitude, you can ﬁne tune your value ofλ. That done, you\\nshould return and re-optimize ηagain.\\n28A readable recent paper which demonstrates the beneﬁts of variable learning rates in attacking\\nMNIST is Deep, Big, Simple Neural Nets Excel on Handwritten Digit Recognition, by Dan Claudiu Cire¸ san,\\nUeli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber (2010).\\n29I don’t have a good principled justiﬁcation for using this as a starting value. If anyone knows of a\\ngood principled discussion of where to start with λ, I’d appreciate hearing it (mn@michaelnielsen.org).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a2189280-3c65-4c9c-b625-d80df2ba3529', embedding=None, metadata={'page_label': '115', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 115\\nExercise\\n• It’s tempting to use gradient descent to try to learn good values for hyper-parameters\\nsuch as λand η. Can you think of an obstacle to using gradient descent to determine\\nλ? Can you think of an obstacle to using gradient descent to determine η?\\nHow I selected hyper-parameters earlier in this book: If you use the recommendations\\nin this section you’ll ﬁnd that you get values forηand λwhich don’t always exactly match\\nthe values I’ve used earlier in the book. The reason is that the book has narrative constraints\\nthat have sometimes made it impractical to optimize the hyper-parameters. Think of all the\\ncomparisons we’ve made of different approaches to learning, e.g., comparing the quadratic\\nand cross-entropy cost functions, comparing the old and new methods of weight initialization,\\nrunning with and without regularization, and so on. To make such comparisons meaningful,\\nI’ve usually tried to keep hyper-parameters constant across the approaches being compared\\n(or to scale them in an appropriate way). Of course, there’s no reason for the same hyper-\\nparameters to be optimal for all the different approaches to learning, so the hyper-parameters\\nI’ve used are something of a compromise.\\nAs an alternative to this compromise, I could have tried to optimize the heck out of the\\nhyper-parameters for every single approach to learning. In principle that’d be a better, fairer\\napproach, since then we’d see the best from every approach to learning. However, we’ve\\nmade dozens of comparisons along these lines, and in practice I found it too computationally\\nexpensive. That’s why I’ve adopted the compromise of using pretty good (but not necessarily\\noptimal) choices for the hyper-parameters.\\nMini-batch size: How should we set the mini-batch size? To answer this question, let’s\\nﬁrst suppose that we’re doing online learning, i.e., that we’re using a mini-batch size of 1.\\nThe obvious worry about online learning is that using mini-batches which contain just\\na single training example will cause signiﬁcant errors in our estimate of the gradient. In\\nfact, though, the errors turn out to not be such a problem. The reason is that the individual\\ngradient estimates don’t need to be super-accurate. All we need is an estimate accurate\\nenough that our cost function tends to keep decreasing. It’s as though you are trying to get\\nto the North Magnetic Pole, but have a wonky compass that’s 10–20 degrees off each time\\nyou look at it. Provided you stop to check the compass frequently , and the compass gets the\\ndirection right on average, you’ll end up at the North Magnetic Pole just ﬁne.\\nBased on this argument, it sounds as though we should use online learning. In fact, the\\nsituation turns out to be more complicated than that. In a problem in the last chapter I\\npointed out that it’s possible to use matrix techniques to compute the gradient update for\\nall examples in a mini-batch simultaneously , rather than looping over them. Depending on\\nthe details of your hardware and linear algebra library this can make it quite a bit faster\\nto compute the gradient estimate for a mini-batch of (for example) size 100, rather than\\ncomputing the mini-batch gradient estimate by looping over the 100 training examples\\nseparately . It might take (say) only 50 times as long, rather than 100 times as long.\\nNow, at ﬁrst it seems as though this doesn’t help us that much. With our mini-batch of\\nsize 100 the learning rule for the weights looks like:\\nw →w′= w −η 1\\n100\\n∑\\nx\\n∇Cx , (3.45)\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0292f255-acad-434e-95cf-f157863e0991', embedding=None, metadata={'page_label': '116', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='116\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nwhere the sum is over training examples in the mini-batch. This is versus\\nw →w′= w −η∇Cx (3.46)\\nfor online learning. Even if it only takes 50 times as long to do the mini-batch update, it\\nstill seems likely to be better to do online learning, because we’d be updating so much more\\nfrequently . Suppose, however, that in the mini-batch case we increase the learning rate by a\\nfactor 100, so the update rule becomes\\nw →w′= w −η\\n∑\\nx\\n∇Cx . (3.47)\\nThat’s a lot like doing 100 separate instances of online learning with a learning rate ofη.\\nBut it only takes 50 times as long as doing a single instance of online learning. Of course, it’s\\nnot truly the same as 100 instances of online learning, since in the mini-batch the ∇Cx ’s are\\nall evaluated for the same set of weights, as opposed to the cumulative learning that occurs\\nin the online case. Still, it seems distinctly possible that using the larger mini-batch would\\nspeed things up.\\nWith these factors in mind, choosing the best mini-batch size is a compromise. Too small,\\nand you don’t get to take full advantage of the beneﬁts of good matrix libraries optimized for\\nfast hardware. Too large and you’re simply not updating your weights often enough. What\\nyou need is to choose a compromise value which maximizes the speed of learning. Fortunately ,\\nthe choice of mini-batch size at which the speed is maximized is relatively independent\\nof the other hyper-parameters (apart from the overall architecture), so you don’t need to\\nhave optimized those hyper-parameters in order to ﬁnd a good mini-batch size. The way\\nto go is therefore to use some acceptable (but not necessarily optimal) values for the other\\nhyper-parameters, and then trial a number of different mini-batch sizes, scaling ηas above.\\nPlot the validation accuracy versus time (as in, real elapsed time, not epoch!), and choose\\nwhichever mini-batch size gives you the most rapid improvement in performance. With the\\nmini-batch size chosen you can then proceed to optimize the other hyper-parameters.\\nOf course, as you’ve no doubt realized, I haven’t done this optimization in our work.\\nIndeed, our implementation doesn’t use the faster approach to mini-batch updates at all. I’ve\\nsimply used a mini-batch size of 10 without comment or explanation in nearly all examples.\\nBecause of this, we could have sped up learning by reducing the mini-batch size. I haven’t\\ndone this, in part because I wanted to illustrate the use of mini-batches beyond size 1, and in\\npart because my preliminary experiments suggested the speedup would be rather modest. In\\npractical implementations, however, we would most certainly implement the faster approach\\nto mini-batch updates, and then make an effort to optimize the mini-batch size, in order to\\nmaximize our overall speed.\\nAutomated techniques: I’ve been describing these heuristics as though you’re optimiz-\\ning your hyper-parameters by hand. Hand-optimization is a good way to build up a feel for\\nhow neural networks behave. However, and unsurprisingly , a great deal of work has been\\ndone on automating the process. A common technique is grid search, which systematically\\nsearches through a grid in hyper-parameter space. A review of both the achievements and\\nthe limitations of grid search (with suggestions for easily-implemented alternatives) may be\\nfound in a 2012 paper30 by James Bergstra and Yoshua Bengio. Many more sophisticated\\n30Random search for hyper-parameter optimization, by James Bergstra and Yoshua Bengio (2012).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='86a61423-ac14-4973-8f1d-7d94e1a9243f', embedding=None, metadata={'page_label': '117', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5. How to choose a neural network’s hyper-parameters?\\n\\x0c\\x0c\\x0c 117\\napproaches have also been proposed. I won’t review all that work here, but do want to men-\\ntion a particularly promising 2012 paper which used a Bayesian approach to automatically\\noptimize hyper-parameters31. The code from the paper is publicly available, and has been\\nused with some success by other researchers.\\nSumming up: Following the rules-of-thumb I’ve described won’t give you the absolute\\nbest possible results from your neural network. But it will likely give you a good start and a\\nbasis for further improvements. In particular, I’ve discussed the hyper-parameters largely\\nindependently . In practice, there are relationships between the hyper-parameters. You may\\nexperiment with η, feel that you’ve got it just right, then start to optimize forλ, only to ﬁnd\\nthat it’s messing up your optimization forη. In practice, it helps to bounce backward and\\nforward, gradually closing in good values. Above all, keep in mind that the heuristics I’ve\\ndescribed are rules of thumb, not rules cast in stone. You should be on the lookout for signs\\nthat things aren’t working, and be willing to experiment. In particular, this means carefully\\nmonitoring your network’s behaviour, especially the validation accuracy .\\nThe difﬁculty of choosing hyper-parameters is exacerbated by the fact that the lore\\nabout how to choose hyper-parameters is widely spread, across many research papers and\\nsoftware programs, and often is only available inside the heads of individual practitioners.\\nThere are many , many papers setting out (sometimes contradictory) recommendations for\\nhow to proceed. However, there are a few particularly useful papers that synthesize and\\ndistill out much of this lore. Yoshua Bengio has a 2012 paper 32 that gives some practical\\nrecommendations for using backpropagation and gradient descent to train neural networks,\\nincluding deep neural nets. Bengio discusses many issues in much more detail than I have,\\nincluding how to do more systematic hyper-parameter searches. Another good paper is a\\n1998 paper33 by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller. Both\\nthese papers appear in an extremely useful 2012 book that collects many tricks commonly\\nused in neural nets34. The book is expensive, but many of the articles have been placed\\nonline by their respective authors with, one presumes, the blessing of the publisher, and may\\nbe located using a search engine.\\nOne thing that becomes clear as you read these articles and, especially , as you engage in\\nyour own experiments, is that hyper-parameter optimization is not a problem that is ever\\ncompletely solved. There’s always another trick you can try to improve performance. There\\nis a saying common among writers that books are never ﬁnished, only abandoned. The same\\nis also true of neural network optimization: the space of hyper-parameters is so large that\\none never really ﬁnishes optimizing, one only abandons the network to posterity. So your\\ngoal should be to develop a workﬂow that enables you to quickly do a pretty good job on\\nthe optimization, while leaving you the ﬂexibility to try more detailed optimizations, if that’s\\nimportant.\\nThe challenge of setting hyper-parameters has led some people to complain that neural\\nnetworks require a lot of work when compared with other machine learning techniques. I’ve\\nheard many variations on the following complaint: “Yes, a well-tuned neural network may\\nget the best performance on the problem. On the other hand, I can try a random forest [or\\n31Practical Bayesian optimization of machine learning algorithms, by Jasper Snoek, Hugo Larochelle,\\nand Ryan Adams.\\n32Practical recommendations for gradient-based training of deep architectures, by Yoshua Bengio\\n(2012).\\n33Efﬁcient BackProp, by Yann LeCun, LÃl’on Bottou, Genevieve Orr and Klaus-Robert Müller (1998)\\n34Neural Networks: Tricks of the Trade, edited by Grégoire Montavon, Geneviève Orr, and Klaus-Robert\\nMüller.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='15d2472e-39c1-401d-a2c1-4b0058ff7eb7', embedding=None, metadata={'page_label': '118', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='118\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nSVM or ... insert your own favorite technique] and it just works. I don’t have time to ﬁgure\\nout just the right neural network.” Of course, from a practical point of view it’s good to\\nhave easy-to-apply techniques. This is particularly true when you’re just getting started on a\\nproblem, and it may not be obvious whether machine learning can help solve the problem at\\nall. On the other hand, if getting optimal performance is important, then you may need to\\ntry approaches that require more specialist knowledge. While it would be nice if machine\\nlearning were always easy , there is no a priori reason it should be trivially simple.\\n3.6 Other techniques\\nEach technique developed in this chapter is valuable to know in its own right, but that’s\\nnot the only reason I’ve explained them. The larger point is to familiarize you with some of\\nthe problems which can occur in neural networks, and with a style of analysis which can\\nhelp overcome those problems. In a sense, we’ve been learning how to think about neural\\nnets. Over the remainder of this chapter I brieﬂy sketch a handful of other techniques. These\\nsketches are less in-depth than the earlier discussions, but should convey some feeling for\\nthe diversity of techniques available for use in neural networks.\\n3.6.1 Variations on stochastic gradient descent\\nStochastic gradient descent by backpropagation has served us well in attacking the MNIST\\ndigit classiﬁcation problem. However, there are many other approaches to optimizing the cost\\nfunction, and sometimes those other approaches offer performance superior to mini-batch\\nstochastic gradient descent. In this section I sketch two such approaches, the Hessian and\\nmomentum techniques.\\nHessian technique: To begin our discussion it helps to put neural networks aside for a\\nbit. Instead, we’re just going to consider the abstract problem of minimizing a cost function\\nC which is a function of many variables, w = w1, w2, . . ., so C = C(w). By Taylor’s theorem,\\nthe cost function can be approximated near a point w by\\nC(w + ∆w) =C(w) +\\n∑\\nj\\n∂C\\n∂wj\\n∆wj + 1\\n2\\n∑\\njk\\n∆wj\\n∂2C\\n∂wj∂wk\\n∆wk + . . . (3.48)\\nWe can rewrite this more compactly as\\nC(w + ∆w) =C(w) +∇C ·∆w + 1\\n2∆wT H∆w + . . . , (3.49)\\nwhere ∇C is the usual gradient vector, andH is a matrix known as theHessian matrix, whose\\njk-th entry is ∂2C/∂wj∂wk. Suppose we approximate C by discarding the higher-order\\nterms represented by . . . above,\\nC(w + ∆w) ≈C(w) +∇C ·∆w + 1\\n2∆wT H∆w. (3.50)\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7f393c2a-d9ae-4757-aae1-66fae4f03c6b', embedding=None, metadata={'page_label': '119', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6. Other techniques\\n\\x0c\\x0c\\x0c 119\\nUsing calculus we can show that the expression on the right-hand side can be minimized35\\nby choosing\\n∆w = −H−1∇C. (3.51)\\nProvided (3.50) is a good approximate expression for the cost function, then we’d expect\\nthat moving from the point w to w + ∆w = w −H−1∇C should signiﬁcantly decrease the cost\\nfunction. That suggests a possible algorithm for minimizing the cost:\\n• Choose a starting point, w.\\n• Update w to a new pointw′= w−H−1∇C, where the HessianH and ∇C are computed\\nat w.\\n• Update w′ to a new point w′′ = w′−H′−1∇′C, where the Hessian H′ and ∇′C are\\ncomputed at w′.\\n• . . .\\nIn practice, (3.50) is only an approximation, and it’s better to take smaller steps. We do this\\nby repeatedly changing w by an amount ∆w = −ηH−1∇C, where ηis known as the learning\\nrate.\\nThis approach to minimizing a cost function is known as theHessian technique or Hessian\\noptimization. There are theoretical and empirical results showing that Hessian methods\\nconverge on a minimum in fewer steps than standard gradient descent. In particular, by\\nincorporating information about second-order changes in the cost function it’s possible\\nfor the Hessian approach to avoid many pathologies that can occur in gradient descent.\\nFurthermore, there are versions of the backpropagation algorithm which can be used to\\ncompute the Hessian.\\nIf Hessian optimization is so great, why aren’t we using it in our neural networks?\\nUnfortunately , while it has many desirable properties, it has one very undesirable property:\\nit’s very difﬁcult to apply in practice. Part of the problem is the sheer size of the Hessian matrix.\\nSuppose you have a neural network with 107 weights and biases. Then the corresponding\\nHessian matrix will contain 107 ×107 = 1014 entries. That’s a lot of entries! And that makes\\ncomputing H−1∇C extremely difﬁcult in practice. However, that doesn’t mean that it’s not\\nuseful to understand. In fact, there are many variations on gradient descent which are\\ninspired by Hessian optimization, but which avoid the problem with overly-large matrices.\\nLet’s take a look at one such technique, momentum-based gradient descent.\\nMomentum-based gradient descent: Intuitively , the advantage Hessian optimization\\nhas is that it incorporates not just information about the gradient, but also information about\\nhow the gradient is changing. Momentum-based gradient descent is based on a similar\\nintuition, but avoids large matrices of second derivatives. To understand the momentum\\ntechnique, think back to our original picture of gradient descent1.5, in which we considered\\na ball rolling down into a valley . At the time, we observed that gradient descent is, despite\\nits name, only loosely similar to a ball falling to the bottom of a valley. The momentum\\ntechnique modiﬁes gradient descent in two ways that make it more similar to the physical\\npicture. First, it introduces a notion of “velocity” for the parameters we’re trying to optimize.\\nThe gradient acts to change the velocity, not (directly) the “position”, in much the same\\nway as physical forces change the velocity , and only indirectly affect position. Second, the\\n35Strictly speaking, for this to be a minimum, and not merely an extremum, we need to assume that\\nthe Hessian matrix is positive deﬁnite. Intuitively, this means that the function C looks like a valley\\nlocally , not a mountain or a saddle.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f3915c11-a7dd-4da7-b28c-8d207184e5a3', embedding=None, metadata={'page_label': '120', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='120\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nmomentum method introduces a kind of friction term, which tends to gradually reduce the\\nvelocity .\\nLet’s give a more precise mathematical description. We introduce velocity variables\\nv = v1, v2, . . ., one for each corresponding wj variable36. Then we replace the gradient\\ndescent update rule w →w′= w −η∇C by\\nv →v′= µv −η∇C (3.52)\\nw →w′= w + v′. (3.53)\\nIn these equations, µis a hyper-parameter which controls the amount of damping or friction\\nin the system. To understand the meaning of the equations it’s helpful to ﬁrst consider the\\ncase where µ= 1, which corresponds to no friction. When that’s the case, inspection of\\nthe equations shows that the “force”∇C is now modifying the velocity ,v, and the velocity\\nis controlling the rate of change of w. Intuitively, we build up the velocity by repeatedly\\nadding gradient terms to it. That means that if the gradient is in (roughly) the same direction\\nthrough several rounds of learning, we can build up quite a bit of steam moving in that\\ndirection. Think, for example, of what happens if we’re moving straight down a slope:\\nWith each step the velocity gets larger down the slope, so we move more and more quickly\\nto the bottom of the valley . This can enable the momentum technique to work much faster\\nthan standard gradient descent. Of course, a problem is that once we reach the bottom of\\nthe valley we will overshoot. Or, if the gradient should change rapidly , then we could ﬁnd\\nourselves moving in the wrong direction. That’s the reason for the µhyper-parameter in\\n(3.52). I said earlier that µcontrols the amount of friction in the system; to be a little more\\nprecise, you should think of 1 −µas the amount of friction in the system. When µ= 1, as\\nwe’ve seen, there is no friction, and the velocity is completely driven by the gradient ∇C.\\nBy contrast, when µ= 0 there’s a lot of friction, the velocity can’t build up, and Equations\\n(3.52) and (3.53) reduce to the usual equation for gradient descent, w →w′= w −η∇C. In\\npractice, using a value of µintermediate between 0 and 1 can give us much of the beneﬁt of\\nbeing able to build up speed, but without causing overshooting. We can choose such a value\\nfor µusing the held-out validation data, in much the same way as we select ηand λ.\\nI’ve avoided naming the hyper-parameterµup to now. The reason is that the standard\\nname for µis badly chosen: it’s called the momentum co-efﬁcient. This is potentially\\n36In a neural net the wj variables would, of course, include all weights and biases.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6eeb79cd-f168-4aa7-b281-fed9a3a5db43', embedding=None, metadata={'page_label': '121', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6. Other techniques\\n\\x0c\\x0c\\x0c 121\\nconfusing, since µis not at all the same as the notion of momentum from physics. Rather, it\\nis much more closely related to friction. However, the term momentum co-efﬁcient is widely\\nused, so we will continue to use it.\\nA nice thing about the momentum technique is that it takes almost no work to mod-\\nify an implementation of gradient descent to incorporate momentum. We can still use\\nbackpropagation to compute the gradients, just as before, and use ideas such as sampling\\nstochastically chosen mini-batches. In this way, we can get some of the advantages of the\\nHessian technique, using information about how the gradient is changing. But it’s done\\nwithout the disadvantages, and with only minor modiﬁcations to our code. In practice, the\\nmomentum technique is commonly used, and often speeds up learning.\\nExercise\\n• What would go wrong if we used µ> 1 in the momentum technique?\\n• What would go wrong if we used µ< 0 in the momentum technique?\\nProblem\\n• Add momentum-based stochastic gradient descent to network2.py.\\nOther approaches to minimizing the cost function: Many other approaches to minimizing the\\ncost function have been developed, and there isn’t universal agreement on which is the best\\napproach. As you go deeper into neural networks it’s worth digging into the other techniques,\\nunderstanding how they work, their strengths and weaknesses, and how to apply them in\\npractice. A paper I mentioned earlier37 introduces and compares several of these techniques,\\nincluding conjugate gradient descent and the BFGS method (see also the closely related\\nlimited-memory BFGS method, known as L-BFGS). Another technique which has recently\\nshown promising results38 is Nesterov’s accelerated gradient technique, which improves on\\nthe momentum technique. However, for many problems, plain stochastic gradient descent\\nworks well, especially if momentum is used, and so we’ll stick to stochastic gradient descent\\nthrough the remainder of this book.\\nOther models of artiﬁcial neuron\\nUp to now we’ve built our neural networks using sigmoid neurons. In principle, a network\\nbuilt from sigmoid neurons can compute any function. In practice, however, networks built\\nusing other model neurons sometimes outperform sigmoid networks. Depending on the\\napplication, networks based on such alternate models may learn faster, generalize better to\\ntest data, or perhaps do both. Let me mention a couple of alternate model neurons, to give\\nyou the ﬂavor of some variations in common use.\\nPerhaps the simplest variation is the tanh (pronounced “tanch”) neuron, which replaces\\nthe sigmoid function by the hyperbolic tangent function. The output of a tanh neuron with\\ninput x, weight vector w, and bias b is given by\\ntanh(w ·x + b), (3.54)\\nwhere tanh is, of course, the hyperbolic tangent function. It turns out that this is very closely\\n37Efﬁcient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller (1998).\\n38See, for example, On the importance of initialization and momentum in deep learning, by Ilya\\nSutskever, James Martens, George Dahl, and Geoffrey Hinton (2012).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8f398e3b-a6a8-4903-96f1-913513aa7083', embedding=None, metadata={'page_label': '122', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='122\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nrelated to the sigmoid neuron. To see this, recall that the tanh function is deﬁned by\\ntanh(z) ≡ez −e−z\\nez + e−z . (3.55)\\nWith a little algebra it can easily be veriﬁed that\\nσ(z) =1 + tanh(z/2)\\n2 , (3.56)\\nthat is, tanh is just a rescaled version of the sigmoid function. We can also see graphically\\nthat the tanh function has the same shape as the sigmoid function,\\n−4 −2 2 4\\n−1\\n−0.5\\n0.5\\n1\\ntanh function\\nOne difference between tanh neurons and sigmoid neurons is that the output from tanh\\nneurons ranges from −1 to 1, not 0 to 1. This means that if you’re going to build a network\\nbased on tanh neurons you may need to normalize your outputs (and, depending on the\\ndetails of the application, possibly your inputs) a little differently than in sigmoid networks.\\nSimilar to sigmoid neurons, a network of tanh neurons can, in principle, compute any\\nfunction39 mapping inputs to the range −1 to 1. Furthermore, ideas such as backpropagation\\nand stochastic gradient descent are as easily applied to a network of tanh neurons as to a\\nnetwork of sigmoid neurons.\\nExercise\\n• Prove the identity in Equation (3.56).\\nWhich type of neuron should you use in your networks, the tanh or sigmoid? A priori the\\nanswer is not obvious, to put it mildly! However, there are theoretical arguments and some\\nempirical evidence to suggest that the tanh sometimes performs better 40. Let me brieﬂy\\ngive you the ﬂavor of one of the theoretical arguments for tanh neurons. Suppose we’re\\nusing sigmoid neurons, so all activations in our network are positive. Let’s consider the\\nweights wl+1\\njk input to the j-th neuron in the (l + 1)-th layer. The rules for backpropagation\\ntell us that the associated gradient will be al\\nkδl+1\\nj . Because the activations are positive the\\n39There are some technical caveats to this statement for both tanh and sigmoid neurons, as well as\\nfor the rectiﬁed linear neurons discussed below. However, informally it’s usually ﬁne to think of neural\\nnetworks as being able to approximate any function to arbitrary accuracy .\\n40See, for example, Efﬁcient BackProp, by Yann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert\\nMüller (1998), and Understanding the difﬁculty of training deep feedforward networks, by Xavier Glorot\\nand Yoshua Bengio (2010).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='31565ce8-f316-41db-89ab-cc1b2a4c5a36', embedding=None, metadata={'page_label': '123', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6. Other techniques\\n\\x0c\\x0c\\x0c 123\\nsign of this gradient will be the same as the sign of δl+1\\nj . What this means is that if δl+1\\nj\\nis positive then all the weights wl+1\\njk will decrease during gradient descent, while if δl+1\\nj is\\nnegative then all the weights wl+1\\njk will increase during gradient descent. In other words,\\nall weights to the same neuron must either increase together or decrease together. That’s\\na problem, since some of the weights may need to increase while others need to decrease.\\nThat can only happen if some of the input activations have different signs. That suggests\\nreplacing the sigmoid by an activation function, such as tanh, which allows both positive and\\nnegative activations. Indeed, because tanh is symmetric about zero, tanh(−z) =−tanh(z),\\nwe might even expect that, roughly speaking, the activations in hidden layers would be\\nequally balanced between positive and negative. That would help ensure that there is no\\nsystematic bias for the weight updates to be one way or the other.\\nHow seriously should we take this argument? While the argument is suggestive, it’s a\\nheuristic, not a rigorous proof that tanh neurons outperform sigmoid neurons. Perhaps there\\nare other properties of the sigmoid neuron which compensate for this problem? Indeed,\\nfor many tasks the tanh is found empirically to provide only a small or no improvement in\\nperformance over sigmoid neurons. Unfortunately , we don’t yet have hard-and-fast rules to\\nknow which neuron types will learn fastest, or give the best generalization performance, for\\nany particular application.\\nAnother variation on the sigmoid neuron is the rectiﬁed linear neuron or rectiﬁed linear\\nunit. The output of a rectiﬁed linear unit with input x, weight vector w, and bias b is given\\nby\\nmax(0, w ·x + b). (3.57)\\nGraphically , the rectifying function max(0, z) looks like this:\\n−4 −2 0 2 4\\n−4\\n−2\\n0\\n2\\n4\\nz\\nmax(0, z)\\nObviously such neurons are quite different from both sigmoid and tanh neurons. However,\\nlike the sigmoid and tanh neurons, rectiﬁed linear units can be used to compute any function,\\nand they can be trained using ideas such as backpropagation and stochastic gradient descent.\\nWhen should you use rectiﬁed linear units instead of sigmoid or tanh neurons? Some\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6b93dc33-44fe-47df-8f11-d81463e6627f', embedding=None, metadata={'page_label': '124', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='124\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nrecent work on image recognition41 has found considerable beneﬁt in using rectiﬁed linear\\nunits through much of the network. However, as with tanh neurons, we do not yet have a\\nreally deep understanding of when, exactly , rectiﬁed linear units are preferable, nor why . To\\ngive you the ﬂavor of some of the issues, recall that sigmoid neurons stop learning when\\nthey saturate, i.e., when their output is near either 0 or 1. As we’ve seen repeatedly in this\\nchapter, the problem is that σ′ terms reduce the gradient, and that slows down learning.\\nTanh neurons suffer from a similar problem when they saturate. By contrast, increasing\\nthe weighted input to a rectiﬁed linear unit will never cause it to saturate, and so there\\nis no corresponding learning slowdown. On the other hand, when the weighted input to\\na rectiﬁed linear unit is negative, the gradient vanishes, and so the neuron stops learning\\nentirely . These are just two of the many issues that make it non-trivial to understand when\\nand why rectiﬁed linear units perform better than sigmoid or tanh neurons.\\nI’ve painted a picture of uncertainty here, stressing that we do not yet have a solid theory\\nof how activation functions should be chosen. Indeed, the problem is harder even than I\\nhave described, for there are inﬁnitely many possible activation functions. Which is the\\nbest for any given problem? Which will result in a network which learns fastest? Which\\nwill give the highest test accuracies? I am surprised how little really deep and systematic\\ninvestigation has been done of these questions. Ideally , we’d have a theory which tells us, in\\ndetail, how to choose (and perhaps modify-on-the-ﬂy) our activation functions. On the other\\nhand, we shouldn’t let the lack of a full theory stop us! We have powerful tools already at\\nhand, and can make a lot of progress with those tools. Through the remainder of this book\\nI’ll continue to use sigmoid neurons as our go-to neuron, since they’re powerful and provide\\nconcrete illustrations of the core ideas about neural nets. But keep in the back of your mind\\nthat these same ideas can be applied to other types of neuron, and that there are sometimes\\nadvantages in doing so.\\nOn stories in neural networks\\nQuestion: How do you approach utilizing and researching machine learning\\ntechniques that are supported almost entirely empirically, as opposed to\\nmathematically? Also in what situations have you noticed some of these\\ntechniques fail?\\nAnswer: You have to realize that our theoretical tools are very weak.\\nSometimes, we have good mathematical intuitions for why a particular\\ntechnique should work. Sometimes our intuition ends up being wrong[...]\\nThe questions become: how well does my method work on this particular\\nproblem, and how large is the set of problems on which it works well.\\n— Question and answer with neural networks researcher Yann LeCun\\nOnce, attending a conference on the foundations of quantum mechanics, I noticed what\\nseemed to me a most curious verbal habit: when talks ﬁnished, questions from the audience\\n41See, for example, What is the Best Multi-Stage Architecture for Object Recognition?, by Kevin Jarrett,\\nKoray Kavukcuoglu, Marc’Aurelio Ranzato and Yann LeCun (2009), , by Xavier Glorot, Antoine Bordes,\\nand Yoshua Bengio (2011), and ImageNet Classiﬁcation with Deep Convolutional Neural Networks, by\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012). Note that these papers ﬁll in important\\ndetails about how to set up the output layer, cost function, and regularization in networks using rectiﬁed\\nlinear units. I’ve glossed over all these details in this brief account. The papers also discuss in more\\ndetail the beneﬁts and drawbacks of using rectiﬁed linear units. Another informative paper is Rectiﬁed\\nLinear Units Improve Restricted Boltzmann Machines, by Vinod Nair and Geoffrey Hinton (2010), which\\ndemonstrates the beneﬁts of using rectiﬁed linear units in a somewhat different approach to neural\\nnetworks.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6798790f-8353-42b9-9b38-6ea3c7a4af45', embedding=None, metadata={'page_label': '125', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6. Other techniques\\n\\x0c\\x0c\\x0c 125\\noften began with “I’m very sympathetic to your point of view, but[...]”. Quantum foundations\\nwas not my usual ﬁeld, and I noticed this style of questioning because at other scientiﬁc\\nconferences I’d rarely or never heard a questioner express their sympathy for the point of\\nview of the speaker. At the time, I thought the prevalence of the question suggested that\\nlittle genuine progress was being made in quantum foundations, and people were merely\\nspinning their wheels. Later, I realized that assessment was too harsh. The speakers were\\nwrestling with some of the hardest problems human minds have ever confronted. Of course\\nprogress was slow! But there was still value in hearing updates on how people were thinking,\\neven if they didn’t always have unarguable new progress to report.\\nYou may have noticed a verbal tic similar to “I’m very sympathetic[...]” in the current\\nbook. To explain what we’re seeing I’ve often fallen back on saying “Heuristically ,[...]”, or\\n“Roughly speaking,[...]”, following up with a story to explain some phenomenon or other.\\nThese stories are plausible, but the empirical evidence I’ve presented has often been pretty\\nthin. If you look through the research literature you’ll see that stories in a similar style appear\\nin many research papers on neural nets, often with thin supporting evidence. What should\\nwe think about such stories?\\nIn many parts of science – especially those parts that deal with simple phenomena – it’s\\npossible to obtain very solid, very reliable evidence for quite general hypotheses. But in\\nneural networks there are large numbers of parameters and hyper-parameters, and extremely\\ncomplex interactions between them. In such extraordinarily complex systems it’s exceedingly\\ndifﬁcult to establish reliable general statements. Understanding neural networks in their full\\ngenerality is a problem that, like quantum foundations, tests the limits of the human mind.\\nInstead, we often make do with evidence for or against a few speciﬁc instances of a general\\nstatement. As a result those statements sometimes later need to be modiﬁed or abandoned,\\nwhen new evidence comes to light.\\nOne way of viewing this situation is that any heuristic story about neural networks carries\\nwith it an implied challenge. For example, consider the statement I quoted earlier, explaining\\nwhy dropout works42: “This technique reduces complex co-adaptations of neurons, since a\\nneuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\\nlearn more robust features that are useful in conjunction with many different random subsets\\nof the other neurons.” This is a rich, provocative statement, and one could build a fruitful\\nresearch program entirely around unpacking the statement, ﬁguring out what in it is true,\\nwhat is false, what needs variation and reﬁnement. Indeed, there is now a small industry of\\nresearchers who are investigating dropout (and many variations), trying to understand how\\nit works, and what its limits are. And so it goes with many of the heuristics we’ve discussed.\\nEach heuristic is not just a (potential) explanation, it’s also a challenge to investigate and\\nunderstand in more detail.\\nOf course, there is not time for any single person to investigate all these heuristic\\nexplanations in depth. It’s going to take decades (or longer) for the community of neural\\nnetworks researchers to develop a really powerful, evidence-based theory of how neural\\nnetworks learn. Does this mean you should reject heuristic explanations as unrigorous, and\\nnot sufﬁciently evidence-based? No! In fact, we need such heuristics to inspire and guide\\nour thinking. It’s like the great age of exploration: the early explorers sometimes explored\\n(and made new discoveries) on the basis of beliefs which were wrong in important ways.\\nLater, those mistakes were corrected as we ﬁlled in our knowledge of geography . When you\\n42From ImageNet Classiﬁcation with Deep Convolutional Neural Networks, by Alex Krizhevsky , Ilya\\nSutskever, and Geoffrey Hinton (2012).\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='166e08c0-143d-4bed-84ab-af5d2cd8ff8e', embedding=None, metadata={'page_label': '126', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='126\\n\\x0c\\x0c\\x0c Improving the way neural networks learn\\nunderstand something poorly – as the explorers understood geography , and as we understand\\nneural nets today – it’s more important to explore boldly than it is to be rigorously correct\\nin every step of your thinking. And so you should view these stories as a useful guide to\\nhow to think about neural nets, while retaining a healthy awareness of the limitations of\\nsuch stories, and carefully keeping track of just how strong the evidence is for any given line\\nof reasoning. Put another way, we need good stories to help motivate and inspire us, and\\nrigorous in-depth investigation in order to uncover the real facts of the matter.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7bb61b60-d4a2-4ed2-93a8-9074ff06620f', embedding=None, metadata={'page_label': '127', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 127\\n44444\\nA visual proof that neural nets\\ncan compute any function\\nOne of the most striking facts about neural networks is that they can compute any function\\nat all. That is, suppose someone hands you some complicated, wiggly function, f (x):\\nf (x)\\nNo matter what the function, there is guaranteed to be a neural network so that for every\\npossible input, x, the value f (x) (or some close approximation) is output from the network,\\ne.g.:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='de92036d-e114-41ad-a652-fe59662c929f', embedding=None, metadata={'page_label': '128', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='128\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nThis result holds even if the function has many inputs, f = f (x1, . . . ,xm), and many outputs.\\nFor instance, here’s a network computing a function withm = 3 inputs and n = 2 outputs:\\nThis result tells us that neural networks have a kind of universality. No matter what function\\nwe want to compute, we know that there is a neural network which can do the job.\\nWhat’s more, this universality theorem holds even if we restrict our networks to have\\njust a single layer intermediate between the input and the output neurons – a so-called single\\nhidden layer. So even very simple network architectures can be extremely powerful.\\nThe universality theorem is well known by people who use neural networks. But why it’s\\ntrue is not so widely understood. Most of the explanations available are quite technical. For\\ninstance, one of the original papers proving the result1 did so using the Hahn-Banach theorem,\\nthe Riesz Representation theorem, and some Fourier analysis. If you’re a mathematician the\\nargument is not difﬁcult to follow, but it’s not so easy for most people. That’s a pity, since\\nthe underlying reasons for universality are simple and beautiful.\\nIn this chapter I give a simple and mostly visual explanation of the universality theorem.\\nWe’ll go step by step through the underlying ideas. You’ll understand why it’s true that neural\\nnetworks can compute any function. You’ll understand some of the limitations of the result.\\nAnd you’ll understand how the result relates to deep neural networks.\\nTo follow the material in the chapter, you do not need to have read earlier chapters\\nin this book. Instead, the chapter is structured to be enjoyable as a self-contained essay.\\nProvided you have just a little basic familiarity with neural networks, you should be able to\\nfollow the explanation. I will, however, provide occasional links to earlier material, to help\\nﬁll in any gaps in your knowledge.\\n1Approximation by superpositions of a sigmoidal function, by George Cybenko (1989). The result\\nwas very much in the air at the time, and several groups proved closely related results. Cybenko’s\\npaper contains a useful discussion of much of that work. Another important early paper is Multilayer\\nfeedforward networks are universal approximators, by Kurt Hornik, Maxwell Stinchcombe, and Halbert\\nWhite (1989). This paper uses the Stone-Weierstrass theorem to arrive at similar results.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='50e8a45b-b473-4a46-9ad9-115517cc6620', embedding=None, metadata={'page_label': '129', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.1. Two caveats\\n\\x0c\\x0c\\x0c 129\\nUniversality theorems are a commonplace in computer science, so much so that we\\nsometimes forget how astonishing they are. But it’s worth reminding ourselves: the ability to\\ncompute an arbitrary function is truly remarkable. Almost any process you can imagine can\\nbe thought of as function computation. Consider the problem of naming a piece of music\\nbased on a short sample of the piece. That can be thought of as computing a function. Or\\nconsider the problem of translating a Chinese text into English. Again, that can be thought\\nof as computing a function 2. Or consider the problem of taking an mp4 movie ﬁle and\\ngenerating a description of the plot of the movie, and a discussion of the quality of the acting.\\nAgain, that can be thought of as a kind of function computation3 Universality means that, in\\nprinciple, neural networks can do all these things and many more.\\nOf course, just because we know a neural network exists that can (say) translate Chinese\\ntext into English, that doesn’t mean we have good techniques for constructing or even\\nrecognizing such a network. This limitation applies also to traditional universality theorems\\nfor models such as Boolean circuits. But, as we’ve seen earlier in the book, neural networks\\nhave powerful algorithms for learning functions. That combination of learning algorithms +\\nuniversality is an attractive mix. Up to now, the book has focused on the learning algorithms.\\nIn this chapter, we focus on universality , and what it means.\\n4.1 Two caveats\\nBefore explaining why the universality theorem is true, I want to mention two caveats to the\\ninformal statement “a neural network can compute any function”.\\nFirst, this doesn’t mean that a network can be used to exactly compute any function.\\nRather, we can get anapproximation that is as good as we want. By increasing the number of\\nhidden neurons we can improve the approximation. For instance, earlier (see 4) I illustrated\\na network computing some function f (x) using three hidden neurons. For most functions\\nonly a low-quality approximation will be possible using three hidden neurons. By increasing\\nthe number of hidden neurons (say , to ﬁve) we can typically get a better approximation:\\nAnd we can do still better by further increasing the number of hidden neurons.\\n2Actually , computing one of many functions, since there are often many acceptable translations of a\\ngiven piece of text.\\n3Ditto the remark about translation and there being many possible functions..\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8fb56efb-30a8-4ee4-923a-15553c23c81e', embedding=None, metadata={'page_label': '130', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='130\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nTo make this statement more precise, suppose we’re given a functionf (x) which we’d\\nlike to compute to within some desired accuracy ε> 0. The guarantee is that by using\\nenough hidden neurons we can always ﬁnd a neural network whose output g(x) satisﬁes\\n|g(x) −f (x)|<ε, for all inputs x. In other words, the approximation will be good to within\\nthe desired accuracy for every possible input.\\nThe second caveat is that the class of functions which can be approximated in the way\\ndescribed are the continuous functions. If a function is discontinuous, i.e., makes sudden,\\nsharp jumps, then it won’t in general be possible to approximate using a neural net. This\\nis not surprising, since our neural networks compute continuous functions of their input.\\nHowever, even if the function we’d really like to compute is discontinuous, it’s often the\\ncase that a continuous approximation is good enough. If that’s so, then we can use a neural\\nnetwork. In practice, this is not usually an important limitation.\\nSumming up, a more precise statement of the universality theorem is that neural networks\\nwith a single hidden layer can be used to approximate any continuous function to any desired\\nprecision. In this chapter we’ll actually prove a slightly weaker version of this result, using\\ntwo hidden layers instead of one. In the problems I’ll brieﬂy outline how the explanation\\ncan, with a few tweaks, be adapted to give a proof which uses only a single hidden layer.\\n4.2 Universality with one input and one output\\nTo understand why the universality theorem is true, let’s start by understanding how to\\nconstruct a neural network which approximates a function with just one input and one\\noutput:\\nf (x)\\nIt turns out that this is the core of the problem of universality. Once we’ve understood\\nthis special case it’s actually pretty easy to extend to functions with many inputs and many\\noutputs.\\nTo build insight into how to construct a network to compute f , let’s start with a network\\ncontaining just a single hidden layer, with two hidden neurons, and an output layer containing\\na single output neuron:\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='fd92b41a-5e9d-4834-8866-8ed7cabd2db4', embedding=None, metadata={'page_label': '131', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Universality with one input and one output\\n\\x0c\\x0c\\x0c 131\\nTo get a feel for how components in the network work, let’s focus on the top hidden neuron.\\nIn the diagram below, click on the weight,w, and drag the mouse a little ways to the right to\\nincrease w. You can immediately see how the function computed by the top hidden neuron\\nchanges:\\n0 1\\n1\\nOutput from neuron\\nw = 7, b = −2\\nw = 7, b = −3\\nw = 7, b = −4\\nw = 7, b = −5\\nw = 7, b = −6\\n0 1\\n1\\nOutput from neuron\\nw = 9, b = −4\\nw = 8, b = −4\\nw = 7, b = −4\\nw = 6, b = −4\\nw = 5, b = −4\\nAs we learnt earlier in the book, what’s being computed by the hidden neuron isσ(wx + b),\\nwhere σ(z) ≡1/(1+ e−z) is the sigmoid function. Up to now, we’ve made frequent use of this\\nalgebraic form. But for the proof of universality we will obtain more insight by ignoring the\\nalgebra entirely , and instead manipulating and observing the shape shown in the graph. This\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e9cfebeb-3981-4800-828d-03305987f8de', embedding=None, metadata={'page_label': '132', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='132\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nwon’t just give us a better feel for what’s going on, it will also give us a proof4 of universality\\nthat applies to activation functions other than the sigmoid function. We can simplify our\\nanalysis quite a bit by increasing the weight so much that the output really is a step function,\\nto a very good approximation. Below I’ve plotted the output from the top hidden neuron\\nwhen the weight is w = 999.\\n1\\n1\\nx\\nOutput from top hidden neuron\\nIt’s actually quite a bit easier to work with step functions than general sigmoid functions.\\nThe reason is that in the output layer we add up contributions from all the hidden neurons.\\nIt’s easy to analyze the sum of a bunch of step functions, but rather more difﬁcult to reason\\nabout what happens when you add up a bunch of sigmoid shaped curves. And so it makes\\nthings much easier to assume that our hidden neurons are outputting step functions. More\\nconcretely , we do this by ﬁxing the weightw to be some very large value, and then setting the\\nposition of the step by modifying the bias. Of course, treating the output as a step function\\nis an approximation, but it’s a very good approximation, and for now we’ll treat it as exact.\\nI’ll come back later to discuss the impact of deviations from this approximation.\\nAt what value of x does the step occur? Put another way , how does the position of the\\nstep depend upon the weight and bias?\\nTo answer this question, try modifying the weight and bias in the diagram above (you\\nmay need to scroll back a bit). Can you ﬁgure out how the position of the step depends on w\\nand b? With a little work you should be able to convince yourself that the position of the\\nstep is proportional to b, and inversely proportional to w.\\nIn fact, the step is at position s = −b/w, as you can see by modifying the weight and\\nbias in the following diagram:\\n4Strictly speaking, the visual approach I’m taking isn’t what’s traditionally thought of as a proof. But\\nI believe the visual approach gives more insight into why the result is true than a traditional proof. And,\\nof course, that kind of insight is the real purpose behind a proof. Occasionally , there will be small gaps in\\nthe reasoning I present: places where I make a visual argument that is plausible, but not quite rigorous.\\nIf this bothers you, then consider it a challenge to ﬁll in the missing steps. But don’t lose sight of the\\nreal purpose: to understand why the universality theorem is true.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e6443d75-bd1a-4572-84c8-c798caba796e', embedding=None, metadata={'page_label': '133', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Universality with one input and one output\\n\\x0c\\x0c\\x0c 133\\n−b/w = 0.4 1\\n1\\nx\\nOutput from top hidden neuron\\nIt will greatly simplify our lives to describe hidden neurons using just a single parameter, s,\\nwhich is the step position, s = −b/w. Try modifying s in the following diagram, in order to\\nget used to the new parameterization:\\ns 1\\n1\\nx\\nOutput from top hidden neuron\\nAs noted above, we’ve implicitly set the weightw on the input to be some large value – big\\nenough that the step function is a very good approximation. We can easily convert a neuron\\nparameterized in this way back into the conventional model, by choosing the bias b = −ws.\\nUp to now we’ve been focusing on the output from just the top hidden neuron. Let’s take\\na look at the behavior of the entire network. In particular, we’ll suppose the hidden neurons\\nare computing step functions parameterized by step points s1 (top neuron) and s2 (bottom\\nneuron). And they’ll have respective output weightsw1 and w2. Here’s the network:\\ns1 s2 1\\nw1\\n1\\nw1 + w2\\nx\\nWeighted output from hidden layer\\nWhat’s being plotted on the right is the weighted outputw1a1 + w2a2 from the hidden layer.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='efb4dd4d-3244-4a9b-873a-b24ca01748a0', embedding=None, metadata={'page_label': '134', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='134\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nHere, a1 and a2 are the outputs from the top and bottom hidden neurons, respectively 5.\\nThese outputs are denoted with as because they’re often known as the neurons’ activations.\\nTry increasing and decreasing the step point s1 of the top hidden neuron. Get a feel\\nfor how this changes the weighted output from the hidden layer. It’s particularly worth\\nunderstanding what happens when s1 goes past s2. You’ll see that the graph changes shape\\nwhen this happens, since we have moved from a situation where the top hidden neuron\\nis the ﬁrst to be activated to a situation where the bottom hidden neuron is the ﬁrst to be\\nactivated.\\nSimilarly , try manipulating the step points2 of the bottom hidden neuron, and get a feel\\nfor how this changes the combined output from the hidden neurons.\\nTry increasing and decreasing each of the output weights. Notice how this rescales the\\ncontribution from the respective hidden neurons. What happens when one of the weights is\\nzero?\\nFinally, try setting w1 to be 0.8 and w2 to be -0.8. You get a “bump” function, which\\nstarts at point s1, ends at point s2, and has height 0.8. For instance, the weighted output\\nmight look like this:\\ns1 s2 1\\nw1 = −w2\\n1\\nx\\nWeighted output from hidden layer\\nOf course, we can rescale the bump to have any height at all. Let’s use a single parameter,h,\\nto denote the height. To reduce clutter I’ll also remove the “s1 = . . .” and “w1 = . . .” notations.\\ns1 s2 1\\n−1\\nh\\n1\\nx\\nWeighted output from hidden layer\\nTry changing the value of h up and down, to see how the height of the bump changes. Try\\nchanging the height so it’s negative, and observe what happens. And try changing the step\\npoints to see how that changes the shape of the bump.\\n5Note, by the way, that the output from the whole network isσ(w1a1 + w2a2 + b), where b is the\\nbias on the output neuron. Obviously , this isn’t the same as the weighted output from the hidden layer,\\nwhich is what we’re plotting here. We’re going to focus on the weighted output from the hidden layer\\nright now, and only later will we think about how that relates to the output from the whole network.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8fdeab48-77d5-40dc-8541-4d1e4e7bc506', embedding=None, metadata={'page_label': '135', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Universality with one input and one output\\n\\x0c\\x0c\\x0c 135\\nYou’ll notice, by the way, that we’re using our neurons in a way that can be thought\\nof not just in graphical terms, but in more conventional programming terms, as a kind of\\nif-then-else statement, e.g.:\\nif input >= step point:\\nadd 1 to the weighted output\\nelse :\\nadd 0 to the weighted output\\nFor the most part I’m going to stick with the graphical point of view. But in what follows you\\nmay sometimes ﬁnd it helpful to switch points of view, and think about things in terms of\\nif-then-else.\\nWe can use our bump-making trick to get two bumps, by gluing two pairs of hidden\\nneurons together into the same network:\\n0 s1\\n1 s1\\n2 s2\\n1 s2\\n2 1\\nh1\\n−1\\n0\\nh2\\n1\\nWeighted output from hidden layer\\nI’ve suppressed the weights here, simply writing theh values for each pair of hidden neurons.\\nTry increasing and decreasing both h values, and observe how it changes the graph. Move\\nthe bumps around by changing the step points.\\nMore generally , we can use this idea to get as many peaks as we want, of any height. In\\nparticular, we can divide the interval [0,1] up into a large number, N, of subintervals, and\\nuse N pairs of hidden neurons to set up peaks of any desired height. Let’s see how this works\\nfor N = 5. That’s quite a few neurons, so I’m going to pack things in a bit. Apologies for the\\ncomplexity of the diagram: I could hide the complexity by abstracting away further, but I\\nthink it’s worth putting up with a little complexity , for the sake of getting a more concrete\\nfeel for how these networks work.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d121917b-a378-4b74-9cd1-c193572444ca', embedding=None, metadata={'page_label': '136', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='136\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\n1\\n−1\\n0\\n1\\nWeighted output\\nYou can see that there are ﬁve pairs of hidden neurons. The step points for the respective\\npairs of neurons are 0, 1/5, then 1/5, 2/5, and so on, out to 4/5, 5/5. These values are ﬁxed\\n– they make it so we get ﬁve evenly spaced bumps on the graph.\\nEach pair of neurons has a value of h associated to it. Remember, the connections output\\nfrom the neurons have weights h and −h (not marked). Click on one of the h values, and\\ndrag the mouse to the right or left to change the value. As you do so, watch the function\\nchange. By changing the output weights we’re actuallydesigning the function!\\nContrariwise, try clicking on the graph, and dragging up or down to change the height\\nof any of the bump functions. As you change the heights, you can see the corresponding\\nchange in h values. And, although it’s not shown, there is also a change in the corresponding\\noutput weights, which are +h and −h.\\nIn other words, we can directly manipulate the function appearing in the graph on the\\nright, and see that reﬂected in the h values on the left. A fun thing to do is to hold the mouse\\nbutton down and drag the mouse from one side of the graph to the other. As you do this you\\ndraw out a function, and get to watch the parameters in the neural network adapt.\\nTime for a challenge.\\nLet’s think back to the function I plotted at the beginning of the chapter:\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='66ee8930-4b17-4d16-8111-1dc11101fdd7', embedding=None, metadata={'page_label': '137', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2. Universality with one input and one output\\n\\x0c\\x0c\\x0c 137\\nf (x)\\nI didn’t say it at the time, but what I plotted is actually the function\\nf (x) =0.2 + 0.4x2 + 0.3x sin(15x) +0.05 cos(50x), (4.1)\\nplotted over x from 0 to 1, and with the y axis taking values from 0 to 1.\\nThat’s obviously not a trivial function.\\nYou’re going to ﬁgure out how to compute it using a neural network.\\nIn our networks above we’ve been analyzing the weighted combination\\n∑\\nj wj aj output\\nfrom the hidden neurons. We now know how to get a lot of control over this quantity . But,\\nas I noted earlier, this quantity is not what’s output from the network. What’s output from\\nthe network is σ(\\n∑\\nj wj aj + b) where b is the bias on the output neuron. Is there some way\\nwe can achieve control over the actual output from the network?\\nThe solution is to design a neural network whose hidden layer has a weighted output\\ngiven by σ−1 ◦f (x), where σ−1 is just the inverse of the σfunction. That is, we want the\\nweighted output from the hidden layer to be:\\n1\\n−2\\n−1\\n1\\n2\\nσ−1 ◦f (x)\\nIf we can do this, then the output from the network as a whole will be a good approximation\\nto f (x)6.\\nYour challenge, then, is to design a neural network to approximate the goal function\\nshown just above. To learn as much as possible, I want you to solve the problem twice. The\\nﬁrst time, please click on the graph, directly adjusting the heights of the different bump\\n6Note that I have set the bias on the output neuron to 0.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f3b7de34-546f-4472-9cd5-a1f2d4bc721d', embedding=None, metadata={'page_label': '138', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='138\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nfunctions. You should ﬁnd it fairly easy to get a good match to the goal function. How\\nwell you’re doing is measured by the average deviation between the goal function and the\\nfunction the network is actually computing. Your challenge is to drive the average deviation\\nas low as possible. You complete the challenge when you drive the average deviation to 0.40\\nor below7\\n1\\n−2\\n−1\\n1\\n2 Weighted output\\nσ= 0.38\\nYou’ve now ﬁgured out all the elements necessary for the network to approximately compute\\nthe function f (x)! It’s only a coarse approximation, but we could easily do much better,\\nmerely by increasing the number of pairs of hidden neurons, allowing more bumps.\\nIn particular, it’s easy to convert all the data we have found back into the standard\\nparametrization used for neural networks. Let me just recap quickly how that works.\\nThe ﬁrst layer of weights all have some large, constant value, say w = 1000.\\nThe biases on the hidden neurons are just b = −ws. So, for instance, for the second\\nhidden neuron s = 0.2 becomes b = −1000 ×0.2 = −200.\\nThe ﬁnal layer of weights are determined by the h values. So, for instance, the value\\nyou’ve chosen above for the ﬁrsth, h = −0.6, means that the output weights from the top\\ntwo hidden neurons are −0.6 and 0.6, respectively . And so on, for the entire layer of output\\nweights.\\nFinally , the bias on the output neuron is 0.\\nThat’s everything: we now have a complete description of a neural network which does\\na pretty good job computing our original goal function. And we understand how to improve\\nthe quality of the approximation by improving the number of hidden neurons.\\nWhat’s more, there was nothing special about our original goal function,f (x) =0.2 +\\n0.4x2 + 0.3 sin(15x) +0.05 cos(50x). We could have used this procedure for any continuous\\nfunction from [0, 1] to [0, 1]. In essence, we’re using our single-layer neural networks to\\n7This paragraph refers to interactive element, available online. The graph shows the ﬁnal result of\\nmanual minimization of average deviation.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='eb418372-8a67-40df-9abe-d2a66ce3ec02', embedding=None, metadata={'page_label': '139', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Many input variables\\n\\x0c\\x0c\\x0c 139\\nbuild a lookup table for the function. And we’ll be able to build on this idea to provide a\\ngeneral proof of universality .\\n4.3 Many input variables\\nLet’s extend our results to the case of many input variables. This sounds complicated, but\\nall the ideas we need can be understood in the case of just two inputs. So let’s address the\\ntwo-input case.\\nWe’ll start by considering what happens when we have two inputs to a neuron:\\nHere, we have inputs x and y, with corresponding weights w1 and w2, and a bias b on the\\nneuron. Let’s set the weightw2 to 0, and then play around with the ﬁrst weight, w1, and the\\nbias, b, to see how they affect the output from the neuron:\\n0 10\\n1\\nxy Output\\nw1 =8,b=−8\\n0 10\\n1\\nxy Output\\nw1 =8,b=−5\\n0 10\\n1\\nxy Output\\nw1 =8,b=−2\\n0 10\\n1\\nxy Output\\nw1 =11,b=−5\\n0 10\\n1\\nxy Output\\nw1 =8,b=−5\\n0 10\\n1\\nxy Output\\nw1 =5,b=−5\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7bc864f0-81fd-4c10-a4fc-12130b4b0377', embedding=None, metadata={'page_label': '140', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='140\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nAs you can see, with w2 = 0 the input y makes no difference to the output from the neuron.\\nIt’s as thoughx is the only input.\\nGiven this, what do you think happens when we increase the weight w1 to w1 = 100,\\nwith w2 remaining 0? If you don’t immediately see the answer, ponder the question for a\\nbit, and see if you can ﬁgure out what happens. Then try it out and see if you’re right. I’ve\\nshown what happens in the following movie:\\nJust as in our earlier discussion, as the input weight gets larger the output approaches a\\nstep function. The difference is that now the step function is in three dimensions. Also as\\nbefore, we can move the location of the step point around by modifying the bias. The actual\\nlocation of the step point is sx ≡−b/w1.\\nLet’s redo the above using the position of the step as the parameter:\\n0 10\\n1\\nx\\ny Output\\nsx = 0.25\\n0 10\\n1\\nx\\ny Output\\nsx = 0.5\\n0 10\\n1\\nx\\ny Output\\nsx = 0.7\\nHere, we assume the weight on the x input has some large value – I’ve usedw1 = 1000 –\\nand the weight w2 = 0. The number on the neuron is the step point, and the little x above\\nthe number reminds us that the step is in the x direction. Of course, it’s also possible to\\nget a step function in the y direction, by making the weight on the y input very large (say,\\nw2 = 1000), and the weight on the x equal to 0, i.e., w1 = 0:\\n0 10\\n1\\nx\\ny Output\\nsy = 0.25\\n0 10\\n1\\nx\\ny Output\\nsy = 0.5\\n0 10\\n1\\nx\\ny Output\\nsy = 0.7\\nThe number on the neuron is again the step point, and in this case the little y above the\\nnumber reminds us that the step is in the y direction. I could have explicitly marked the\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='28bd371f-eb97-4c20-8388-2fa7db1e590a', embedding=None, metadata={'page_label': '141', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Many input variables\\n\\x0c\\x0c\\x0c 141\\nweights on the x and y inputs, but decided not to, since it would make the diagram rather\\ncluttered. But do keep in mind that the little y marker implicitly tells us that the y weight is\\nlarge, and the x weight is 0.\\nWe can use the step functions we’ve just constructed to compute a three-dimensional\\nbump function. To do this, we use two neurons, each computing a step function in the x\\ndirection. Then we combine those step functions with weight h and −h, respectively , where\\nh is the desired height of the bump. It’s all illustrated in the following diagram:\\n0 10\\n1\\nx\\ny Output\\ns1\\nx = 0.3, s2\\nx = 0.7, h = 1\\n0 10\\n1\\nx\\ny Output\\ns1\\nx = 0.3, s2\\nx = 0.7, h = 0.75\\n0 10\\n1\\nx\\ny Output\\ns1\\nx = 0.4, s2\\nx = 0.7, h = 1\\nTry changing the value of the height, h. Observe how it relates to the weights in the network.\\nAnd see how it changes the height of the bump function on the right.\\nAlso, try changing the step point 0.30 associated to the top hidden neuron. Witness how\\nit changes the shape of the bump. What happens when you move it past the step point 0.70\\nassociated to the bottom hidden neuron?\\nWe’ve ﬁgured out how to make a bump function in thex direction. Of course, we can\\neasily make a bump function in the y direction, by using two step functions in they direction.\\nRecall that we do this by making the weight large on the y input, and the weight 0 on the x\\ninput. Here’s the result:\\n0 10\\n1\\nx\\ny Output\\ns1\\ny = 0.4, s2\\ny = 0.6, h = 0.8\\n0 10\\n1\\nx\\ny Output\\ns1\\ny = 0.3, s2\\ny = 0.7, h = 0.75\\n0 10\\n1\\nx\\ny Output\\ns1\\ny = 0.3, s2\\ny = 0.7, h = 1\\nThis looks nearly identical to the earlier network! The only thing explicitly shown as changing\\nis that there’s now little y markers on our hidden neurons. That reminds us that they’re\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d2740c71-6204-4f2a-83b9-d3d587ef82af', embedding=None, metadata={'page_label': '142', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='142\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nproducing y step functions, not x step functions, and so the weight is very large on the y\\ninput, and zero on the x input, not vice versa. As before, I decided not to show this explicitly ,\\nin order to avoid clutter. Let’s consider what happens when we add up two bump functions,\\none in the x direction, the other in the y direction, both of height h:\\n0 10\\n1\\nx\\ny Output\\ns1\\nx = 0.4, s2\\nx = 0.6\\ns1\\ny = 0.3, s2\\ny = 0.7, h = 1\\n0 10\\n1\\nx\\ny Output\\ns1\\nx = 0.4, s2\\nx = 0.6\\ns1\\ny = 0.3, s2\\ny = 0.7, h = 0.6\\nTo simplify the diagram I’ve dropped the connections with zero weight. For now, I’ve left in\\nthe little x and y markers on the hidden neurons, to remind you in what directions the bump\\nfunctions are being computed. We’ll drop even those markers later, since they’re implied\\nby the input variable. Try varying the parameter h. As you can see, this causes the output\\nweights to change, and also the heights of both the x and y bump functions. What we’ve\\nbuilt looks a little like a tower function:\\n0 10\\n1\\nx\\ny Output\\nTower function\\nIf we could build such tower functions, then we could use them to approximate arbitrary\\nfunctions, just by adding up many towers of different heights, and in different locations:\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c02cb9d4-f51c-43ab-9a99-d34a2c7db26c', embedding=None, metadata={'page_label': '143', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Many input variables\\n\\x0c\\x0c\\x0c 143\\n0 10\\n1\\nx\\ny Output\\nMany towers\\nOf course, we haven’t yet ﬁgured out how to build a tower function. What we have constructed\\nlooks like a central tower, of height 2h, with a surrounding plateau, of height h.\\nBut we can make a tower function. Remember that earlier we saw neurons can be used\\nto implement a type of ’inlineif-then-else statement:\\nif input >= threshold:\\noutput 1\\nelse :\\noutput 0\\nThat was for a neuron with just a single input. What we want is to apply a similar idea to\\nthe combined output from the hidden neurons:\\nif combined output from hidden neurons >= threshold:\\noutput 1\\nelse :\\noutput 0\\nIf we choose the threshold appropriately — say, a value of 3h/2, which is sandwiched\\nbetween the height of the plateau and the height of the central tower – we could squash the\\nplateau down to zero, and leave just the tower standing.\\nCan you see how to do this? Try experimenting with the following network to ﬁgure it\\nout. Note that we’re now plotting the output from the entire network, not just the weighted\\noutput from the hidden layer. This means we add a bias term to the weighted output from the\\nhidden layer, and apply the sigma function. Can you ﬁnd values for h and b which produce\\na tower? This is a bit tricky, so if you think about this for a while and remain stuck, here’s\\ntwo hints: (1) To get the output neuron to show the right kind of if-then-else behaviour,\\nwe need the input weights (all h or −h) to be large; and (2) the value of b determines the\\nscale of the if-then-else threshold.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ff077b78-5020-40c5-a037-eec7101a6b2b', embedding=None, metadata={'page_label': '144', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='144\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\n0 10\\n1\\nx\\ny Output\\nh = 0.3, b = −0.5\\n0 10\\n1\\nx\\ny Output\\nh = 1.0, b = −2.0\\n0 10\\n1\\nx\\ny Output\\nh = 3.0, b = −5.0\\nWith our initial parameters, the output looks like a ﬂattened version of the earlier diagram,\\nwith its tower and plateau. To get the desired behaviour, we increase the parameter h until\\nit becomes large. That gives the if-then-else thresholding behaviour. Second, to get the\\nthreshold right, we’ll chooseb ≈−3h/2. Try it, and see how it works!\\nHere’s what it looks like, when we useh = 10:\\n0 10\\n1\\nx\\ny Output\\nh = 10, b = −5\\n0 10\\n1\\nx\\ny Output\\nh = 10, b = −7\\n0 10\\n1\\nx\\ny Output\\nh = 10, b = −12\\n0 10\\n1\\nx\\ny Output\\nh = 10, b = −15\\nEven for this relatively modest value of h, we get a pretty good tower function. And, of\\ncourse, we can make it as good as we want by increasing h still further, and keeping the bias\\nas b = −3h/2.\\nLet’s try gluing two such networks together, in order to compute two different tower\\nfunctions. To make the respective roles of the two sub-networks clear I’ve put them in\\nseparate boxes, below: each box computes a tower function, using the technique described\\nabove. The graph on the right shows the weighted output from the second hidden layer, that\\nis, it’s a weighted combination of tower functions.\\n0 10\\n1\\nx\\ny Output\\nw1 = 0.8, w2 = 0.5\\n0 10\\n1\\nx\\ny Output\\nw1 = 0.2, w2 = 0.5\\n0 10\\n1\\nx\\ny Output\\nw1 = 0.2, w2 = 0.9\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='684c8613-b303-4d93-877d-da1e0b36a303', embedding=None, metadata={'page_label': '145', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.3. Many input variables\\n\\x0c\\x0c\\x0c 145\\nIn particular, you can see that by modifying the weights in the ﬁnal layer you can change the\\nheight of the output towers.\\nThe same idea can be used to compute as many towers as we like. We can also make\\nthem as thin as we like, and whatever height we like. As a result, we can ensure that the\\nweighted output from the second hidden layer approximates any desired function of two\\nvariables:\\n0 10\\n1\\nx\\ny Output\\nMany towers\\nIn particular, by making the weighted output from the second hidden layer a good approxi-\\nmation to σ−1 ◦f , we ensure the output from our network will be a good approximation to\\nany desired function, f .\\nWhat about functions of more than two variables?\\nLet’s try three variables x1, x2, x3. The following network can be used to compute a\\ntower function in four dimensions:\\nHere, the x1, x2, x3 denote inputs to the network. The s1, t1 and so on are step points for\\nneurons – that is, all the weights in the ﬁrst layer are large, and the biases are set to give the\\nstep points s1, t1, s2, . . .. The weights in the second layer alternate +h,−h, where h is some\\nvery large number. And the output bias is −5h/2.\\nThis network computes a function which is 1 provided three conditions are met: x1 is\\nbetween s1 and t1; x2 is between s2 and t2; and x3 is between s3 and t3. The network is 0\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5a6bad74-d329-4a3f-a6eb-58d4b6a9c232', embedding=None, metadata={'page_label': '146', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='146\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\neverywhere else. That is, it’s a kind of tower which is 1 in a little region of input space, and\\n0 everywhere else.\\nBy gluing together many such networks we can get as many towers as we want, and\\nso approximate an arbitrary function of three variables. Exactly the same idea works in m\\ndimensions. The only change needed is to make the output bias (−m + 1/2)h, in order to\\nget the right kind of sandwiching behavior to level the plateau.\\nOkay , so we now know how to use neural networks to approximate a real-valued function\\nof many variables. What about vector-valued functions f (x1, . . . ,xm) ∈Rn? Of course, such a\\nfunction can be regarded as justn separate real-valued functions, f 1(x1, . . . ,xm),f 2(x1, . . . ,xm),\\nand so on. So we create a network approximating f 1, another network for f 2, and so on.\\nAnd then we simply glue all the networks together. So that’s also easy to cope with.\\nProblem\\n• We’ve seen how to use networks with two hidden layers to approximate an arbitrary\\nfunction. Can you ﬁnd a proof showing that it’s possible with just a single hidden\\nlayer? As a hint, try working in the case of just two input variables, and showing that:\\n(a) it’s possible to get step functions not just in thex or y directions, but in an arbitrary\\ndirection; (b) by adding up many of the constructions from part (a) it’s possible to\\napproximate a tower function which is circular in shape, rather than rectangular;\\n(c) using these circular towers, it’s possible to approximate an arbitrary function. To\\ndo part (c) it may help to use ideas from a bit later in this chapter.\\n4.4 Extension beyond sigmoid neurons\\nWe’ve proved that networks made up of sigmoid neurons can compute any function. Recall\\nthat in a sigmoid neuron the inputs x1, x2, . . .result in the output σ(\\n∑\\nj wj xj + b), where wj\\nare the weights, b is the bias, and σis the sigmoid function:\\nWhat if we consider a different type of neuron, one using some other activation function,\\ns(z):\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7cbc9f43-7666-432d-aba2-17395cb00d5d', embedding=None, metadata={'page_label': '147', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.4. Extension beyond sigmoid neurons\\n\\x0c\\x0c\\x0c 147\\nThat is, we’ll assume that if our neurons have inputsx1, x2, . . ., weightsw1, w2, . . . and bias\\nb, then the output is s(\\n∑\\nj wj xj + b).\\nWe can use this activation function to get a step function, just as we did with the sigmoid.\\nTry ramping up the weight in the following, say to w = 100:\\n0 1\\n1\\nOutput from neuron\\nw = 100, b = −3\\nw = 15, b = −3\\nw = 8, b = −3\\nw = 6, b = −3\\nw = 4, b = −3\\n0 1\\n1 w = 6, b = −5\\nw = 6, b = −3\\nw = 6, b = −1\\nJust as with the sigmoid, this causes the activation function to contract, and ultimately it\\nbecomes a very good approximation to a step function. Try changing the bias, and you’ll see\\nthat we can set the position of the step to be wherever we choose. And so we can use all the\\nsame tricks as before to compute any desired function.\\nWhat properties does s(z) need to satisfy in order for this to work? We do need to assume\\nthat s(z) is well-deﬁned as z →∞ and z →∞. These two limits are the two values taken\\non by our step function. We also need to assume that these limits are different from one\\nanother. If they weren’t, there’d be no step, simply a ﬂat graph! But provided the activation\\nfunction s(z) satisﬁes these properties, neurons based on such an activation function are\\nuniversal for computation.\\nProblems\\n• Earlier in the book we met another type of neuron known as a rectiﬁed linear unit.\\nExplain why such neurons don’t satisfy the conditions just given for universality . Find a\\nproof of universality showing that rectiﬁed linear units are universal for computation.\\n• Suppose we consider linear neurons, i.e., neurons with the activation functions(z) =z.\\nExplain why linear neurons don’t satisfy the conditions just given for universality . Show\\nthat such neurons can’t be used to do universal computation.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='745d777d-d369-4294-87df-48345dbcfa7b', embedding=None, metadata={'page_label': '148', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='148\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\n4.5 Fixing up the step functions\\nUp to now, we’ve been assuming that our neurons can produce step functions exactly . That’s\\na pretty good approximation, but it is only an approximation. In fact, there will be a narrow\\nwindow of failure, illustrated in the following graph, in which the function behaves very\\ndifferently from a step function:\\nIn these windows of failure the explanation I’ve given for universality will fail.\\nNow, it’s not a terrible failure. By making the weights input to the neurons big enough\\nwe can make these windows of failure as small as we like. Certainly, we can make the\\nwindow much narrower than I’ve shown above – narrower, indeed, than our eye could see.\\nSo perhaps we might not worry too much about this problem.\\nNonetheless, it’d be nice to have some way of addressing the problem.\\nIn fact, the problem turns out to be easy to ﬁx. Let’s look at the ﬁx for neural networks\\ncomputing functions with just one input and one output. The same ideas work also to address\\nthe problem when there are more inputs and outputs.\\nIn particular, suppose we want our network to compute some function, f . As before, we\\ndo this by trying to design our network so that the weighted output from our hidden layer of\\nneurons is σ−1 ◦f (x):\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='3ceb3534-8f93-4dca-a493-78f818cb96b1', embedding=None, metadata={'page_label': '149', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.5. Fixing up the step functions\\n\\x0c\\x0c\\x0c 149\\nIf we were to do this using the technique described earlier, we’d use the hidden neurons to\\nproduce a sequence of bump functions:\\nAgain, I’ve exaggerated the size of the windows of failure, in order to make them easier to\\nsee. It should be pretty clear that if we add all these bump functions up we’ll end up with a\\nreasonable approximation to σ−1 ◦f (x), except within the windows of failure.\\nSuppose that instead of using the approximation just described, we use a set of hidden\\nneurons to compute an approximation to half our original goal function, i.e., toσ−1 ◦f (x)/2.\\nOf course, this looks just like a scaled down version of the last graph:\\nAnd suppose we use another set of hidden neurons to compute an approximation to σ−1 ◦\\nf (x)/2, but with the bases of the bumps shifted by half the width of a bump:\\nNow we have two different approximations to σ−1 ◦f (x)/2. If we add up the two approxi-\\nmations we’ll get an overall approximation toσ−1 ◦f (x). That overall approximation will\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='25781562-1a7b-43a5-915b-240f4a345ec7', embedding=None, metadata={'page_label': '150', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='150\\n\\x0c\\x0c\\x0c A visual proof that neural nets can compute any function\\nstill have failures in small windows. But the problem will be much less than before. The\\nreason is that points in a failure window for one approximation won’t be in a failure window\\nfor the other. And so the approximation will be a factor roughly 2 better in those windows.\\nWe could do even better by adding up a large number,M, of overlapping approximations\\nto the function σ−1 ◦f (x)/M. Provided the windows of failure are narrow enough, a point\\nwill only ever be in one window of failure. And provided we’re using a large enough number\\nM of overlapping approximations, the result will be an excellent overall approximation.\\nConclusion\\nThe explanation for universality we’ve discussed is certainly not a practical prescription for\\nhow to compute using neural networks! In this, it’s much like proofs of universality for NAND\\ngates and the like. For this reason, I’ve focused mostly on trying to make the construction\\nclear and easy to follow, and not on optimizing the details of the construction. However, you\\nmay ﬁnd it a fun and instructive exercise to see if you can improve the construction.\\nAlthough the result isn’t directly useful in constructing networks, it’s important because\\nit takes off the table the question of whether any particular function is computable using a\\nneural network. The answer to that question is always “yes”. So the right question to ask is\\nnot whether any particular function is computable, but rather what’s a good way to compute\\nthe function.\\nThe universality construction we’ve developed uses just two hidden layers to compute an\\narbitrary function. Furthermore, as we’ve discussed, it’s possible to get the same result with\\njust a single hidden layer. Given this, you might wonder why we would ever be interested\\nin deep networks, i.e., networks with many hidden layers. Can’t we simply replace those\\nnetworks with shallow, single hidden layer networks?\\nWhile in principle that’s possible, there are good practical reasons to use deep networks.\\nAs argued in Chapter 1, deep networks have a hierarchical structure which makes them\\nparticularly well adapted to learn the hierarchies of knowledge that seem to be useful in\\nsolving real-world problems. Put more concretely , when attacking problems such as image\\nrecognition, it helps to use a system that understands not just individual pixels, but also\\nincreasingly more complex concepts: from edges to simple geometric shapes, all the way\\nup through complex, multi-object scenes. In later chapters, we’ll see evidence suggesting\\nthat deep networks do a better job than shallow networks at learning such hierarchies of\\nknowledge. To sum up: universality tells us that neural networks can compute any function;\\nand empirical evidence suggests that deep networks are the networks best adapted to learn\\nthe functions useful in solving many real-world problems.\\n7Chapter acknowledgments: Thanks to Jen Dodd and Chris Olah for many discussions about univer-\\nsality in neural networks. My thanks, in particular, to Chris for suggesting the use of a lookup table to\\nprove universality . The interactive visual form of the chapter is inspired by the work of people such as\\nMike Bostock, Amit Patel, Bret Victor, and Steven Wittens.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ed4d788d-2fa1-495f-bfa2-61d35a228e3e', embedding=None, metadata={'page_label': '151', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 151\\n55555\\nWhy are deep neural networks\\nhard to train?\\nImagine you’re an engineer who has been asked to design a computer from scratch. One day\\nyou’re working away in your ofﬁce, designing logical circuits, setting outAND gates, OR gates,\\nand so on, when your boss walks in with bad news. The customer has just added a surprising\\ndesign requirement: the circuit for the entire computer must be just two layers deep:\\nYou’re dumbfounded, and tell your boss: “The customer is crazy!”\\nYour boss replies: “I think they’re crazy , too. But what the customer wants, they get.”\\nIn fact, there’s a limited sense in which the customer isn’t crazy . Suppose you’re allowed\\nto use a special logical gate which lets you AND together as many inputs as you want. And\\nyou’re also allowed a many-inputNAND gate, that is, a gate which can AND multiple inputs\\nand then negate the output. With these special gates it turns out to be possible to compute\\nany function at all using a circuit that’s just two layers deep.\\nBut just because something is possible doesn’t make it a good idea. In practice, when\\nsolving circuit design problems (or most any kind of algorithmic problem), we usually start', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='3c9d1e7a-d352-4ef1-b5f6-e5f37d10490f', embedding=None, metadata={'page_label': '152', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='152\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nby ﬁguring out how to solve sub-problems, and then gradually integrate the solutions. In\\nother words, we build up to a solution through multiple layers of abstraction.\\nFor instance, suppose we’re designing a logical circuit to multiply two numbers. Chances\\nare we want to build it up out of sub-circuits doing operations like adding two numbers.\\nThe sub-circuits for adding two numbers will, in turn, be built up out of sub-sub-circuits for\\nadding two bits. Very roughly speaking our circuit will look like:\\nThat is, our ﬁnal circuit contains at least three layers of circuit elements. In fact, it’ll probably\\ncontain more than three layers, as we break the sub-tasks down into smaller units than I’ve\\ndescribed. But you get the general idea.\\nSo deep circuits make the process of design easier. But they’re not just helpful for design.\\nThere are, in fact, mathematical proofs showing that for some functions very shallow circuits\\nrequire exponentially more circuit elements to compute than do deep circuits. For instance,\\na famous series of papers in the early 1980s1 showed that computing the parity of a set of\\nbits requires exponentially many gates, if done with a shallow circuit. On the other hand, if\\nyou use deeper circuits it’s easy to compute the parity using a small circuit: you just compute\\nthe parity of pairs of bits, then use those results to compute the parity of pairs of pairs of bits,\\nand so on, building up quickly to the overall parity. Deep circuits thus can be intrinsically\\nmuch more powerful than shallow circuits.\\nUp to now, this book has approached neural networks like the crazy customer. Almost all\\nthe networks we’ve worked with have just a single hidden layer of neurons (plus the input\\nand output layers):\\n1The history is somewhat complex, so I won’t give detailed references. See Johan Håstad’s 2012 paper\\nOn the correlation of parity and small-depth circuits for an account of the early history and references.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6ed369dd-452a-48e7-ba0f-174e5b8bf45b', embedding=None, metadata={'page_label': '153', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 153\\nThese simple networks have been remarkably useful: in earlier chapters we used networks\\nlike this to classify handwritten digits with better than 98 percent accuracy! Nonetheless,\\nintuitively we’d expect networks with many more hidden layers to be more powerful:\\nSuch networks could use the intermediate layers to build up multiple layers of abstraction,\\njust as we do in Boolean circuits. For instance, if we’re doing visual pattern recognition,\\nthen the neurons in the ﬁrst layer might learn to recognize edges, the neurons in the second\\nlayer could learn to recognize more complex shapes, say triangle or rectangles, built up from\\nedges. The third layer would then recognize still more complex shapes. And so on. These\\nmultiple layers of abstraction seem likely to give deep networks a compelling advantage in\\nlearning to solve complex pattern recognition problems. Moreover, just as in the case of\\ncircuits, there are theoretical results suggesting that deep networks are intrinsically more\\npowerful than shallow networks2.\\n2For certain problems and network architectures this is proved in On the number of response regions of\\ndeep feed forward networks with piece-wise linear activations, by Razvan Pascanu, Guido Montúfar, and\\nYoshua Bengio (2014). See also the more informal discussion in section 2 of Learning deep architectures\\nfor AI, by Yoshua Bengio (2009).\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='691ede88-75b8-4748-8716-6f38c92ee5bf', embedding=None, metadata={'page_label': '154', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='154\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nHow can we train such deep networks? In this chapter, we’ll try training deep networks\\nusing our workhorse learning algorithm – stochastic gradient descent by backpropagation.\\nBut we’ll run into trouble, with our deep networks not performing much (if at all) better\\nthan shallow networks.\\nThat failure seems surprising in the light of the discussion above. Rather than give up on\\ndeep networks, we’ll dig down and try to understand what’s making our deep networks hard\\nto train. When we look closely , we’ll discover that the different layers in our deep network\\nare learning at vastly different speeds. In particular, when later layers in the network are\\nlearning well, early layers often get stuck during training, learning almost nothing at all. This\\nstuckness isn’t simply due to bad luck. Rather, we’ll discover there are fundamental reasons\\nthe learning slowdown occurs, connected to our use of gradient-based learning techniques.\\nAs we delve into the problem more deeply, we’ll learn that the opposite phenomenon\\ncan also occur: the early layers may be learning well, but later layers can become stuck. In\\nfact, we’ll ﬁnd that there’s an intrinsic instability associated to learning by gradient descent\\nin deep, many-layer neural networks. This instability tends to result in either the early or\\nthe later layers getting stuck during training.\\nThis all sounds like bad news. But by delving into these difﬁculties, we can begin to gain\\ninsight into what’s required to train deep networks effectively. And so these investigations\\nare good preparation for the next chapter, where we’ll use deep learning to attack image\\nrecognition problems.\\n5.1 The vanishing gradient problem\\nSo, what goes wrong when we try to train a deep network?\\nTo answer that question, let’s ﬁrst revisit the case of a network with just a single hidden\\nlayer. As per usual, we’ll use the MNIST digit classiﬁcation problem as our playground for\\nlearning and experimentation3.\\nIf you wish, you can follow along by training networks on your computer. It is also, of\\ncourse, ﬁne to just read along. If you do wish to follow live, then you’ll need Python 2.7,\\nNumpy , and a copy of the code, which you can get by cloning the relevant repository from\\nthe command line:\\ngit clone https://github.com/mnielsen/neural -networks - and -deep -learning.git\\nIf you don’t use git then you can download the data and code here. You’ll need to change\\ninto the src subdirectory . Then, from a Python shell we load the MNIST data:\\n>>> import mnist_loader\\n>>> training_data , validation_data , test_data = \\\\\\n... mnist_loader.load_data_wrapper()\\nWe set up our network:\\n>>> import network2\\n>>> net = network2.Network([784, 30, 10])\\n3I introduced the MNIST problem and data here1.5 and here1.6.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b242fb72-e6c2-48e8-a956-b5ae898112ca', embedding=None, metadata={'page_label': '155', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1. The vanishing gradient problem\\n\\x0c\\x0c\\x0c 155\\nThis network has 784 neurons in the input layer, corresponding to the 28 ×28 = 784 pixels\\nin the input image. We use 30 hidden neurons, as well as 10 output neurons, corresponding\\nto the 10 possible classiﬁcations for the MNIST digits (‘0’, ‘1’, ‘2’,..., ‘9’).\\nLet’s try training our network for 30 complete epochs, using mini-batches of 10 training\\nexamples at a time, a learning rate η= 0.1, and regularization parameter λ= 5.0. As we\\ntrain we’ll monitor the classiﬁcation accuracy on thevalidation_data4:\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda=5.0,\\n... evaluation_data=validation_data , monitor_evaluation_accuracy=True)\\nWe get a classiﬁcation accuracy of 96.48 percent (or thereabouts – it’ll vary a bit from run to\\nrun), comparable to our earlier results with a similar conﬁguration.\\nNow, let’s add another hidden layer, also with 30 neurons in it, and try training with the\\nsame hyper-parameters:\\n>>> net = network2.Network([784, 30, 30, 10])\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda=5.0,\\n... evaluation_data=validation_data , monitor_evaluation_accuracy=True)\\nThis gives an improved classiﬁcation accuracy, 96.90 percent. That’s encouraging: a little\\nmore depth is helping. Let’s add another 30-neuron hidden layer:\\n>>> net = network2.Network([784, 30, 30, 30, 10])\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda=5.0,\\n... evaluation_data=validation_data , monitor_evaluation_accuracy=True)\\nThat doesn’t help at all. In fact, the result drops back down to 96.57 percent, close to our\\noriginal shallow network. And suppose we insert one further hidden layer:\\n>>> net = network2.Network([784, 30, 30, 30, 30, 10])\\n>>> net.SGD(training_data , 30, 10, 0.1, lmbda=5.0,\\n... evaluation_data=validation_data , monitor_evaluation_accuracy=True)\\nThe classiﬁcation accuracy drops again, to 96.53 percent. That’s probably not a statistically\\nsigniﬁcant drop, but it’s not encouraging, either.\\nThis behaviour seems strange. Intuitively , extra hidden layers ought to make the network\\nable to learn more complex classiﬁcation functions, and thus do a better job classifying.\\nCertainly , things shouldn’t get worse, since the extra layers can, in the worst case, simply do\\nnothing5. But that’s not what’s going on.\\nSo what is going on? Let’s assume that the extra hidden layers really could help in\\nprinciple, and the problem is that our learning algorithm isn’t ﬁnding the right weights and\\nbiases. We’d like to ﬁgure out what’s going wrong in our learning algorithm, and how to do\\nbetter.\\nTo get some insight into what’s going wrong, let’s visualize how the network learns.\\nBelow, I’ve plotted part of a[784,30,30,10] network, i.e., a network with two hidden layers,\\neach containing 30 hidden neurons. Each neuron in the diagram has a little bar on it,\\n4Note that the networks is likely to take some minutes to train, depending on the speed of your\\nmachine. So if you’re running the code you may wish to continue reading and return later, not wait for\\nthe code to ﬁnish executing.\\n5See this later problem5.2 to understand how to build a hidden layer that does nothing.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='02dfe6ed-9093-4777-8865-561bfb4ddaf2', embedding=None, metadata={'page_label': '156', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='156\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nrepresenting how quickly that neuron is changing as the network learns. A big bar means\\nthe neuron’s weights and bias are changing rapidly, while a small bar means the weights\\nand bias are changing slowly . More precisely , the bars denote the gradient∂C/∂b for each\\nneuron, i.e., the rate of change of the cost with respect to the neuron’s bias. Back in Chapter\\n2 we saw that this gradient quantity controlled not just how rapidly the bias changes during\\nlearning, but also how rapidly the weights input to the neuron change, too. Don’t worry if\\nyou don’t recall the details: the thing to keep in mind is simply that these bars show how\\nquickly each neuron’s weights and bias are changing as the network learns.\\nTo keep the diagram simple, I’ve shown just the top six neurons in the two hidden layers.\\nI’ve omitted the input neurons, since they’ve got no weights or biases to learn. I’ve also\\nomitted the output neurons, since we’re doing layer-wise comparisons, and it makes most\\nsense to compare layers with the same number of neurons. The results are plotted at the\\nvery beginning of training, i.e., immediately after the network is initialized. Here they are6:\\nThe network was initialized randomly , and so it’s not surprising that there’s a lot of variation\\nin how rapidly the neurons learn. Still, one thing that jumps out is that the bars in the second\\nhidden layer are mostly much larger than the bars in the ﬁrst hidden layer. As a result, the\\nneurons in the second hidden layer will learn quite a bit faster than the neurons in the ﬁrst\\nhidden layer. Is this merely a coincidence, or are the neurons in the second hidden layer\\nlikely to learn faster than neurons in the ﬁrst hidden layer in general?\\nTo determine whether this is the case, it helps to have a global way of comparing the\\nspeed of learning in the ﬁrst and second hidden layers. To do this, let’s denote the gradient\\nas δl\\nj = ∂C/∂bl\\nj, i.e., the gradient for the j-th neuron in the l-th layer7 We can think of the\\ngradient δ1 as a vector whose entries determine how quickly the ﬁrst hidden layer learns,\\nand δ2 as a vector whose entries determine how quickly the second hidden layer learns.\\n6The data plotted is generated using the program generate_gradient.py. The same program is\\nalso used to generate the results quoted later in this section.\\n7Back in Chapter 2 we referred to this as the error, but here we’ll adopt the informal term “gradient”.\\nI say “informal” because of course this doesn’t explicitly include the partial derivatives of the cost with\\nrespect to the weights, ∂C/∂w.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='45dfa94f-d9e2-48ec-a94d-93cf9a74cf59', embedding=None, metadata={'page_label': '157', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.1. The vanishing gradient problem\\n\\x0c\\x0c\\x0c 157\\nWe’ll then use the lengths of these vectors as (rough!) global measures of the speed at which\\nthe layers are learning. So,for instance, the length ∥δ1∥measures the speed at which the\\nﬁrst hidden layer is learning, while the length ∥δ2∥measures the speed at which the second\\nhidden layer is learning.\\nWith these deﬁnitions, and in the same conﬁguration as was plotted above, we ﬁnd\\n∥δ1∥= 0.07... and ∥δ2∥= 0.31.... So this conﬁrms our earlier suspicion: the neurons in\\nthe second hidden layer really are learning much faster than the neurons in the ﬁrst hidden\\nlayer.\\nWhat happens if we add more hidden layers? If we have three hidden layers, in a\\n[784,30,30,30,10] network, then the respective speeds of learning turn out to be 0.012,\\n0.060, and 0.283. Again, earlier hidden layers are learning much slower than later hidden\\nlayers. Suppose we add yet another layer with 30 hidden neurons. In that case, the respective\\nspeeds of learning are 0.003, 0.017, 0.070, and 0.285. The pattern holds: early layers learn\\nslower than later layers.\\nWe’ve been looking at the speed of learning at the start of training, that is, just after the\\nnetworks are initialized. How does the speed of learning change as we train our networks?\\nLet’s return to look at the network with just two hidden layers. The speed of learning changes\\nas follows:\\nTo generate these results, I used batch gradient descent with just 1,000 training images,\\ntrained over 500 epochs. This is a bit different than the way we usually train – I’ve used no\\nmini-batches, and just 1,000 training images, rather than the full 50,000 image training set.\\nI’m not trying to do anything sneaky, or pull the wool over your eyes, but it turns out that\\nusing mini-batch stochastic gradient descent gives much noisier (albeit very similar, when\\nyou average away the noise) results. Using the parameters I’ve chosen is an easy way of\\nsmoothing the results out, so we can see what’s going on.\\nIn any case, as you can see the two layers start out learning at very different speeds (as\\nwe already know). The speed in both layers then drops very quickly , before rebounding. But\\nthrough it all, the ﬁrst hidden layer learns much more slowly than the second hidden layer.\\nWhat about more complex networks? Here’s the results of a similar experiment, but this\\ntime with three hidden layers (a [784,30,30,30,10] network):\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='acd560f1-b000-4994-8246-54dff0f4feec', embedding=None, metadata={'page_label': '158', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='158\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nAgain, early hidden layers learn much more slowly than later hidden layers. Finally , let’s add\\na fourth hidden layer (a [784,30,30,30,30,10] network), and see what happens when we\\ntrain:\\nAgain, early hidden layers learn much more slowly than later hidden layers. In this case,\\nthe ﬁrst hidden layer is learning roughly 100 times slower than the ﬁnal hidden layer. No\\nwonder we were having trouble training these networks earlier!\\nWe have here an important observation: in at least some deep neural networks, the\\ngradient tends to get smaller as we move backward through the hidden layers. This means\\nthat neurons in the earlier layers learn much more slowly than neurons in later layers. And\\nwhile we’ve seen this in just a single network, there are fundamental reasons why this\\nhappens in many neural networks. The phenomenon is known as the vanishing gradient\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7304b77e-b1e1-4355-ab94-d628ce158431', embedding=None, metadata={'page_label': '159', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2. What’s causing the vanishing gradient problem? Unstable gradients in deep neural nets\\n\\x0c\\x0c\\x0c 159\\nproblem8.\\nWhy does the vanishing gradient problem occur? Are there ways we can avoid it? And\\nhow should we deal with it in training deep neural networks? In fact, we’ll learn shortly\\nthat it’s not inevitable, although the alternative is not very attractive, either: sometimes the\\ngradient gets much larger in earlier layers! This is the exploding gradient problem, and it’s\\nnot much better news than the vanishing gradient problem. More generally, it turns out\\nthat the gradient in deep neural networks is unstable, tending to either explode or vanish\\nin earlier layers. This instability is a fundamental problem for gradient-based learning in\\ndeep neural networks. It’s something we need to understand, and, if possible, take steps to\\naddress.\\nOne response to vanishing (or unstable) gradients is to wonder if they’re really such\\na problem. Momentarily stepping away from neural nets, imagine we were trying to nu-\\nmerically minimize a function f (x) of a single variable. Wouldn’t it be good news if the\\nderivative f ′(x) was small? Wouldn’t that mean we were already near an extremum? In a\\nsimilar way , might the small gradient in early layers of a deep network mean that we don’t\\nneed to do much adjustment of the weights and biases?\\nOf course, this isn’t the case. Recall that we randomly initialized the weight and biases\\nin the network. It is extremely unlikely our initial weights and biases will do a good job at\\nwhatever it is we want our network to do. To be concrete, consider the ﬁrst layer of weights\\nin a [784,30,30,30,10] network for the MNIST problem. The random initialization means\\nthe ﬁrst layer throws away most information about the input image. Even if later layers have\\nbeen extensively trained, they will still ﬁnd it extremely difﬁcult to identify the input image,\\nsimply because they don’t have enough information. And so it can’t possibly be the case that\\nnot much learning needs to be done in the ﬁrst layer. If we’re going to train deep networks,\\nwe need to ﬁgure out how to address the vanishing gradient problem.\\n5.2 What’s causing the vanishing gradient problem? Unstable\\ngradients in deep neural nets\\nTo get insight into why the vanishing gradient problem occurs, let’s consider the simplest\\ndeep neural network: one with just a single neuron in each layer. Here’s a network with\\nthree hidden layers:\\nHere, w1, w2, . . .are the weights, b1, b2, . . .are the biases, and C is some cost function. Just\\nto remind you how this works, the output aj from the j-th neuron is σ(zj), where σis the\\nusual sigmoid activation function, and zj = wj aj−1 + bj is the weighted input to the neuron.\\nI’ve drawn the costC at the end to emphasize that the cost is a function of the network’s\\noutput, a4: if the actual output from the network is close to the desired output, then the cost\\nwill be low, while if it’s far away , the cost will be high.\\n8See Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, by Sepp\\nHochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber (2001). This paper studied recurrent\\nneural nets, but the essential phenomenon is the same as in the feedforward networks we are studying.\\nSee also Sepp Hochreiter’s earlier Diploma Thesis, Untersuchungen zu dynamischen neuronalen Netzen\\n(1991, in German).\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2677c72f-80d5-47f1-8984-3f1d1c7b3ee4', embedding=None, metadata={'page_label': '160', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='160\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nWe’re going to study the gradient∂C/∂b1 associated to the ﬁrst hidden neuron. We’ll\\nﬁgure out an expression for ∂C/∂b1, and by studying that expression we’ll understand why\\nthe vanishing gradient problem occurs.\\nI’ll start by simply showing you the expression for ∂C/∂b1. It looks forbidding, but\\nit’s actually got a simple structure, which I’ll describe in a moment. Here’s the expression\\n(ignore the network, for now, and note that σ′is just the derivative of the σfunction):\\nThe structure in the expression is as follows: there is a σ′(zj) term in the product for each\\nneuron in the network; a weight wj term for each weight in the network; and a ﬁnal∂C/∂a4\\nterm, corresponding to the cost function at the end. Notice that I’ve placed each term in the\\nexpression above the corresponding part of the network. So the network itself is a mnemonic\\nfor the expression.\\nYou’re welcome to take this expression for granted, and skip to the discussion of how it\\nrelates to the vanishing gradient problem. There’s no harm in doing this, since the expression\\nis a special case of our earlier discussion of backpropagation. But there’s also a simple\\nexplanation of why the expression is true, and so it’s fun (and perhaps enlightening) to take\\na look at that explanation.\\nImagine we make a small change ∆b1 in the bias b1. That will set off a cascading series\\nof changes in the rest of the network. First, it causes a change ∆a1 in the output from the\\nﬁrst hidden neuron. That, in turn, will cause a change ∆z2 in the weighted input to the\\nsecond hidden neuron. Then a change ∆a2 in the output from the second hidden neuron.\\nAnd so on, all the way through to a change ∆C in the cost at the output. We have\\n∂C\\n∂b1\\n≈ ∆C\\n∆b1\\n. (5.1)\\nThis suggests that we can ﬁgure out an expression for the gradient ∂C/∂b1 by carefully\\ntracking the effect of each step in this cascade.\\nTo do this, let’s think about how∆b1 causes the output a1 from the ﬁrst hidden neuron\\nto change. We have a1 = σ(z1) =σ(w1a0 + b1), so\\n∆a1 ≈∂σ(w1a0 + b1)\\n∂b1\\n∆b1 = σ′(z1)∆b1. (5.2)\\nThat σ′(z1) term should look familiar: it’s the ﬁrst term in our claimed expression for the\\ngradient ∂C/∂b1. Intuitively , this term converts a change∆b1 in the bias into a change ∆a1\\nin the output activation. That change ∆a1 in turn causes a change in the weighted input\\nz2 = w2a1 + b2 to the second hidden neuron:\\n∆z2 ≈∂z2\\n∂a1\\n∆a1 = w2∆a1. (5.3)\\nCombining our expressions for∆z2 and ∆a1, we see how the change in the biasb1 propagates\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='19d44cce-e8f0-4447-b4f6-3ae15dd9aa58', embedding=None, metadata={'page_label': '161', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2. What’s causing the vanishing gradient problem? Unstable gradients in deep neural nets\\n\\x0c\\x0c\\x0c 161\\nalong the network to affect z2:\\n∆z2 ≈σ′(z1)w2∆b1. (5.4)\\nAgain, that should look familiar: we’ve now got the ﬁrst two terms in our claimed expression\\nfor the gradient ∂C/∂b1.\\nWe can keep going in this fashion, tracking the way changes propagate through the rest\\nof the network. At each neuron we pick up a σ′(zj) term, and through each weight we pick\\nup a wj term. The end result is an expression relating the ﬁnal change ∆C in cost to the\\ninitial change ∆b1 in the bias:\\n∆C ≈σ′(z1)w2σ′(z2) . . .σ′(z4) ∂C\\n∂a4\\n∆b1. (5.5)\\nDividing by ∆b1 we do indeed get the desired expression for the gradient:\\n∂C\\n∂b1\\n= σ′(z1)w2σ′(z2) . . .σ′(z4) ∂C\\n∂a4\\n. (5.6)\\nWhy the vanishing gradient problem occurs: To understand why the vanishing gradi-\\nent problem occurs, let’s explicitly write out the entire expression for the gradient:\\n∂C\\n∂b1\\n= σ′(z1) w2σ′(z2) w3σ′(z3) w4σ′(z4) ∂C\\n∂a4\\n. (5.7)\\nExcepting the very last term, this expression is a product of terms of the form wjσ′(zj). To\\nunderstand how each of those terms behave, let’s look at a plot of the functionσ′:\\n−4 −2 0 2 4\\n0\\n0.1\\n0.2\\nDerivative of sigmoid function\\nThe derivative reaches a maximum at σ′(0) =1/4. Now, if we use our standard approach to\\ninitializing the weights in the network, then we’ll choose the weights using a Gaussian with\\nmean 0 and standard deviation 1. So the weights will usually satisfy |wj|<1. Putting these\\nobservations together, we see that the terms wjσ′(zj) will usually satisfy |wjσ′(zj)|<1/4.\\nAnd when we take a product of many such terms, the product will tend to exponentially\\ndecrease: the more terms, the smaller the product will be. This is starting to smell like a\\npossible explanation for the vanishing gradient problem.\\nTo make this all a bit more explicit, let’s compare the expression for ∂C/∂b1 to an\\nexpression for the gradient with respect to a later bias, say ∂C/∂b3. Of course, we haven’t\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='67187e16-fa97-4e28-909e-35bdf4408e2e', embedding=None, metadata={'page_label': '162', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='162\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nexplicitly worked out an expression for ∂C/∂b3, but it follows the same pattern described\\nabove for ∂C/∂b1. Here’s the comparison of the two expressions:\\nThe two expressions share many terms. But the gradient ∂C/∂b1 includes two extra terms\\neach of the form wjσ′(zj). As we’ve seen, such terms are typically less than 1/4 in magnitude.\\nAnd so the gradient ∂C/∂b1 will usually be a factor of 16 (or more) smaller than ∂C/∂b3.\\nThis is the essential origin of the vanishing gradient problem.\\nOf course, this is an informal argument, not a rigorous proof that the vanishing gradient\\nproblem will occur. There are several possible escape clauses. In particular, we might wonder\\nwhether the weights wj could grow during training. If they do, it’s possible the termswjσ′(zj)\\nin the product will no longer satisfy |wjσ′(zj)|<1/4. Indeed, if the terms get large enough\\n– greater than 1 – then we will no longer have a vanishing gradient problem. Instead, the\\ngradient will actually grow exponentially as we move backward through the layers. Instead\\nof a vanishing gradient problem, we’ll have an exploding gradient problem.\\nThe exploding gradient problem: Let’s look at an explicit example where exploding\\ngradients occur. The example is somewhat contrived: I’m going to ﬁx parameters in the\\nnetwork in just the right way to ensure we get an exploding gradient. But even though the\\nexample is contrived, it has the virtue of ﬁrmly establishing that exploding gradients aren’t\\nmerely a hypothetical possibility , they really can happen.\\nThere are two steps to getting an exploding gradient. First, we choose all the weights\\nin the network to be large, say w1 = w2 = w3 = w4 = 100. Second, we’ll choose the biases\\nso that the σ′(zj) terms are not too small. That’s actually pretty easy to do: all we need\\ndo is choose the biases to ensure that the weighted input to each neuron is zj = 0 (and so\\nσ′(zj) =1/4). So, for instance, we want z1 = w1a0 + b1 = 0. We can achieve this by setting\\nb1 = −100 ×a0. We can use the same idea to select the other biases. When we do this, we\\nsee that all the terms wjσ′(zj) are equal to 100 ×1/4 = 25. With these choices we get an\\nexploding gradient.\\nThe unstable gradient problem: The fundamental problem here isn’t so much the\\nvanishing gradient problem or the exploding gradient problem. It’s that the gradient in\\nearly layers is the product of terms from all the later layers. When there are many layers,\\nthat’s an intrinsically unstable situation. The only way all layers can learn at close to the\\nsame speed is if all those products of terms come close to balancing out. Without some\\nmechanism or underlying reason for that balancing to occur, it’s highly unlikely to happen\\nsimply by chance. In short, the real problem here is that neural networks suffer from an\\nunstable gradient problem. As a result, if we use standard gradient-based learning techniques,\\ndifferent layers in the network will tend to learn at wildly different speeds.\\nExercise\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5232c875-6e2c-42b2-81ed-a4b7abbcab0e', embedding=None, metadata={'page_label': '163', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.3. Unstable gradients in more complex networks\\n\\x0c\\x0c\\x0c 163\\n• In our discussion of the vanishing gradient problem, we made use of the fact that\\n|σ′(z)|<1/4. Suppose we used a different activation function, one whose derivative\\ncould be much larger. Would that help us avoid the unstable gradient problem?\\nThe prevalence of the vanishing gradient problem: We’ve seen that the gradient can\\neither vanish or explode in the early layers of a deep network. In fact, when using sigmoid\\nneurons the gradient will usually vanish. To see why , consider again the expression|wσ′(z)|.\\nTo avoid the vanishing gradient problem we need |wσ′(z)|≥1. You might think this could\\nhappen easily if w is very large. However, it’s more difﬁcult than it looks. The reason is\\nthat the σ′(z) term also depends on w: σ′(z) =σ′(wa + b), where a is the input activation.\\nSo when we make w large, we need to be careful that we’re not simultaneously making\\nσ′(wa + b) small. That turns out to be a considerable constraint. The reason is that when\\nwe make w large we tend to make wa + b very large. Looking at the graph of σ′you can see\\nthat this puts us off in the “wings” of theσ′function, where it takes very small values. The\\nonly way to avoid this is if the input activation falls within a fairly narrow range of values\\n(this qualitative explanation is made quantitative in the ﬁrst problem below). Sometimes\\nthat will chance to happen. More often, though, it does not happen. And so in the generic\\ncase we have vanishing gradients.\\nProblems\\n• Consider the product |wσ′(wa + b)|. Suppose |wσ′(wa + b)|≥ 1. (1) Argue that\\nthis can only ever occur if |w|≥ 4. (2) Supposing that |w|≥ 4, consider the set of\\ninput activations a for which |wσ′(wa + b)|≥1. Show that the set of a satisfying that\\nconstraint can range over an interval no greater in width than\\n2\\n|w|ln\\n\\x12\\n|w|(1 +\\np\\n1 −4/|w|)\\n2 −1\\n\\x13\\n. (5.8)\\n(3) Show numerically that the above expression bounding the width of the range is\\ngreatest at |w|≈6.9, where it takes a value≈0.45. And so even given that everything\\nlines up just perfectly , we still have a fairly narrow range of input activations which\\ncan avoid the vanishing gradient problem.\\n• Identity neuron: Consider a neuron with a single input, x, a corresponding weight,\\nw1, a bias b, and a weight w2 on the output. Show that by choosing the weights and\\nbias appropriately , we can ensurew2σ(w1 x + b) ≈x for x ∈[0, 1]. Such a neuron can\\nthus be used as a kind of identity neuron, that is, a neuron whose output is the same\\n(up to rescaling by a weight factor) as its input. Hint: It helps to rewrite x = 1/2 + ∆,\\nto assume w1 is small, and to use a Taylor series expansion in w1∆.\\n5.3 Unstable gradients in more complex networks\\nWe’ve been studying toy networks, with just one neuron in each hidden layer. What about\\nmore complex deep networks, with many neurons in each hidden layer?\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8beaa694-046f-4ae1-b8a5-d9df84dcaba7', embedding=None, metadata={'page_label': '164', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='164\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\nIn fact, much the same behaviour occurs in such networks. In the earlier chapter on back-\\npropagation we saw that the gradient in the l-th layer of an L layer network is given by:\\nδl = Σ′(zl )(wl+1)T Σ′(zl+1)(wl+2)T . . .Σ′(zL)∇a C (5.9)\\nHere, Σ′(zl ) is a diagonal matrix whose entries are the σ′(z) values for the weighted inputs\\nto the l-th layer. The wl are the weight matrices for the different layers. And ∇a C is the\\nvector of partial derivatives of C with respect to the output activations.\\nThis is a much more complicated expression than in the single-neuron case. Still, if\\nyou look closely , the essential form is very similar, with lots of pairs of the form(wj)T Σ′(zj).\\nWhat’s more, the matricesΣ′(zj) have small entries on the diagonal, none larger than 1/4.\\nProvided the weight matrices wj aren’t too large, each additional term(wj)T Σ′(zl ) tends to\\nmake the gradient vector smaller, leading to a vanishing gradient. More generally , the large\\nnumber of terms in the product tends to lead to an unstable gradient, just as in our earlier\\nexample. In practice, empirically it is typically found in sigmoid networks that gradients\\nvanish exponentially quickly in earlier layers. As a result, learning slows down in those layers.\\nThis slowdown isn’t merely an accident or an inconvenience: it’s a fundamental consequence\\nof the approach we’re taking to learning.\\n5.4 Other obstacles to deep learning\\nIn this chapter we’ve focused on vanishing gradients – and, more generally, unstable gra-\\ndients – as an obstacle to deep learning. In fact, unstable gradients are just one obstacle\\nto deep learning, albeit an important fundamental obstacle. Much ongoing research aims\\nto better understand the challenges that can occur when training deep networks. I won’t\\ncomprehensively summarize that work here, but just want to brieﬂy mention a couple of\\npapers, to give you the ﬂavor of some of the questions people are asking.\\nAs a ﬁrst example, in 2010 Glorot and Bengio9 found evidence suggesting that the use of\\n9Understanding the difﬁculty of training deep feedforward neural networks, by Xavier Glorot and\\nYoshua Bengio (2010). See also the earlier discussion of the use of sigmoids in Efﬁcient BackProp, by\\nYann LeCun, Léon Bottou, Genevieve Orr and Klaus-Robert Müller (1998).\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='e1350b61-3e6b-4e73-ab9e-10699a29ad6a', embedding=None, metadata={'page_label': '165', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.4. Other obstacles to deep learning\\n\\x0c\\x0c\\x0c 165\\nsigmoid activation functions can cause problems training deep networks. In particular, they\\nfound evidence that the use of sigmoids will cause the activations in the ﬁnal hidden layer to\\nsaturate near 0 early in training, substantially slowing down learning. They suggested some\\nalternative activation functions, which appear not to suffer as much from this saturation\\nproblem.\\nAs a second example, in 2013 Sutskever, Martens, Dahl and Hinton10 studied the impact\\non deep learning of both the random weight initialization and the momentum schedule in\\nmomentum-based stochastic gradient descent. In both cases, making good choices made a\\nsubstantial difference in the ability to train deep networks.\\nThese examples suggest that “What makes deep networks hard to train?” is a complex\\nquestion. In this chapter, we’ve focused on the instabilities associated to gradient-based\\nlearning in deep networks. The results in the last two paragraphs suggest that there is\\nalso a role played by the choice of activation function, the way weights are initialized, and\\neven details of how learning by gradient descent is implemented. And, of course, choice of\\nnetwork architecture and other hyper-parameters is also important. Thus, many factors can\\nplay a role in making deep networks hard to train, and understanding all those factors is\\nstill a subject of ongoing research. This all seems rather downbeat and pessimism-inducing.\\nBut the good news is that in the next chapter we’ll turn that around, and develop several\\napproaches to deep learning that to some extent manage to overcome or route around all\\nthese challenges.\\n10On the importance of initialization and momentum in deep learning, by Ilya Sutskever, James\\nMartens, George Dahl and Geoffrey Hinton (2013).\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='98c71428-3857-4eb5-a045-333c2925f43f', embedding=None, metadata={'page_label': '166', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='166\\n\\x0c\\x0c\\x0c Why are deep neural networks hard to train?\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='591954c2-992f-4fe5-b39d-8eb713853066', embedding=None, metadata={'page_label': '167', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 167\\n66666\\nDeep learning\\nIn the last chapter we learned that deep neural networks are often much harder to train\\nthan shallow neural networks. That’s unfortunate, since we have good reason to believe\\nthat if we could train deep nets they’d be much more powerful than shallow nets. But while\\nthe news from the last chapter is discouraging, we won’t let it stop us. In this chapter, we’ll\\ndevelop techniques which can be used to train deep networks, and apply them in practice.\\nWe’ll also look at the broader picture, brieﬂy reviewing recent progress on using deep nets\\nfor image recognition, speech recognition, and other applications. And we’ll take a brief,\\nspeculative look at what the future may hold for neural nets, and for artiﬁcial intelligence.\\nThe chapter is a long one. To help you navigate, let’s take a tour. The sections are only\\nloosely coupled, so provided you have some basic familiarity with neural nets, you can jump\\nto whatever most interests you.\\nThe main part of the chapter is an introduction to one of the most widely used types of\\ndeep network: deep convolutional networks. We’ll work through a detailed example – code\\nand all – of using convolutional nets to solve the problem of classifying handwritten digits\\nfrom the MNIST data set:\\nWe’ll start our account of convolutional networks with the shallow networks used to attack this\\nproblem earlier in the book. Through many iterations we’ll build up more and more powerful\\nnetworks. As we go we’ll explore many powerful techniques: convolutions, pooling, the use\\nof GPUs to do far more training than we did with our shallow networks, the algorithmic\\nexpansion of our training data (to reduce overﬁtting), the use of the dropout technique (also\\nto reduce overﬁtting), the use of ensembles of networks, and others. The result will be a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6b27373e-ed13-453f-97e7-c2c7ed637f42', embedding=None, metadata={'page_label': '168', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='168\\n\\x0c\\x0c\\x0c Deep learning\\nsystem that offers near-human performance. Of the 10,000 MNIST test images – images\\nnot seen during training! – our system will classify 9,967 correctly . Here’s a peek at the 33\\nimages which are misclassiﬁed. Note that the correct classiﬁcation is in the top right; our\\nprogram’s classiﬁcation is in the bottom right:\\nMany of these are tough even for a human to classify . Consider, for example, the third image\\nin the top row. To me it looks more like a “9” than an “8”, which is the ofﬁcial classiﬁcation.\\nOur network also thinks it’s a “9”. This kind of “error” is at the very least understandable,\\nand perhaps even commendable. We conclude our discussion of image recognition with a\\nsurvey of some of the spectacular recent progress using networks (particularly convolutional\\nnets) to do image recognition.\\nThe remainder of the chapter discusses deep learning from a broader and less detailed\\nperspective. We’ll brieﬂy survey other models of neural networks, such as recurrent neural\\nnets and long short-term memory units, and how such models can be applied to problems in\\nspeech recognition, natural language processing, and other areas. And we’ll speculate about\\nthe future of neural networks and deep learning, ranging from ideas like intention-driven\\nuser interfaces, to the role of deep learning in artiﬁcial intelligence.\\nThe chapter builds on the earlier chapters in the book, making use of and integrating\\nideas such as backpropagation, regularization, the softmax function, and so on. However, to\\nread the chapter you don’t need to have worked in detail through all the earlier chapters. It\\nwill, however, help to have read Chapter 1, on the basics of neural networks. When I use\\nconcepts from Chapters 2 to 5, I provide links so you can familiarize yourself, if necessary .\\nIt’s worth noting what the chapter is not. It’s not a tutorial on the latest and greatest\\nneural networks libraries. Nor are we going to be training deep networks with dozens of\\nlayers to solve problems at the very leading edge. Rather, the focus is on understanding\\nsome of the core principles behind deep neural networks, and applying them in the simple,\\neasy-to-understand context of the MNIST problem. Put another way: the chapter is not\\ngoing to bring you right up to the frontier. Rather, the intent of this and earlier chapters is to\\nfocus on fundamentals, and so to prepare you to understand a wide range of current work.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b9ec77b3-ab70-4ac1-a051-dac207688a0d', embedding=None, metadata={'page_label': '169', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1. Introducing convolutional networks\\n\\x0c\\x0c\\x0c 169\\n6.1 Introducing convolutional networks\\nIn earlier chapters, we taught our neural networks to do a pretty good job recognizing images\\nof handwritten digits:\\nWe did this using networks in which adjacent network layers are fully connected to one\\nanother. That is, every neuron in the network is connected to every neuron in adjacent\\nlayers:\\nIn particular, for each pixel in the input image, we encoded the pixel’s intensity as the value\\nfor a corresponding neuron in the input layer. For the 28 ×28 pixel images we’ve been using,\\nthis means our network has 784 (= 28 ×28) input neurons. We then trained the network’s\\nweights and biases so that the network’s output would – we hope! – correctly identify the\\ninput image: ‘0’, ‘1’, ‘2’, ..., ‘8’, or ‘9’.\\nOur earlier networks work pretty well: we’ve obtained a classiﬁcation accuracy better\\nthan 98 percent, using training and test data from the MNIST handwritten digit data set. But\\nupon reﬂection, it’s strange to use networks with fully-connected layers to classify images.\\nThe reason is that such a network architecture does not take into account the spatial structure\\nof the images. For instance, it treats input pixels which are far apart and close together\\non exactly the same footing. Such concepts of spatial structure must instead be inferred\\nfrom the training data. But what if, instead of starting with a network architecture which is\\ntabula rasa, we used an architecture which tries to take advantage of the spatial structure?\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b0fd6986-65d1-4562-bbaf-6d0bb01877c5', embedding=None, metadata={'page_label': '170', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='170\\n\\x0c\\x0c\\x0c Deep learning\\nIn this section I describe convolutional neural networks 1. These networks use a special\\narchitecture which is particularly well-adapted to classify images. Using this architecture\\nmakes convolutional networks fast to train. This, in turn, helps us train deep, many-layer\\nnetworks, which are very good at classifying images. Today , deep convolutional networks or\\nsome close variant are used in most neural networks for image recognition.\\nConvolutional neural networks use three basic ideas: local receptive ﬁelds, shared weights,\\nand pooling. Let’s look at each of these ideas in turn.\\nLocal receptive ﬁelds: In the fully-connected layers shown earlier, the inputs were\\ndepicted as a vertical line of neurons. In a convolutional net, it’ll help to think instead of\\nthe inputs as a 28 ×28 square of neurons, whose values correspond to the 28 ×28 pixel\\nintensities we’re using as inputs:\\nAs per usual, we’ll connect the input pixels to a layer of hidden neurons. But we won’t\\nconnect every input pixel to every hidden neuron. Instead, we only make connections in\\nsmall, localized regions of the input image.\\nTo be more precise, each neuron in the ﬁrst hidden layer will be connected to a small\\nregion of the input neurons, say, for example, a 5 ×5 region, corresponding to 25 input\\npixels. So, for a particular hidden neuron, we might have connections that look like this:\\nThat region in the input image is called the local receptive ﬁeld for the hidden neuron. It’s a\\nlittle window on the input pixels. Each connection learns a weight. And the hidden neuron\\n1The origins of convolutional neural networks go back to the 1970s. But the seminal paper establishing\\nthe modern subject of convolutional networks was a 1998 paper, Gradient-based learning applied to\\ndocument recognition, by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. LeCun has\\nsince made an interesting remark on the terminology for convolutional nets: “The [biological] neural\\ninspiration in models like convolutional nets is very tenuous. That’s why I call them ‘convolutional\\nnets’ not ‘convolutional neural nets’, and why we call the nodes ‘units’ and not ‘neurons’ ”. Despite this\\nremark, convolutional nets use many of the same ideas as the neural networks we’ve studied up to now:\\nideas such as backpropagation, gradient descent, regularization, non-linear activation functions, and so\\non. And so we will follow common practice, and consider them a type of neural network. I will use the\\nterms “convolutional neural network” and “convolutional net(work)” interchangeably. I will also use\\nthe terms “[artiﬁcial] neuron” and “unit” interchangeably .\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bff77f51-0e30-4191-b172-5094f79abfd0', embedding=None, metadata={'page_label': '171', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1. Introducing convolutional networks\\n\\x0c\\x0c\\x0c 171\\nlearns an overall bias as well. You can think of that particular hidden neuron as learning to\\nanalyze its particular local receptive ﬁeld.\\nWe then slide the local receptive ﬁeld across the entire input image. For each local\\nreceptive ﬁeld, there is a different hidden neuron in the ﬁrst hidden layer. To illustrate this\\nconcretely , let’s start with a local receptive ﬁeld in the top-left corner:\\nThen we slide the local receptive ﬁeld over by one pixel to the right (i.e., by one neuron), to\\nconnect to a second hidden neuron:\\nAnd so on, building up the ﬁrst hidden layer. Note that if we have a28×28 input image, and\\n5 ×5 local receptive ﬁelds, then there will be 24 ×24 neurons in the hidden layer. This is\\nbecause we can only move the local receptive ﬁeld 23 neurons across (or 23 neurons down),\\nbefore colliding with the right-hand side (or bottom) of the input image.\\nI’ve shown the local receptive ﬁeld being moved by one pixel at a time. In fact, sometimes\\na different stride length is used. For instance, we might move the local receptive ﬁeld 2\\npixels to the right (or down), in which case we’d say a stride length of 2 is used. In this\\nchapter we’ll mostly stick with stride length 1, but it’s worth knowing that people sometimes\\nexperiment with different stride lengths2.\\nShared weights and biases: I’ve said that each hidden neuron has a bias and 5 ×5\\nweights connected to its local receptive ﬁeld. What I did not yet mention is that we’re going\\nto use the same weights and bias for each of the 24 ×24 hidden neurons. In other words,\\nfor the j, k-th hidden neuron, the output is:\\nσ\\n\\x82\\nb +\\n4∑\\nl=0\\n4∑\\nm=0\\nwl,maj+l,k+m\\n\\x8c\\n. (6.1)\\n2As was done in earlier chapters, if we’re interested in trying different stride lengths then we can use\\nvalidation data to pick out the stride length which gives the best performance. For more details, see the\\nearlier discussion of how to choose hyper-parameters in a neural network. The same approach may also\\nbe used to choose the size of the local receptive ﬁeld – there is, of course, nothing special about using a\\n5 ×5 local receptive ﬁeld. In general, larger local receptive ﬁelds tend to be helpful when the input\\nimages are signiﬁcantly larger than the 28 ×28 pixel MNIST images.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7240e2a6-f61b-496b-9e35-02edae2b7d0b', embedding=None, metadata={'page_label': '172', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='172\\n\\x0c\\x0c\\x0c Deep learning\\nHere, σis the neural activation function – perhaps the sigmoid function we used in earlier\\nchapters. b is the shared value for the bias. wl,m is a 5 ×5 array of shared weights. And,\\nﬁnally , we useax,y to denote the input activation at position x, y.\\nThis means that all the neurons in the ﬁrst hidden layer detect exactly the same feature3,\\njust at different locations in the input image. To see why this makes sense, suppose the\\nweights and bias are such that the hidden neuron can pick out, say, a vertical edge in a\\nparticular local receptive ﬁeld. That ability is also likely to be useful at other places in the\\nimage. And so it is useful to apply the same feature detector everywhere in the image. To put\\nit in slightly more abstract terms, convolutional networks are well adapted to the translation\\ninvariance of images: move a picture of a cat (say) a little ways, and it’s still an image of a\\ncat4.\\nFor this reason, we sometimes call the map from the input layer to the hidden layer a\\nfeature map. We call the weights deﬁning the feature map the shared weights. And we call\\nthe bias deﬁning the feature map in this way the shared bias. The shared weights and bias\\nare often said to deﬁne a kernel or ﬁlter. In the literature, people sometimes use these terms\\nin slightly different ways, and for that reason I’m not going to be more precise; rather, in a\\nmoment, we’ll look at some concrete examples.\\nThe network structure I’ve described so far can detect just a single kind of localized\\nfeature. To do image recognition we’ll need more than one feature map. And so a complete\\nconvolutional layer consists of several different feature maps:\\nIn the example shown, there are 3 feature maps. Each feature map is deﬁned by a set of\\n5 ×5 shared weights, and a single shared bias. The result is that the network can detect\\n3 different kinds of features, with each feature being detectable across the entire image.\\nI’ve shown just 3 feature maps, to keep the diagram above simple. However, in practice\\nconvolutional networks may use more (and perhaps many more) feature maps. One of the\\nearly convolutional networks, LeNet-5, used 6 feature maps, each associated to a 5 ×5 local\\nreceptive ﬁeld, to recognize MNIST digits. So the example illustrated above is actually pretty\\nclose to LeNet-5. In the examples we develop later in the chapter we’ll use convolutional\\nlayers with 20 and 40 feature maps. Let’s take a quick peek at some of the features which\\nare learned.\\n3I haven’t precisely deﬁned the notion of a feature. Informally, think of the feature detected by a\\nhidden neuron as the kind of input pattern that will cause the neuron to activate: it might be an edge in\\nthe image, for instance, or maybe some other type of shape.\\n4In fact, for the MNIST digit classiﬁcation problem we’ve been studying, the images are centered and\\nsize-normalized. So MNIST has less translation invariance than images found “in the wild”, so to speak.\\nStill, features like edges and corners are likely to be useful across much of the input space.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9eb19b60-ee59-4772-b408-3051002f3085', embedding=None, metadata={'page_label': '173', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1. Introducing convolutional networks\\n\\x0c\\x0c\\x0c 173\\nThe 20 images correspond to 20 different feature maps (or ﬁlters, or kernels). Each map is\\nrepresented as a 5 ×5 block image, corresponding to the 5 ×5 weights in the local receptive\\nﬁeld. Whiter blocks mean a smaller (typically, more negative) weight, so the feature map\\nresponds less to corresponding input pixels. Darker blocks mean a larger weight, so the\\nfeature map responds more to the corresponding input pixels. Very roughly speaking, the\\nimages above show the type of features the convolutional layer responds to.\\nSo what can we conclude from these feature maps? It’s clear there is spatial structure\\nhere beyond what we’d expect at random: many of the features have clear sub-regions\\nof light and dark. That shows our network really is learning things related to the spatial\\nstructure. However, beyond that, it’s difﬁcult to see what these feature detectors are learning.\\nCertainly , we’re not learning (say) the Gabor ﬁlters which have been used in many traditional\\napproaches to image recognition. In fact, there’s now a lot of work on better understanding\\nthe features learnt by convolutional networks. If you’re interested in following up on\\nthat work, I suggest starting with the paper Visualizing and Understanding Convolutional\\nNetworks by Matthew Zeiler and Rob Fergus (2013).\\nA big advantage of sharing weights and biases is that it greatly reduces the number of\\nparameters involved in a convolutional network. For each feature map we need 25 = 5 ×5\\nshared weights, plus a single shared bias. So each feature map requires 26 parameters. If we\\nhave 20 feature maps that’s a total of20 ×26 = 520 parameters deﬁning the convolutional\\nlayer. By comparison, suppose we had a fully connected ﬁrst layer, with 784 = 28 ×28 input\\nneurons, and a relatively modest 30 hidden neurons, as we used in many of the examples\\nearlier in the book. That’s a total of784 ×30 weights, plus an extra 30 biases, for a total\\nof 23,550 parameters. In other words, the fully-connected layer would have more than 40\\ntimes as many parameters as the convolutional layer.\\nOf course, we can’t really do a direct comparison between the number of parameters,\\nsince the two models are different in essential ways. But, intuitively , it seems likely that the\\nuse of translation invariance by the convolutional layer will reduce the number of parameters\\nit needs to get the same performance as the fully-connected model. That, in turn, will\\nresult in faster training for the convolutional model, and, ultimately , will help us build deep\\nnetworks using convolutional layers.\\nIncidentally , the name convolutional comes from the fact that the operation in Equation\\n(125) is sometimes known as a convolution. A little more precisely , people sometimes write\\nthat equation as a1 = σ(b + w ∗a0), where a1 denotes the set of output activations from\\none feature map, a0 is the set of input activations, and ∗is called a convolution operation.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='99254bc8-3988-4c01-b81a-0c8d50d1cd87', embedding=None, metadata={'page_label': '174', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='174\\n\\x0c\\x0c\\x0c Deep learning\\nWe’re not going to make any deep use of the mathematics of convolutions, so you don’t need\\nto worry too much about this connection. But it’s worth at least knowing where the name\\ncomes from.\\nPooling layers: In addition to the convolutional layers just described, convolutional\\nneural networks also contain pooling layers. Pooling layers are usually used immediately\\nafter convolutional layers. What the pooling layers do is simplify the information in the\\noutput from the convolutional layer.\\nIn detail, a pooling layer takes each feature map5 output from the convolutional layer\\nand prepares a condensed feature map. For instance, each unit in the pooling layer may\\nsummarize a region of (say) 2 ×2 neurons in the previous layer. As a concrete example,\\none common procedure for pooling is known as max-pooling. In max-pooling, a pooling\\nunit simply outputs the maximum activation in the 2 ×2 input region, as illustrated in the\\nfollowing diagram:\\nNote that since we have 24 ×24 neurons output from the convolutional layer, after pooling\\nwe have 12 ×12 neurons.\\nAs mentioned above, the convolutional layer usually involves more than a single feature\\nmap. We apply max-pooling to each feature map separately . So if there were three feature\\nmaps, the combined convolutional and max-pooling layers would look like:\\nWe can think of max-pooling as a way for the network to ask whether a given feature is found\\nanywhere in a region of the image. It then throws away the exact positional information.\\nThe intuition is that once a feature has been found, its exact location isn’t as important as its\\nrough location relative to other features. A big beneﬁt is that there are many fewer pooled\\nfeatures, and so this helps reduce the number of parameters needed in later layers.\\nMax-pooling isn’t the only technique used for pooling. Another common approach is\\nknown as L2 pooling. Here, instead of taking the maximum activation of a 2 ×2 region of\\n5The nomenclature is being used loosely here. In particular, I’m using “feature map” to mean not the\\nfunction computed by the convolutional layer, but rather the activation of the hidden neurons output\\nfrom the layer. This kind of mild abuse of nomenclature is pretty common in the research literature.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6bccf72d-d4ed-4ae1-a4ad-580df549c398', embedding=None, metadata={'page_label': '175', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1. Introducing convolutional networks\\n\\x0c\\x0c\\x0c 175\\nneurons, we take the square root of the sum of the squares of the activations in the 2 ×2\\nregion. While the details are different, the intuition is similar to max-pooling: L2 pooling is\\na way of condensing information from the convolutional layer. In practice, both techniques\\nhave been widely used. And sometimes people use other types of pooling operation. If\\nyou’re really trying to optimize performance, you may use validation data to compare several\\ndifferent approaches to pooling, and choose the approach which works best. But we’re not\\ngoing to worry about that kind of detailed optimization.\\nPutting it all together: We can now put all these ideas together to form a complete\\nconvolutional neural network. It’s similar to the architecture we were just looking at, but\\nhas the addition of a layer of 10 output neurons, corresponding to the 10 possible values for\\nMNIST digits (‘0’, ‘1’, ‘2’, etc):\\nThe network begins with28×28 input neurons, which are used to encode the pixel intensities\\nfor the MNIST image. This is then followed by a convolutional layer using a 5 ×5 local\\nreceptive ﬁeld and 3 feature maps. The result is a layer of3×24×24 hidden feature neurons.\\nThe next step is a max-pooling layer, applied to 2 ×2 regions, across each of the 3 feature\\nmaps. The result is a layer of 3 ×12 ×12 hidden feature neurons.\\nThe ﬁnal layer of connections in the network is a fully-connected layer. That is, this layer\\nconnects every neuron from the max-pooled layer to every one of the 10 output neurons.\\nThis fully-connected architecture is the same as we used in earlier chapters. Note, however,\\nthat in the diagram above, I’ve used a single arrow, for simplicity, rather than showing all\\nthe connections. Of course, you can easily imagine the connections.\\nThis convolutional architecture is quite different to the architectures used in earlier\\nchapters. But the overall picture is similar: a network made of many simple units, whose\\nbehaviors are determined by their weights and biases. And the overall goal is still the same:\\nto use training data to train the network’s weights and biases so that the network does a\\ngood job classifying input digits.\\nIn particular, just as earlier in the book, we will train our network using stochastic\\ngradient descent and backpropagation. This mostly proceeds in exactly the same way as in\\nearlier chapters. However, we do need to make a few modiﬁcations to the backpropagation\\nprocedure. The reason is that our earlier derivation of backpropagation was for networks\\nwith fully-connected layers. Fortunately, it’s straightforward to modify the derivation for\\nconvolutional and max-pooling layers. If you’d like to understand the details, then I invite\\nyou to work through the following problem. Be warned that the problem will take some time\\nto work through, unless you’ve really internalized the earlier derivation of backpropagation\\n(in which case it’s easy).\\nProblem\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='cf094e46-fad0-48d4-8d82-775df33da1b8', embedding=None, metadata={'page_label': '176', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='176\\n\\x0c\\x0c\\x0c Deep learning\\n• Backpropagation in a convolutional network The core equations of backpropagation in\\na network with fully-connected layers are (BP1)–(BP4). Suppose we have a network\\ncontaining a convolutional layer, a max-pooling layer, and a fully-connected output\\nlayer, as in the network discussed above. How are the equations of backpropagation\\nmodiﬁed?\\n6.2 Convolutional neural networks in practice\\nWe’ve now seen the core ideas behind convolutional neural networks. Let’s look at how they\\nwork in practice, by implementing some convolutional networks, and applying them to the\\nMNIST digit classiﬁcation problem. The program we’ll use to do this is callednetwork3.py,\\nand it’s an improved version of the programsnetwork.py and network2.py developed in\\nearlier chapters6. If you wish to follow along, the code is available on GitHub. Note that\\nwe’ll work through the code fornetwork3.py itself in the next section. In this section, we’ll\\nuse network3.py as a library to build convolutional networks.\\nThe programs network.py and network2.py were implemented using Python and\\nthe matrix library Numpy. Those programs worked from ﬁrst principles, and got right\\ndown into the details of backpropagation, stochastic gradient descent, and so on. But now\\nthat we understand those details, for network3.py we’re going to use a machine learning\\nlibrary known as Theano7. Using Theano makes it easy to implement backpropagation for\\nconvolutional neural networks, since it automatically computes all the mappings involved.\\nTheano is also quite a bit faster than our earlier code (which was written to be easy to\\nunderstand, not fast), and this makes it practical to train more complex networks. In\\nparticular, one great feature of Theano is that it can run code on either a CPU or, if available,\\na GPU. Running on a GPU provides a substantial speedup and, again, helps make it practical\\nto train more complex networks.\\nIf you wish to follow along, then you’ll need to get Theano running on your system.\\nTo install Theano, follow the instructions at the project’s homepage. The examples which\\nfollow were run using Theano 0.68. Some were run under Mac OS X Yosemite, with no GPU.\\nSome were run on Ubuntu 14.04, with an NVIDIA GPU. And some of the experiments were\\nrun under both. To get network3.py running you’ll need to set the GPU ﬂag to either True\\nor False (as appropriate) in the network3.py source. Beyond that, to get Theano up and\\nrunning on a GPU you may ﬁnd the instructions here helpful. There are also tutorials on the\\nweb, easily found using Google, which can help you get things working. If you don’t have a\\nGPU available locally, then you may wish to look into Amazon Web Services EC2 G2 spot\\ninstances. Note that even with a GPU the code will take some time to execute. Many of the\\nexperiments take from minutes to hours to run. On a CPU it may take days to run the most\\ncomplex of the experiments. As in earlier chapters, I suggest setting things running, and\\ncontinuing to read, occasionally coming back to check the output from the code. If you’re\\n6Note also that network3.py incorporates ideas from the Theano library’s documentation on convo-\\nlutional neural nets (notably the implementation of LeNet-5), from Misha Denil’s implementation of\\ndropout, and from Chris Olah.\\n7See Theano: A CPU and GPU Math Expression Compiler in Python, by James Bergstra, Olivier\\nBreuleux, Frederic Bastien, Pascal Lamblin, Ravzan Pascanu, Guillaume Desjardins, Joseph Turian, David\\nWarde-Farley , and Yoshua Bengio (2010). Theano is also the basis for the popular Pylearn2 and Keras\\nneural networks libraries. Other popular neural nets libraries at the time of this writing include Caffe\\nand Torch.\\n8As I release this chapter, the current version of Theano has changed to version 0.7. I’ve actually\\nrerun the examples under Theano 0.7 and get extremely similar results to those reported in the text.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f75f4fe9-d012-479a-aff0-4e4fe0b1cfa7', embedding=None, metadata={'page_label': '177', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2. Convolutional neural networks in practice\\n\\x0c\\x0c\\x0c 177\\nusing a CPU, you may wish to reduce the number of training epochs for the more complex\\nexperiments, or perhaps omit them entirely .\\nTo get a baseline, we’ll start with a shallow architecture using just a single hidden layer,\\ncontaining 100 hidden neurons. We’ll train for 60 epochs, using a learning rate ofη= 0.1, a\\nmini-batch size of 10, and no regularization. Here we go9:\\n>>> import network3\\n>>> from network3 import Network\\n>>> from network3 import ConvPoolLayer , FullyConnectedLayer , SoftmaxLayer\\n>>> training_data , validation_data , test_data = network3.load_data_shared()\\n>>> mini_batch_size = 10\\n>>> net = Network([FullyConnectedLayer(n_in=784, n_out =100),SoftmaxLayer(n_in\\n=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(training_data , 60, mini_batch_size , 0.1, validation_data , test_data)\\nI obtained a best classiﬁcation accuracy of 97.80 percent. This is the classiﬁcation accuracy\\non the test_data, evaluated at the training epoch where we get the best classiﬁcation\\naccuracy on the validation_data. Using the validation data to decide when to evaluate\\nthe test accuracy helps avoid overﬁtting to the test data (see this earlier discussion of the use\\nof validation data). We will follow this practice below. Your results may vary slightly , since\\nthe network’s weights and biases are randomly initialized10.\\nThis 97.80 percent accuracy is close to the 98.04 percent accuracy obtained back in\\nChapter 3, using a similar network architecture and learning hyper-parameters. In particular,\\nboth examples used a shallow network, with a single hidden layer containing 100 hidden\\nneurons. Both also trained for 60 epochs, used a mini-batch size of 10, and a learning rate\\nof η= 0.1.\\nThere were, however, two differences in the earlier network. First, we regularized the\\nearlier network, to help reduce the effects of overﬁtting. Regularizing the current network\\ndoes improve the accuracies, but the gain is only small, and so we’ll hold off worrying about\\nregularization until later. Second, while the ﬁnal layer in the earlier network used sigmoid\\nactivations and the cross-entropy cost function, the current network uses a softmax ﬁnal\\nlayer, and the log-likelihood cost function. As explained in Chapter 3 this isn’t a big change.\\nI haven’t made this switch for any particularly deep reason – mostly, I’ve done it because\\nsoftmax plus log-likelihood cost is more common in modern image classiﬁcation networks.\\nCan we do better than these results using a deeper network architecture?\\nLet’s begin by inserting a convolutional layer, right at the beginning of the network. We’ll\\nuse 5 by 5 local receptive ﬁelds, a stride length of 1, and 20 feature maps. We’ll also insert\\na max-pooling layer, which combines the features using 2 by 2 pooling windows. So the\\noverall network architecture looks much like the architecture discussed in the last section,\\nbut with an extra fully-connected layer:\\n9Code for the experiments in this section may be found in this script. Note that the code in the script\\nsimply duplicates and parallels the discussion in this section.\\nNote also that throughout the section I’ve explicitly speciﬁed the number of training epochs. I’ve done\\nthis for clarity about how we’re training. In practice, it’s worth using early stopping, that is, tracking\\naccuracy on the validation set, and stopping training when we are conﬁdent the validation accuracy has\\nstopped improving.\\n10In fact, in this experiment I actually did three separate runs training a network with this architecture.\\nI then reported the test accuracy which corresponded to the best validation accuracy from any of the\\nthree runs. Using multiple runs helps reduce variation in results, which is useful when comparing many\\narchitectures, as we are doing. I’ve followed this procedure below, except where noted. In practice, it\\nmade little difference to the results obtained.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8253977e-10af-414d-bb2e-f7fd8866b671', embedding=None, metadata={'page_label': '178', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='178\\n\\x0c\\x0c\\x0c Deep learning\\nIn this architecture, we can think of the convolutional and pooling layers as learning about\\nlocal spatial structure in the input training image, while the later, fully-connected layer learns\\nat a more abstract level, integrating global information from across the entire image. This is\\na common pattern in convolutional neural networks.\\nLet’s train such a network, and see how it performs11:\\n>>> net = Network([\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5),\\npoolsize=(2, 2)),\\nFullyConnectedLayer(n_in=20*12*12 , n_out =100),\\nSoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(training_data , 60, mini_batch_size , 0.1, validation_data , test_data)\\nThat gets us to 98.78 percent accuracy, which is a considerable improvement over any of\\nour previous results. Indeed, we’ve reduced our error rate by better than a third, which is a\\ngreat improvement.\\nIn specifying the network structure, I’ve treated the convolutional and pooling layers as\\na single layer. Whether they’re regarded as separate layers or as a single layer is to some\\nextent a matter of taste. network3.py treats them as a single layer because it makes the\\ncode for network3.py a little more compact. However, it is easy to modify network3.py so\\nthe layers can be speciﬁed separately , if desired.\\nExercise\\n• What classiﬁcation accuracy do you get if you omit the fully-connected layer, and\\njust use the convolutional-pooling layer and softmax layer? Does the inclusion of the\\nfully-connected layer help?\\nCan we improve on the 98.78 percent classiﬁcation accuracy?\\nLet’s try inserting a second convolutional-pooling layer. We’ll make the insertion between\\nthe existing convolutional-pooling layer and the fully-connected hidden layer. Again, we’ll\\nuse a 5 ×5 local receptive ﬁeld, and pool over 2 ×2 regions. Let’s see what happens when\\nwe train using similar hyper-parameters to before:\\n>>> net = Network([\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5),\\npoolsize=(2, 2)),\\nConvPoolLayer(\\n11I’ve continued to use a mini-batch size of 10 here. In fact, as we discussed earlier it may be possible\\nto speed up training using larger mini-batches. I’ve continued to use the same mini-batch size mostly for\\nconsistency with the experiments in earlier chapters.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='036d79a4-542c-4860-8cda-7b54b9eb604e', embedding=None, metadata={'page_label': '179', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2. Convolutional neural networks in practice\\n\\x0c\\x0c\\x0c 179\\nimage_shape=(mini_batch_size , 20, 12, 12),\\nfilter_shape=(40, 20, 5, 5),\\npoolsize=(2, 2)),\\nFullyConnectedLayer(n_in=40*4*4, n_out =100),\\nSoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(training_data , 60, mini_batch_size , 0.1, validation_data , test_data)\\nOnce again, we get an improvement: we’re now at 99.06 percent classiﬁcation accuracy!\\nThere’s two natural questions to ask at this point. The ﬁrst question is: what does it even\\nmean to apply a second convolutional-pooling layer? In fact, you can think of the second\\nconvolutional-pooling layer as having as input 12 ×12 “images”, whose “pixels” represent\\nthe presence (or absence) of particular localized features in the original input image. So you\\ncan think of this layer as having as input a version of the original input image. That version\\nis abstracted and condensed, but still has a lot of spatial structure, and so it makes sense to\\nuse a second convolutional-pooling layer.\\nThat’s a satisfying point of view, but gives rise to a second question. The output from\\nthe previous layer involves 20 separate feature maps, and so there are 20 ×12 ×12 inputs\\nto the second convolutional-pooling layer. It’s as though we’ve got 20 separate images\\ninput to the convolutional-pooling layer, not a single image, as was the case for the ﬁrst\\nconvolutional-pooling layer. How should neurons in the second convolutional-pooling layer\\nrespond to these multiple input images? In fact, we’ll allow each neuron in this layer to learn\\nfrom all 20 ×5 ×5 input neurons in its local receptive ﬁeld. More informally: the feature\\ndetectors in the second convolutional-pooling layer have access to all the features from the\\nprevious layer, but only within their particular local receptive ﬁeld12.\\nProblem\\n• Using the tanh activation function Several times earlier in the book I’ve mentioned\\narguments that the tanh function may be a better activation function than the sigmoid\\nfunction. We’ve never acted on those suggestions, since we were already making\\nplenty of progress with the sigmoid. But now let’s try some experiments with tanh\\nas our activation function. Try training the network with tanh activations in the\\nconvolutional and fully-connected layers13. Begin with the same hyper-parameters as\\nfor the sigmoid network, but train for 20 epochs instead of 60. How well does your\\nnetwork perform? What if you continue out to 60 epochs? Try plotting the per-epoch\\nvalidation accuracies for both tanh- and sigmoid-based networks, all the way out to 60\\nepochs. If your results are similar to mine, you’ll ﬁnd the tanh networks train a little\\nfaster, but the ﬁnal accuracies are very similar. Can you explain why the tanh network\\nmight train faster? Can you get a similar training speed with the sigmoid, perhaps\\nby changing the learning rate, or doing some rescaling14? Try a half-dozen iterations\\non the learning hyper-parameters or network architecture, searching for ways that\\ntanh may be superior to the sigmoid. Note: This is an open-ended problem. Personally,\\nI did not ﬁnd much advantage in switching to tanh, although I haven’t experimented\\nexhaustively, and perhaps you may ﬁnd a way. In any case, in a moment we will ﬁnd an\\nadvantage in switching to the rectiﬁed linear activation function, and so we won’t go any\\n12This issue would have arisen in the ﬁrst layer if the input images were in color. In that case we’d\\nhave 3 input features for each pixel, corresponding to red, green and blue channels in the input image.\\nSo we’d allow the feature detectors to have access to all color information, but only within a given local\\nreceptive ﬁeld.\\n13Note that you can pass activation_fn=tanh as a parameter to the ConvPoolLayer and\\nFullyConnectedLayer classes.\\n14You may perhaps ﬁnd inspiration in recalling that σ(z) = (1 + tanh(z/2))/2.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4655b87a-db1f-4c92-98c7-c6b2e7c1d1f8', embedding=None, metadata={'page_label': '180', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='180\\n\\x0c\\x0c\\x0c Deep learning\\ndeeper into the use of tanh.\\nUsing rectiﬁed linear units: The network we’ve developed at this point is actually a variant\\nof one of the networks used in the seminal 1998 paper15 introducing the MNIST problem,\\na network known as LeNet-5. It’s a good foundation for further experimentation, and for\\nbuilding up understanding and intuition. In particular, there are many ways we can vary the\\nnetwork in an attempt to improve our results.\\nAs a beginning, let’s change our neurons so that instead of using a sigmoid activation\\nfunction, we use rectiﬁed linear units. That is, we’ll use the activation function f (z) ≡\\nmax(0, z). We’ll train for 60 epochs, with a learning rate ofη= 0.03. I also found that it\\nhelps a little to use some l2 regularization, with regularization parameter λ= 0.1:\\n>>> from network3 import ReLU\\n>>> net = Network([\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5), poolsize=(2, 2), activation_fn=ReLU),\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 20, 12, 12), filter_shape=(40, 20, 5, 5),\\npoolsize=(2, 2), activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4, n_out\\n=100, activation_fn=ReLU),\\nSoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(training_data , 60, mini_batch_size , 0.03, validation_data , test_data ,\\nlmbda =0.1)\\nI obtained a classiﬁcation accuracy of 99.23 percent. It’s a modest improvement over the\\nsigmoid results (99.06). However, across all my experiments I found that networks based\\non rectiﬁed linear units consistently outperformed networks based on sigmoid activation\\nfunctions. There appears to be a real gain in moving to rectiﬁed linear units for this problem.\\nWhat makes the rectiﬁed linear activation function better than the sigmoid or tanh\\nfunctions? At present, we have a poor understanding of the answer to this question. Indeed,\\nrectiﬁed linear units have only begun to be widely used in the past few years. The reason\\nfor that recent adoption is empirical: a few people tried rectiﬁed linear units, often on the\\nbasis of hunches or heuristic arguments16. They got good results classifying benchmark data\\nsets, and the practice has spread. In an ideal world we’d have a theory telling us which\\nactivation function to pick for which application. But at present we’re a long way from such\\na world. I should not be at all surprised if further major improvements can be obtained by\\nan even better choice of activation function. And I also expect that in coming decades a\\npowerful theory of activation functions will be developed. Today, we still have to rely on\\npoorly understood rules of thumb and experience.\\nExpanding the training data: Another way we may hope to improve our results is by\\nalgorithmically expanding the training data. A simple way of expanding the training data is\\nto displace each training image by a single pixel, either up one pixel, down one pixel, left\\none pixel, or right one pixel. We can do this by running the program expand_mnist.py\\nfrom the shell prompt17:\\n15Gradient-based learning applied to document recognition, by Yann LeCun, Léon Bottou, Yoshua\\nBengio, and Patrick Haffner (1998). There are many differences of detail, but broadly speaking our\\nnetwork is quite similar to the networks described in the paper.\\n16A common justiﬁcation is that max(0, z) doesn’t saturate in the limit of large z, unlike sigmoid\\nneurons, and this helps rectiﬁed linear units continue learning. The argument is ﬁne, as far it goes, but\\nit’s hardly a detailed justiﬁcation, more of a just-so story. Note that we discussed the problems with\\nsaturation back in Chapter 2.\\n17The code for expand_mnist.py is available here.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='27df3b21-7b9d-4922-be31-03819a9f8021', embedding=None, metadata={'page_label': '181', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2. Convolutional neural networks in practice\\n\\x0c\\x0c\\x0c 181\\n$ python expand_mnist.py\\nRunning this program takes the 50,000 MNIST training images, and prepares an expanded\\ntraining set, with 250,000 training images. We can then use those training images to train\\nour network. We’ll use the same network as above, with rectiﬁed linear units. In my initial\\nexperiments I reduced the number of training epochs – this made sense, since we’re training\\nwith 5 times as much data. But, in fact, expanding the data turned out to considerably\\nreduce the effect of overﬁtting. And so, after some experimentation, I eventually went back\\nto training for 60 epochs. In any case, let’s train:\\n>>> expanded_training_data , _, _ = network3.load_data_shared(\"../data/\\nmnist_expanded.pkl.gz\")\\n>>> net = Network([\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nConvPoolLayer(\\nimage_shape=(mini_batch_size , 20, 12, 12),\\nfilter_shape=(40, 20, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nFullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\\nSoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(expanded_training_data , 60, mini_batch_size , 0.03, validation_data ,\\ntest_data , lmbda =0.1)\\nUsing the expanded training data I obtained a 99.37 percent training accuracy . So this almost\\ntrivial change gives a substantial improvement in classiﬁcation accuracy. Indeed, as we\\ndiscussed earlier this idea of algorithmically expanding the data can be taken further. Just to\\nremind you of the ﬂavour of some of the results in that earlier discussion: in 2003 Simard,\\nSteinkraus and Platt18 improved their MNIST performance to 99.6 percent using a neural\\nnetwork otherwise very similar to ours, using two convolutional-pooling layers, followed by\\na hidden fully-connected layer with 100 neurons. There were a few differences of detail in\\ntheir architecture – they didn’t have the advantage of using rectiﬁed linear units, for instance –\\nbut the key to their improved performance was expanding the training data. They did this by\\nrotating, translating, and skewing the MNIST training images. They also developed a process\\nof “elastic distortion”, a way of emulating the random oscillations hand muscles undergo\\nwhen a person is writing. By combining all these processes they substantially increased the\\neffective size of their training data, and that’s how they achieved 99.6 percent accuracy .\\nProblem\\n• The idea of convolutional layers is to behave in an invariant way across images. It\\nmay seem surprising, then, that our network can learn more when all we’ve done is\\ntranslate the input data. Can you explain why this is actually quite reasonable?\\nInserting an extra fully-connected layer: Can we do even better? One possibility is to use\\nexactly the same procedure as above, but to expand the size of the fully-connected layer. I\\ntried with 300 and 1,000 neurons, obtaining results of 99.46 and 99.43 percent, respectively .\\nThat’s interesting, but not really a convincing win over the earlier result (99.37 percent)\\n18Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis, by Patrice\\nSimard, Dave Steinkraus, and John Platt (2003).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='30fa0775-7822-4ab3-829a-9ee6ab8c2ed8', embedding=None, metadata={'page_label': '182', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='182\\n\\x0c\\x0c\\x0c Deep learning\\nWhat about adding an extra fully-connected layer? Let’s try inserting an extra fully-\\nconnected layer, so that we have two 100-hidden neuron fully-connected layers:\\n>>> net = Network([\\nConvPoolLayer(image_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nConvPoolLayer(image_shape=(mini_batch_size , 20, 12, 12),\\nfilter_shape=(40, 20, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nFullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\\nFullyConnectedLayer(n_in=100, n_out=100, activation_fn=ReLU),\\nSoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\\n>>> net.SGD(expanded_training_data , 60, mini_batch_size , 0.03, validation_data ,\\ntest_data , lmbda =0.1)\\nDoing this, I obtained a test accuracy of 99.43 percent. Again, the expanded net isn’t helping\\nso much. Running similar experiments with fully-connected layers containing 300 and 1,000\\nneurons yields results of 99.48 and 99.47 percent. That’s encouraging, but still falls short of\\na really decisive win.\\nWhat’s going on here? Is it that the expanded or extra fully-connected layers really\\ndon’t help with MNIST? Or might it be that our network has the capacity to do better, but\\nwe’re going about learning the wrong way? For instance, maybe we could use stronger\\nregularization techniques to reduce the tendency to overﬁt. One possibility is the dropout\\ntechnique introduced back in Chapter 3. Recall that the basic idea of dropout is to remove\\nindividual activations at random while training the network. This makes the model more\\nrobust to the loss of individual pieces of evidence, and thus less likely to rely on particular\\nidiosyncracies of the training data. Let’s try applying dropout to the ﬁnal fully-connected\\nlayers:\\n>>> net = Network([\\nConvPoolLayer(image_shape=(mini_batch_size , 1, 28, 28),\\nfilter_shape=(20, 1, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nConvPoolLayer(image_shape=(mini_batch_size , 20, 12, 12),\\nfilter_shape=(40, 20, 5, 5),\\npoolsize=(2, 2),\\nactivation_fn=ReLU),\\nFullyConnectedLayer(\\nn_in=40*4*4, n_out=1000, activation_fn=ReLU , p_dropout =0.5),\\nFullyConnectedLayer(\\nn_in=1000, n_out=1000, activation_fn=ReLU , p_dropout =0.5),\\nSoftmaxLayer(n_in=1000, n_out=10, p_dropout =0.5)],\\nmini_batch_size)\\n>>> net.SGD(expanded_training_data , 40, mini_batch_size , 0.03,\\nvalidation_data , test_data)\\nUsing this, we obtain an accuracy of 99.60 percent, which is a substantial improvement over\\nour earlier results, especially our main benchmark, the network with 100 hidden neurons,\\nwhere we achieved 99.37 percent.\\nThere are two changes worth noting.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f8ca8843-671e-4188-994a-d65fff25928a', embedding=None, metadata={'page_label': '183', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2. Convolutional neural networks in practice\\n\\x0c\\x0c\\x0c 183\\nFirst, I reduced the number of training epochs to 40: dropout reduced overﬁtting, and\\nso we learned faster.\\nSecond, the fully-connected hidden layers have 1,000 neurons, not the 100 used earlier.\\nOf course, dropout effectively omits many of the neurons while training, so some expansion\\nis to be expected. In fact, I tried experiments with both 300 and 1,000 hidden neurons, and\\nobtained (very slightly) better validation performance with 1,000 hidden neurons.\\nUsing an ensemble of networks: An easy way to improve performance still further is to\\ncreate several neural networks, and then get them to vote to determine the best classiﬁcation.\\nSuppose, for example, that we trained 5 different neural networks using the prescription\\nabove, with each achieving accuracies near to 99.6 percent. Even though the networks\\nwould all have similar accuracies, they might well make different errors, due to the different\\nrandom initializations. It’s plausible that taking a vote amongst our 5 networks might yield\\na classiﬁcation better than any individual network.\\nThis sounds too good to be true, but this kind of ensembling is a common trick with both\\nneural networks and other machine learning techniques. And it does in fact yield further\\nimprovements: we end up with 99.67 percent accuracy. In other words, our ensemble of\\nnetworks classiﬁes all but 33 of the 10,000 test images correctly .\\nThe remaining errors in the test set are shown below. The label in the top right is the\\ncorrect classiﬁcation, according to the MNIST data, while in the bottom right is the label\\noutput by our ensemble of nets:\\nIt’s worth looking through these in detail. The ﬁrst two digits, a 6 and a 5, are genuine errors\\nby our ensemble. However, they’re also understandable errors, the kind a human could\\nplausibly make. That 6 really does look a lot like a 0, and the 5 looks a lot like a 3. The third\\nimage, supposedly an 8, actually looks to me more like a 9. So I’m siding with the network\\nensemble here: I think it’s done a better job than whoever originally drew the digit. On the\\nother hand, the fourth image, the 6, really does seem to be classiﬁed badly by our networks.\\nAnd so on. In most cases our networks’ choices seem at least plausible, and in some cases\\nthey’ve done a better job classifying than the original person did writing the digit. Overall,\\nour networks offer exceptional performance, especially when you consider that they correctly\\nclassiﬁed 9,967 images which aren’t shown. In that context, the few clear errors here seem\\nquite understandable. Even a careful human makes the occasional mistake. And so I expect\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1606f2b0-4581-471b-b047-ec61046649ac', embedding=None, metadata={'page_label': '184', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='184\\n\\x0c\\x0c\\x0c Deep learning\\nthat only an extremely careful and methodical human would do much better. Our network\\nis getting near to human performance.\\nWhy we only applied dropout to the fully-connected layers: If you look carefully at\\nthe code above, you’ll notice that we applied dropout only to the fully-connected section of\\nthe network, not to the convolutional layers. In principle we could apply a similar procedure\\nto the convolutional layers. But, in fact, there’s no need: the convolutional layers have\\nconsiderable inbuilt resistance to overﬁtting. The reason is that the shared weights mean\\nthat convolutional ﬁlters are forced to learn from across the entire image. This makes them\\nless likely to pick up on local idiosyncracies in the training data. And so there is less need to\\napply other regularizers, such as dropout.\\nGoing further: It’s possible to improve performance on MNIST still further. Rodrigo\\nBenenson has compiled an informative summary page, showing progress over the years, with\\nlinks to papers. Many of these papers use deep convolutional networks along lines similar to\\nthe networks we’ve been using. If you dig through the papers you’ll ﬁnd many interesting\\ntechniques, and you may enjoy implementing some of them. If you do so it’s wise to start\\nimplementation with a simple network that can be trained quickly , which will help you more\\nrapidly understand what is going on.\\nFor the most part, I won’t try to survey this recent work. But I can’t resist making one\\nexception. It’s a 2010 paper by Cire¸ san, Meier, Gambardella, and Schmidhuber19. What I\\nlike about this paper is how simple it is. The network is a many-layer neural network, using\\nonly fully-connected layers (no convolutions). Their most successful network had hidden\\nlayers containing 2,500, 2,000, 1,500, 1,000, and 500 neurons, respectively . They used ideas\\nsimilar to Simard et al to expand their training data. But apart from that, they used few\\nother tricks, including no convolutional layers: it was a plain, vanilla network, of the kind\\nthat, with enough patience, could have been trained in the 1980s (if the MNIST data set had\\nexisted), given enough computing power. They achieved a classiﬁcation accuracy of 99.65\\npercent, more or less the same as ours. The key was to use a very large, very deep network,\\nand to use a GPU to speed up training. This let them train for many epochs. They also took\\nadvantage of their long training times to gradually decrease the learning rate from 10−3 to\\n10−6. It’s a fun exercise to try to match these results using an architecture like theirs.\\nWhy are we able to train? We saw in the last chapter that there are fundamental\\nobstructions to training in deep, many-layer neural networks. In particular, we saw that the\\ngradient tends to be quite unstable: as we move from the output layer to earlier layers the\\ngradient tends to either vanish (the vanishing gradient problem) or explode (the exploding\\ngradient problem). Since the gradient is the signal we use to train, this causes problems.\\nHow have we avoided those results?\\nOf course, the answer is that we haven’t avoided these results. Instead, we’ve done\\na few things that help us proceed anyway. In particular: (1) Using convolutional layers\\ngreatly reduces the number of parameters in those layers, making the learning problem\\nmuch easier; (2) Using more powerful regularization techniques (notably dropout and\\nconvolutional layers) to reduce overﬁtting, which is otherwise more of a problem in more\\ncomplex networks; (3) Using rectiﬁed linear units instead of sigmoid neurons, to speed up\\ntraining – empirically , often by a factor of 3–5; (4) Using GPUs and being willing to train for\\na long period of time. In particular, in our ﬁnal experiments we trained for 40 epochs using\\na data set 5 times larger than the raw MNIST training data. Earlier in the book we mostly\\n19Deep, Big, Simple Neural Nets Excel on Handwritten Digit Recognition, by Dan Claudiu Cire¸ san,\\nUeli Meier, Luca Maria Gambardella, and Jürgen Schmidhuber (2010).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bfcac386-5714-47f4-9eb1-ec3dba92d30f', embedding=None, metadata={'page_label': '185', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 185\\ntrained for 30 epochs using just the raw training data. Combining factors (3) and (4) it’s as\\nthough we’ve trained a factor perhaps 30 times longer than before.\\nYour response may be “Is that it? Is that all we had to do to train deep networks? What’s\\nall the fuss about?”\\nOf course, we’ve used other ideas, too: making use of sufﬁciently large data sets (to\\nhelp avoid overﬁtting); using the right cost function (to avoid a learning slowdown); using\\ngood weight initializations (also to avoid a learning slowdown, due to neuron saturation);\\nalgorithmically expanding the training data. We discussed these and other ideas in earlier\\nchapters, and have for the most part been able to reuse these ideas with little comment in\\nthis chapter.\\nWith that said, this really is a rather simple set of ideas. Simple, but powerful, when\\nused in concert. Getting started with deep learning has turned out to be pretty easy!\\nHow deep are these networks, anyway? Counting the convolutional-pooling layers\\nas single layers, our ﬁnal architecture has 4 hidden layers. Does such a network really\\ndeserve to be called a deep network? Of course, 4 hidden layers is many more than in\\nthe shallow networks we studied earlier. Most of those networks only had a single hidden\\nlayer, or occasionally 2 hidden layers. On the other hand, as of 2015 state-of-the-art deep\\nnetworks sometimes have dozens of hidden layers. I’ve occasionally heard people adopt a\\ndeeper-than-thou attitude, holding that if you’re not keeping-up-with-the-Joneses in terms\\nof number of hidden layers, then you’re not really doing deep learning. I’m not sympathetic\\nto this attitude, in part because it makes the deﬁnition of deep learning into something\\nwhich depends upon the result-of-the-moment. The real breakthrough in deep learning was\\nto realize that it’s practical to go beyond the shallow 1- and 2-hidden layer networks that\\ndominated work until the mid-2000s. That really was a signiﬁcant breakthrough, opening\\nup the exploration of much more expressive models. But beyond that, the number of layers\\nis not of primary fundamental interest. Rather, the use of deeper networks is a tool to use to\\nhelp achieve other goals – like better classiﬁcation accuracies.\\nA word on procedure: In this section, we’ve smoothly moved from single hidden-layer\\nshallow networks to many-layer convolutional networks. It all seemed so easy! We make a\\nchange and, for the most part, we get an improvement. If you start experimenting, I can\\nguarantee things won’t always be so smooth. The reason is that I’ve presented a cleaned-up\\nnarrative, omitting many experiments – including many failed experiments. This cleaned-up\\nnarrative will hopefully help you get clear on the basic ideas. But it also runs the risk of\\nconveying an incomplete impression. Getting a good, working network can involve a lot of\\ntrial and error, and occasional frustration. In practice, you should expect to engage in quite\\na bit of experimentation. To speed that process up you may ﬁnd it helpful to revisit Chapter\\n3’s discussion of how to choose a neural network’s hyper-parameters, and perhaps also to\\nlook at some of the further reading suggested in that section.\\n6.3 The code for our convolutional networks\\nAlright, let’s take a look at the code for our program,network3.py. Structurally , it’s similar\\nto network2.py, the program we developed in Chapter 3, although the details differ, due\\nto the use of Theano. We’ll start by looking at the FullyConnectedLayer class, which is\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='da43508c-4f35-4cf2-90ea-3f1db51d6f3c', embedding=None, metadata={'page_label': '186', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='186\\n\\x0c\\x0c\\x0c Deep learning\\nsimilar to the layers studied earlier in the book. Here’s the code (discussion below)20:\\nclass FullyConnectedLayer( object ):\\ndef __init__(self , n_in , n_out , activation_fn=sigmoid , p_dropout =0.0):\\nself.n_in = n_in\\nself.n_out = n_out\\nself.activation_fn = activation_fn\\nself.p_dropout = p_dropout\\n# Initialize weights and biases\\nself.w = theano.shared(\\nnp.asarray(\\nnp.random.normal(\\nloc=0.0, scale=np.sqrt(1.0/ n_out), size=(n_in , n_out)),\\ndtype=theano.config.floatX),\\nname=’w’, borrow=True)\\nself.b = theano.shared(\\nnp.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out ,)),\\ndtype=theano.config.floatX),\\nname=’b’, borrow=True)\\nself.params = [self.w, self.b]\\ndef set_inpt(self , inpt , inpt_dropout , mini_batch_size):\\nself.inpt = inpt.reshape((mini_batch_size , self.n_in))\\nself.output = self.activation_fn(\\n(1-self.p_dropout)*T.dot(self.inpt , self.w) + self.b)\\nself.y_out = T.argmax(self.output , axis=1)\\nself.inpt_dropout = dropout_layer(\\ninpt_dropout.reshape((mini_batch_size , self.n_in)), self.p_dropout)\\nself.output_dropout = self.activation_fn(\\nT.dot(self.inpt_dropout , self.w) + self.b)\\ndef accuracy(self , y):\\n\"Return the accuracy for the mini -batch.\"\\nreturn T.mean(T.eq(y, self.y_out))\\nMuch of the __init__ method is self-explanatory, but a few remarks may help clarify the\\ncode. As per usual, we randomly initialize the weights and biases as normal random variables\\nwith suitable standard deviations. The lines doing this look a little forbidding. However,\\nmost of the complication is just loading the weights and biases into what Theano calls shared\\nvariables. This ensures that these variables can be processed on the GPU, if one is available.\\nWe won’t get too much into the details of this. If you’re interested, you can dig into the\\nTheano documentation. Note also that this weight and bias initialization is designed for the\\nsigmoid activation function (as discussed earlier). Ideally, we’d initialize the weights and\\nbiases somewhat differently for activation functions such as the tanh and rectiﬁed linear\\nfunction. This is discussed further in problems below. The __init__ method ﬁnishes with\\nself.params = [self.w, self.b]. This is a handy way to bundle up all the learnable\\nparameters associated to the layer. Later on, the Network.SGD method will use params\\nattributes to ﬁgure out what variables in a Network instance can learn.\\nThe set_inpt method is used to set the input to the layer, and to compute the corre-\\nsponding output. I use the name inpt rather than input because input is a built-in function\\n20Note added November 2016: several readers have noted that in the line initializing self.w, I set\\nscale=np.sqrt(1.0/n_out), when the arguments of Chapter 3 suggest a better initialization may be\\nscale=np.sqrt(1.0/n_in). This was simply a mistake on my part. In an ideal world I’d rerun all the\\nexamples in this chapter with the correct code. Still, I’ve moved on to other projects, so am going to let\\nthe error go.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7bf02553-7763-4b98-9218-58f7b1a3e48d', embedding=None, metadata={'page_label': '187', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 187\\nin Python, and messing with built-ins tends to cause unpredictable behavior and difﬁcult-to-\\ndiagnose bugs. Note that we actually set the input in two separate ways: as self.inpt and\\nself.inpt_dropout. This is done because during training we may want to use dropout. If\\nthat’s the case then we want to remove a fractionself.p_dropout of the neurons. That’s\\nwhat the function dropout_layer in the second-last line of the set_inpt method is do-\\ning. So self.inpt_dropout and self.output_dropout are used during training, while\\nself.inpt and self.output are used for all other purposes, e.g., evaluating accuracy on the\\nvalidation and test data.\\nThe ConvPoolLayer and SoftmaxLayer class deﬁnitions are similar toFullyConnectedLayer\\n. Indeed, they’re so close that I won’t excerpt the code here. If you’re interested you can look\\nat the full listing for network3.py, later in this section.\\nHowever, a couple of minor differences of detail are worth mentioning. Most obviously ,\\nin both ConvPoolLayer and SoftmaxLayer we compute the output activations in the way\\nappropriate to that layer type. Fortunately, Theano makes that easy, providing built-in\\noperations to compute convolutions, max-pooling, and the softmax function.\\nLess obviously, when we introduced the softmax layer, we never discussed how to\\ninitialize the weights and biases. Elsewhere we’ve argued that for sigmoid layers we should\\ninitialize the weights using suitably parameterized normal random variables. But that\\nheuristic argument was speciﬁc to sigmoid neurons (and, with some amendment, to tanh\\nneurons). However, there’s no particular reason the argument should apply to softmax layers.\\nSo there’s no a priori reason to apply that initialization again. Rather than do that, I shall\\ninitialize all the weights and biases to be 0. This is a rather ad hoc procedure, but works\\nwell enough in practice.\\nOkay , we’ve looked at all the layer classes. What about theNetwork class? Let’s start by\\nlooking at the __init__ method:\\nclass Network( object ):\\ndef __init__(self , layers , mini_batch_size):\\n\"\"\"Takes a list of ‘layers ‘, describing the network architecture , and\\na value for the ‘mini_batch_size ‘ to be used during training\\nby stochastic gradient descent.\\n\"\"\"\\nself.layers = layers\\nself.mini_batch_size = mini_batch_size\\nself.params = [param for layer in self.layers for param in layer.params]\\nself.x = T.matrix(\"x\")\\nself.y = T.ivector(\"y\")\\ninit_layer = self.layers[0]\\ninit_layer.set_inpt(self.x, self.x, self.mini_batch_size)\\nfor j in xrange (1, len (self.layers)):\\nprev_layer , layer = self.layers[j-1], self.layers[j]\\nlayer.set_inpt(\\nprev_layer.output , prev_layer.output_dropout , self.mini_batch_size)\\nself.output = self.layers[-1].output\\nself.output_dropout = self.layers[-1].output_dropout\\nMost of this is self-explanatory , or nearly so. The lineself.params = [param for layer\\nin ...] bundles up the parameters for each layer into a single list. As anticipated above,\\nthe Network.SGD method will use self.params to ﬁgure out what variables in the Network\\ncan learn. The lines self.x = T.matrix(\"x\") and self.y = T.ivector(\"y\") deﬁne\\nTheano symbolic variables named x and y. These will be used to represent the input and\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1970a266-b3a9-400d-b403-6b2d73c0dd41', embedding=None, metadata={'page_label': '188', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='188\\n\\x0c\\x0c\\x0c Deep learning\\ndesired output from the network.\\nNow, this isn’t a Theano tutorial, and so we won’t get too deeply into what it means\\nthat these are symbolic variables21. But the rough idea is that these represent mathematical\\nvariables, not explicit values. We can do all the usual things one would do with such variables:\\nadd, subtract, and multiply them, apply functions, and so on. Indeed, Theano provides\\nmany ways of manipulating such symbolic variables, doing things like convolutions, max-\\npooling, and so on. But the big win is the ability to do fast symbolic differentiation, using a\\nvery general form of the backpropagation algorithm. This is extremely useful for applying\\nstochastic gradient descent to a wide variety of network architectures. In particular, the next\\nfew lines of code deﬁne symbolic outputs from the network. We start by setting the input to\\nthe initial layer, with the line\\ninit_layer.set_inpt(self.x, self.x, self.mini_batch_size)\\nNote that the inputs are set one mini-batch at a time, which is why the mini-batch size is\\nthere. Note also that we pass the input self.x in twice: this is because we may use the\\nnetwork in two different ways (with or without dropout). The for loop then propagates the\\nsymbolic variable self.x forward through the layers of the Network. This allows us to deﬁne\\nthe ﬁnal output and output_dropout attributes, which symbolically represent the output\\nfrom the Network.\\nNow that we’ve understood how aNetwork is initialized, let’s look at how it is trained,\\nusing the SGD method. The code looks lengthy, but its structure is actually rather simple.\\nExplanatory comments after the code.\\ndef SGD(self , training_data , epochs , mini_batch_size , eta ,\\nvalidation_data , test_data , lmbda =0.0):\\n\"\"\"Train the network using mini -batch stochastic gradient descent.\"\"\"\\ntraining_x , training_y = training_data\\nvalidation_x , validation_y = validation_data\\ntest_x , test_y = test_data\\n# compute number of minibatches for training , validation and testing\\nnum_training_batches = size(training_data)/mini_batch_size\\nnum_validation_batches = size(validation_data)/mini_batch_size\\nnum_test_batches = size(test_data)/mini_batch_size\\n# define the (regularized) cost function , symbolic gradients , and updates\\nl2_norm_squared = sum ([(layer.w**2). sum () for layer in self.layers])\\ncost = self.layers[-1].cost(self)+\\\\\\n0.5*lmbda*l2_norm_squared/num_training_batches\\ngrads = T.grad(cost , self.params)\\nupdates = [(param , param -eta*grad)\\nfor param , grad in zip (self.params , grads)]\\n# define functions to train a mini -batch , and to compute the\\n# accuracy in validation and test mini -batches.\\ni = T.lscalar() # mini -batch index\\ntrain_mb = theano.function(\\n[i], cost , updates=updates ,\\ngivens={\\nself.x:\\ntraining_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\n21The Theano documentation provides a good introduction to Theano. And if you get stuck, you may\\nﬁnd it helpful to look at one of the other tutorials available online. For instance, this tutorial covers\\nmany basics.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c1aa4867-843d-4ed1-967c-8e1f5ffb2a66', embedding=None, metadata={'page_label': '189', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 189\\nself.y:\\ntraining_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\nvalidate_mb_accuracy = theano.function(\\n[i], self.layers[-1].accuracy(self.y),\\ngivens={\\nself.x:\\nvalidation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\nself.y:\\nvalidation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\ntest_mb_accuracy = theano.function(\\n[i], self.layers[-1].accuracy(self.y),\\ngivens={\\nself.x:\\ntest_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\nself.y:\\ntest_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\nself.test_mb_predictions = theano.function(\\n[i], self.layers[-1].y_out ,\\ngivens={\\nself.x:\\ntest_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\n# Do the actual training\\nbest_validation_accuracy = 0.0\\nfor epoch in xrange (epochs):\\nfor minibatch_index in xrange (num_training_batches):\\niteration = num_training_batches*epoch+minibatch_index\\nif iteration % 1000 == 0:\\nprint (\"Training mini -batch number {0}\". format (iteration))\\ncost_ij = train_mb(minibatch_index)\\nif (iteration+1) % num_training_batches == 0:\\nvalidation_accuracy = np.mean(\\n[validate_mb_accuracy(j) for j in xrange (num_validation_batches)])\\nprint (\"Epoch {0}: validation accuracy {1:.2%}\". format (\\nepoch , validation_accuracy))\\nif validation_accuracy >= best_validation_accuracy:\\nprint (\"This is the best validation accuracy to date.\")\\nbest_validation_accuracy = validation_accuracy\\nbest_iteration = iteration\\nif test_data:\\ntest_accuracy = np.mean(\\n[test_mb_accuracy(j) for j in xrange (num_test_batches)])\\nprint (’The corresponding test accuracy is {0:.2%}’. format (\\ntest_accuracy))\\nprint (\"Finished training network.\")\\nprint (\"Best validation accuracy of {0:.2%} obtained at iteration {1}\". format (\\nbest_validation_accuracy , best_iteration))\\nprint (\"Corresponding test accuracy of {0:.2%}\". format (test_accuracy))\\nThe ﬁrst few lines are straightforward, separating the datasets into x and y components,\\nand computing the number of mini-batches used in each dataset. The next few lines are\\nmore interesting, and show some of what makes Theano fun to work with. Let’s explicitly\\nexcerpt the lines here:\\n# define the (regularized) cost function , symbolic gradients , and updates\\nl2_norm_squared = sum ([(layer.w**2). sum () for layer in self.layers])\\ncost = self.layers[-1].cost(self)+\\\\\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='bc96cafb-bdbd-4891-b2b5-c6fd98607dcd', embedding=None, metadata={'page_label': '190', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='190\\n\\x0c\\x0c\\x0c Deep learning\\n0.5*lmbda*l2_norm_squared/num_training_batches\\ngrads = T.grad(cost , self.params)\\nupdates = [(param , param -eta*grad) for param , grad in zip (self.params , grads)]\\nIn these lines we symbolically set up the regularized log-likelihood cost function, compute the\\ncorresponding derivatives in the gradient function, as well as the corresponding parameter\\nupdates. Theano lets us achieve all of this in just these few lines. The only thing hidden is\\nthat computing the cost involves a call to the cost method for the output layer; that code is\\nelsewhere in network3.py. But that code is short and simple, anyway . With all these things\\ndeﬁned, the stage is set to deﬁne the train_mb function, a Theano symbolic function which\\nuses the updates to update the Network parameters, given a mini-batch index. Similarly,\\nvalidate_mb_accuracy and test_mb_accuracy compute the accuracy of the Network on\\nany given mini-batch of validation or test data. By averaging over these functions, we will\\nbe able to compute accuracies on the entire validation and test data sets.\\nThe remainder of the SGD method is self-explanatory – we simply iterate over the\\nepochs, repeatedly training the network on mini-batches of training data, and computing\\nthe validation and test accuracies.\\nOkay , we’ve now understood the most important pieces of code innetwork3.py. Let’s\\ntake a brief look at the entire program. You don’t need to read through this in detail, but you\\nmay enjoy glancing over it, and perhaps diving down into any pieces that strike your fancy .\\nThe best way to really understand it is, of course, by modifying it, adding extra features, or\\nrefactoring anything you think could be done more elegantly . After the code, there are some\\nproblems which contain a few starter suggestions for things to do. Here’s the code22:\\n\"\"\"network3.py\\n~~~~~~~~~~~~~~\\nA Theano -based program for training and running simple neural\\nnetworks.\\nSupports several layer types (fully connected , convolutional , max\\npooling , softmax), and activation functions (sigmoid , tanh , and\\nrectified linear units , with more easily added).\\nWhen run on a CPU , this program is much faster than network.py and\\nnetwork2.py. However , unlike network.py and network2.py it can also\\nbe run on a GPU , which makes it faster still.\\nBecause the code is based on Theano , the code is different in many\\nways from network.py and network2.py. However , where possible I have\\ntried to maintain consistency with the earlier programs. In\\nparticular , the API is similar to network2.py. Note that I have\\nfocused on making the code simple , easily readable , and easily\\nmodifiable. It is not optimized , and omits many desirable features.\\nThis program incorporates ideas from the Theano documentation on\\nconvolutional neural nets (notably ,\\nhttp:// deeplearning.net/tutorial/lenet.html ), from Misha Denil’s\\nimplementation of dropout (https://github.com/mdenil/dropout ), and\\nfrom Chris Olah (http://colah.github.io ).\\n22Using Theano on a GPU can be a little tricky . In particular, it’s easy to make the mistake of pulling\\ndata off the GPU, which can slow things down a lot. I’ve tried to avoid this. With that said, this code\\ncan certainly be sped up quite a bit further with careful optimization of Theano’s conﬁguration. See the\\nTheano documentation for more details.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2154071a-00c1-485b-9ff3-2c3ae80e28d2', embedding=None, metadata={'page_label': '191', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 191\\nWritten for Theano 0.6 and 0.7, needs some changes for more recent\\nversions of Theano.\\n\"\"\"\\n#### Libraries\\n# Standard library\\nimport cPickle\\nimport gzip\\n# Third -party libraries\\nimport numpy as np\\nimport theano\\nimport theano.tensor as T\\nfrom theano.tensor.nnet import conv\\nfrom theano.tensor.nnet import softmax\\nfrom theano.tensor import shared_randomstreams\\nfrom theano.tensor.signal import downsample\\n# Activation functions for neurons\\ndef linear(z): return z\\ndef ReLU(z): return T.maximum(0.0, z)\\nfrom theano.tensor.nnet import sigmoid\\nfrom theano.tensor import tanh\\n#### Constants\\nGPU = True\\nif GPU:\\nprint \"Trying to run under a GPU. If this is not desired , then modify \"+\\\\\\n\"network3.py\\\\nto set the GPU flag to False.\"\\ntry : theano.config.device = ’gpu’\\nexcept : pass # it’s already set\\ntheano.config.floatX = ’float32’\\nelse :\\nprint \"Running with a CPU. If this is not desired , then the modify \"+\\\\\\n\"network3.py to set\\\\nthe GPU flag to True.\"\\n#### Load the MNIST data\\ndef load_data_shared(filename=\"../data/mnist.pkl.gz\"):\\nf = gzip. open (filename , ’rb’)\\ntraining_data , validation_data , test_data = cPickle.load(f)\\nf.close()\\ndef shared(data):\\n\"\"\"Place the data into shared variables. This allows Theano to copy\\nthe data to the GPU , if one is available.\\n\"\"\"\\nshared_x = theano.shared(\\nnp.asarray(data[0], dtype=theano.config.floatX), borrow=True)\\nshared_y = theano.shared(\\nnp.asarray(data[1], dtype=theano.config.floatX), borrow=True)\\nreturn shared_x , T.cast(shared_y , \"int32\")\\nreturn [shared(training_data), shared(validation_data), shared(test_data)]\\n#### Main class used to construct and train networks\\nclass Network( object ):\\ndef __init__(self , layers , mini_batch_size):\\n\"\"\"Takes a list of ‘layers ‘, describing the network architecture , and\\na value for the ‘mini_batch_size ‘ to be used during training\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0bc9fb18-bd45-4661-9b63-aa6d31c4d3be', embedding=None, metadata={'page_label': '192', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='192\\n\\x0c\\x0c\\x0c Deep learning\\nby stochastic gradient descent.\\n\"\"\"\\nself.layers = layers\\nself.mini_batch_size = mini_batch_size\\nself.params = [param for layer in self.layers for param in layer.params]\\nself.x = T.matrix(\"x\")\\nself.y = T.ivector(\"y\")\\ninit_layer = self.layers[0]\\ninit_layer.set_inpt(self.x, self.x, self.mini_batch_size)\\nfor j in xrange (1, len (self.layers)):\\nprev_layer , layer = self.layers[j-1], self.layers[j]\\nlayer.set_inpt(\\nprev_layer.output , prev_layer.output_dropout , self.mini_batch_size)\\nself.output = self.layers[-1].output\\nself.output_dropout = self.layers[-1].output_dropout\\ndef SGD(self , training_data , epochs , mini_batch_size , eta ,\\nvalidation_data , test_data , lmbda =0.0):\\n\"\"\"Train the network using mini -batch stochastic gradient descent.\"\"\"\\ntraining_x , training_y = training_data\\nvalidation_x , validation_y = validation_data\\ntest_x , test_y = test_data\\n# compute number of minibatches for training , validation and testing\\nnum_training_batches = size(training_data)/mini_batch_size\\nnum_validation_batches = size(validation_data)/mini_batch_size\\nnum_test_batches = size(test_data)/mini_batch_size\\n# define the (regularized) cost function , symbolic gradients , and updates\\nl2_norm_squared = sum ([(layer.w**2). sum () for layer in self.layers])\\ncost = self.layers[-1].cost(self)+\\\\\\n0.5*lmbda*l2_norm_squared/num_training_batches\\ngrads = T.grad(cost , self.params)\\nupdates = [(param , param -eta*grad)\\nfor param , grad in zip (self.params , grads)]\\n# define functions to train a mini -batch , and to compute the\\n# accuracy in validation and test mini -batches.\\ni = T.lscalar() # mini -batch index\\ntrain_mb = theano.function(\\n[i], cost , updates=updates ,\\ngivens={\\nself.x:\\ntraining_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\nself.y:\\ntraining_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\nvalidate_mb_accuracy = theano.function(\\n[i], self.layers[-1].accuracy(self.y),\\ngivens={\\nself.x:\\nvalidation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\nself.y:\\nvalidation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\ntest_mb_accuracy = theano.function(\\n[i], self.layers[-1].accuracy(self.y),\\ngivens={\\nself.x:\\ntest_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1a71a9aa-2245-444b-9824-867a999eda4a', embedding=None, metadata={'page_label': '193', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 193\\nself.y:\\ntest_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\nself.test_mb_predictions = theano.function(\\n[i], self.layers[-1].y_out ,\\ngivens={\\nself.x:\\ntest_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\\n})\\n# Do the actual training\\nbest_validation_accuracy = 0.0\\nfor epoch in xrange (epochs):\\nfor minibatch_index in xrange (num_training_batches):\\niteration = num_training_batches*epoch+minibatch_index\\nif iteration % 1000 == 0:\\nprint (\"Training mini -batch number {0}\". format (iteration))\\ncost_ij = train_mb(minibatch_index)\\nif (iteration+1) % num_training_batches == 0:\\nvalidation_accuracy = np.mean(\\n[validate_mb_accuracy(j) for j in xrange (num_validation_batches)])\\nprint (\"Epoch {0}: validation accuracy {1:.2%}\". format (\\nepoch , validation_accuracy))\\nif validation_accuracy >= best_validation_accuracy:\\nprint (\"This is the best validation accuracy to date.\")\\nbest_validation_accuracy = validation_accuracy\\nbest_iteration = iteration\\nif test_data:\\ntest_accuracy = np.mean(\\n[test_mb_accuracy(j) for j in xrange (num_test_batches)])\\nprint (’The corresponding test accuracy is {0:.2%}’. format (\\ntest_accuracy))\\nprint (\"Finished training network.\")\\nprint (\"Best validation accuracy of {0:.2%} obtained at iteration {1}\". format (\\nbest_validation_accuracy , best_iteration))\\nprint (\"Corresponding test accuracy of {0:.2%}\". format (test_accuracy))\\n#### Define layer types\\nclass ConvPoolLayer( object ):\\n\"\"\"Used to create a combination of a convolutional and a max -pooling\\nlayer. A more sophisticated implementation would separate the\\ntwo , but for our purposes we’ll always use them together , and it\\nsimplifies the code , so it makes sense to combine them.\\n\"\"\"\\ndef __init__(self , filter_shape , image_shape , poolsize=(2, 2),\\nactivation_fn=sigmoid):\\n\"\"\"‘filter_shape ‘ is a tuple of length 4, whose entries are the number\\nof filters , the number of input feature maps , the filter height , and the\\nfilter width.\\n‘image_shape ‘ is a tuple of length 4, whose entries are the\\nmini -batch size , the number of input feature maps , the image\\nheight , and the image width.\\n‘poolsize ‘ is a tuple of length 2, whose entries are the y and\\nx pooling sizes.\\n\"\"\"\\nself.filter_shape = filter_shape\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d4ba8ce-31ee-4327-b445-f960f158d0c1', embedding=None, metadata={'page_label': '194', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='194\\n\\x0c\\x0c\\x0c Deep learning\\nself.image_shape = image_shape\\nself.poolsize = poolsize\\nself.activation_fn=activation_fn\\n# initialize weights and biases\\nn_out = (filter_shape [0]*np.prod(filter_shape [2:])/np.prod(poolsize))\\nself.w = theano.shared(\\nnp.asarray(\\nnp.random.normal(loc=0, scale=np.sqrt(1.0/ n_out), size=filter_shape),\\ndtype=theano.config.floatX),\\nborrow=True)\\nself.b = theano.shared(\\nnp.asarray(\\nnp.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\\ndtype=theano.config.floatX),\\nborrow=True)\\nself.params = [self.w, self.b]\\ndef set_inpt(self , inpt , inpt_dropout , mini_batch_size):\\nself.inpt = inpt.reshape(self.image_shape)\\nconv_out = conv.conv2d(\\ninput =self.inpt , filters=self.w, filter_shape=self.filter_shape ,\\nimage_shape=self.image_shape)\\npooled_out = downsample.max_pool_2d(\\ninput =conv_out , ds=self.poolsize , ignore_border=True)\\nself.output = self.activation_fn(\\npooled_out + self.b.dimshuffle(’x’, 0, ’x’, ’x’))\\nself.output_dropout = self.output # no dropout in the convolutional layers\\nclass FullyConnectedLayer( object ):\\ndef __init__(self , n_in , n_out , activation_fn=sigmoid , p_dropout =0.0):\\nself.n_in = n_in\\nself.n_out = n_out\\nself.activation_fn = activation_fn\\nself.p_dropout = p_dropout\\n# Initialize weights and biases\\nself.w = theano.shared(\\nnp.asarray(\\nnp.random.normal(\\nloc=0.0, scale=np.sqrt(1.0/ n_out), size=(n_in , n_out)),\\ndtype=theano.config.floatX),\\nname=’w’, borrow=True)\\nself.b = theano.shared(\\nnp.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out ,)),\\ndtype=theano.config.floatX),\\nname=’b’, borrow=True)\\nself.params = [self.w, self.b]\\ndef set_inpt(self , inpt , inpt_dropout , mini_batch_size):\\nself.inpt = inpt.reshape((mini_batch_size , self.n_in))\\nself.output = self.activation_fn(\\n(1-self.p_dropout)*T.dot(self.inpt , self.w) + self.b)\\nself.y_out = T.argmax(self.output , axis=1)\\nself.inpt_dropout = dropout_layer(\\ninpt_dropout.reshape((mini_batch_size , self.n_in)), self.p_dropout)\\nself.output_dropout = self.activation_fn(\\nT.dot(self.inpt_dropout , self.w) + self.b)\\ndef accuracy(self , y):\\n\"Return the accuracy for the mini -batch.\"\\nreturn T.mean(T.eq(y, self.y_out))\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1b184df4-8d8d-4275-9eca-9ca883eadc8f', embedding=None, metadata={'page_label': '195', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.3. The code for our convolutional networks\\n\\x0c\\x0c\\x0c 195\\nclass SoftmaxLayer( object ):\\ndef __init__(self , n_in , n_out , p_dropout =0.0):\\nself.n_in = n_in\\nself.n_out = n_out\\nself.p_dropout = p_dropout\\n# Initialize weights and biases\\nself.w = theano.shared(\\nnp.zeros((n_in , n_out), dtype=theano.config.floatX),\\nname=’w’, borrow=True)\\nself.b = theano.shared(\\nnp.zeros((n_out ,), dtype=theano.config.floatX),\\nname=’b’, borrow=True)\\nself.params = [self.w, self.b]\\ndef set_inpt(self , inpt , inpt_dropout , mini_batch_size):\\nself.inpt = inpt.reshape((mini_batch_size , self.n_in))\\nself.output = softmax((1-self.p_dropout)*T.dot(self.inpt , self.w) + self.b)\\nself.y_out = T.argmax(self.output , axis=1)\\nself.inpt_dropout = dropout_layer(\\ninpt_dropout.reshape((mini_batch_size , self.n_in)), self.p_dropout)\\nself.output_dropout = softmax(T.dot(self.inpt_dropout , self.w) + self.b)\\ndef cost(self , net):\\n\"Return the log -likelihood cost.\"\\nreturn -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\\ndef accuracy(self , y):\\n\"Return the accuracy for the mini -batch.\"\\nreturn T.mean(T.eq(y, self.y_out))\\n#### Miscellanea\\ndef size(data):\\n\"Return the size of the dataset ‘data ‘.\"\\nreturn data[0]. get_value(borrow=True).shape[0]\\ndef dropout_layer(layer , p_dropout):\\nsrng = shared_randomstreams.RandomStreams(\\nnp.random.RandomState (0).randint (999999))\\nmask = srng.binomial(n=1, p=1-p_dropout , size=layer.shape)\\nreturn layer*T.cast(mask , theano.config.floatX)\\nProblems\\n• At present, the SGD method requires the user to manually choose the number of\\nepochs to train for. Earlier in the book we discussed an automated way of selecting\\nthe number of epochs to train for, known as early stopping. Modify network3.py to\\nimplement early stopping.\\n• Add a Network method to return the accuracy on an arbitrary data set.\\n• Modify the SGD method to allow the learning rate ηto be a function of the epoch\\nnumber. Hint: After working on this problem for a while, you may ﬁnd it useful to see\\nthe discussion at this link.\\n• Earlier in the chapter I described a technique for expanding the training data by apply-\\ning (small) rotations, skewing, and translation. Modify network3.py to incorporate\\nall these techniques. Note: Unless you have a tremendous amount of memory, it is\\nnot practical to explicitly generate the entire expanded data set. So you should consider\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='aa1630c1-7866-471c-a88a-80fefe5ec0f3', embedding=None, metadata={'page_label': '196', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='196\\n\\x0c\\x0c\\x0c Deep learning\\nalternate approaches.\\n• Add the ability to load and save networks to network3.py.\\n• A shortcoming of the current code is that it provides few diagnostic tools. Can you\\nthink of any diagnostics to add that would make it easier to understand to what extent\\na network is overﬁtting? Add them.\\n• We’ve used the same initialization procedure for rectiﬁed linear units as for sigmoid\\n(and tanh) neurons. Our argument for that initialization was speciﬁc to the sigmoid\\nfunction. Consider a network made entirely of rectiﬁed linear units (including outputs).\\nShow that rescaling all the weights in the network by a constant factor c >0 simply\\nrescales the outputs by a factor cL−1, where L is the number of layers. How does\\nthis change if the ﬁnal layer is a softmax? What do you think of using the sigmoid\\ninitialization procedure for the rectiﬁed linear units? Can you think of a better\\ninitialization procedure? Note: This is a very open-ended problem, not something\\nwith a simple self-contained answer. Still, considering the problem will help you better\\nunderstand networks containing rectiﬁed linear units.\\n• Our analysis of the unstable gradient problem was for sigmoid neurons. How does\\nthe analysis change for networks made up of rectiﬁed linear units? Can you think of a\\ngood way of modifying such a network so it doesn’t suffer from the unstable gradient\\nproblem? Note: The word good in the second part of this makes the problem a research\\nproblem. It’s actually easy to think of ways of making such modiﬁcations. But I haven’t\\ninvestigated in enough depth to know of a really good technique.\\n6.4 Recent progress in image recognition\\nIn 1998, the year MNIST was introduced, it took weeks to train a state-of-the-art workstation\\nto achieve accuracies substantially worse than those we can achieve using a GPU and less\\nthan an hour of training. Thus, MNIST is no longer a problem that pushes the limits of\\navailable technique; rather, the speed of training means that it is a problem good for teaching\\nand learning purposes. Meanwhile, the focus of research has moved on, and modern work\\ninvolves much more challenging image recognition problems. In this section, I brieﬂy describe\\nsome recent work on image recognition using neural networks.\\nThe section is different to most of the book. Through the book I’ve focused on ideas likely\\nto be of lasting interest – ideas such as backpropagation, regularization, and convolutional\\nnetworks. I’ve tried to avoid results which are fashionable as I write, but whose long-term\\nvalue is unknown. In science, such results are more often than not ephemera which fade and\\nhave little lasting impact. Given this, a skeptic might say: “well, surely the recent progress\\nin image recognition is an example of such ephemera? In another two or three years, things\\nwill have moved on. So surely these results are only of interest to a few specialists who want\\nto compete at the absolute frontier? Why bother discussing it?”\\nSuch a skeptic is right that some of the ﬁner details of recent papers will gradually\\ndiminish in perceived importance. With that said, the past few years have seen extraordinary\\nimprovements using deep nets to attack extremely difﬁcult image recognition tasks. Imagine\\na historian of science writing about computer vision in the year 2100. They will identify the\\nyears 2011 to 2015 (and probably a few years beyond) as a time of huge breakthroughs,\\ndriven by deep convolutional nets. That doesn’t mean deep convolutional nets will still be\\nused in 2100, much less detailed ideas such as dropout, rectiﬁed linear units, and so on. But\\nit does mean that an important transition is taking place, right now, in the history of ideas.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1ce01b45-5802-4039-946b-0f604edec0e7', embedding=None, metadata={'page_label': '197', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4. Recent progress in image recognition\\n\\x0c\\x0c\\x0c 197\\nIt’s a bit like watching the discovery of the atom, or the invention of antibiotics: invention\\nand discovery on a historic scale. And so while we won’t dig down deep into details, it’s\\nworth getting some idea of the exciting discoveries currently being made.\\nThe 2012 LRMD paper: Let me start with a 2012 paper23 from a group of researchers\\nfrom Stanford and Google. I’ll refer to this paper as LRMD, after the last names of the\\nﬁrst four authors. LRMD used a neural network to classify images from ImageNet, a very\\nchallenging image recognition problem. The 2011 ImageNet data that they used included\\n16 million full color images, in 20 thousand categories. The images were crawled from the\\nopen net, and classiﬁed by workers from Amazon’s Mechanical Turk service. Here’s a few\\nImageNet images24:\\nThese are, respectively , in the categories for beading plane, brown root rot fungus, scalded\\nmilk, and the common roundworm. If you’re looking for a challenge, I encourage you to\\nvisit ImageNet’s list of hand tools, which distinguishes between beading planes, block planes,\\nchamfer planes, and about a dozen other types of plane, amongst other categories. I don’t\\nknow about you, but I cannot conﬁdently distinguish between all these tool types. This is\\nobviously a much more challenging image recognition task than MNIST! LRMD’s network\\nobtained a respectable 15.8 percent accuracy for correctly classifying ImageNet images. That\\nmay not sound impressive, but it was a huge improvement over the previous best result\\nof 9.3 percent accuracy . That jump suggested that neural networks might offer a powerful\\napproach to very challenging image recognition tasks, such as ImageNet.\\nThe 2012 KSH paper: The work of LRMD was followed by a 2012 paper of Krizhevsky ,\\nSutskever and Hinton (KSH)25. KSH trained and tested a deep convolutional neural network\\nusing a restricted subset of the ImageNet data. The subset they used came from a popular\\nmachine learning competition – the ImageNet Large-Scale Visual Recognition Challenge\\n(ILSVRC). Using a competition dataset gave them a good way of comparing their approach\\nto other leading techniques. The ILSVRC-2012 training set contained about 1.2 million\\nImageNet images, drawn from 1,000 categories. The validation and test sets contained\\n50,000 and 150,000 images, respectively , drawn from the same 1,000 categories.\\nOne difﬁculty in running the ILSVRC competition is that many ImageNet images contain\\nmultiple objects. Suppose an image shows a labrador retriever chasing a soccer ball. The\\nso-called “correct” ImageNet classiﬁcation of the image might be as a labrador retriever.\\n23Building high-level features using large scale unsupervised learning, by Quoc Le, Marc’Aurelio\\nRanzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng (2012).\\nNote that the detailed architecture of the network used in the paper differed in many details from the\\ndeep convolutional networks we’ve been studying. Broadly speaking, however, LRMD is based on many\\nsimilar ideas.\\n24These are from the 2014 dataset, which is somewhat changed from 2011. Qualitatively, however,\\nthe dataset is extremely similar. Details about ImageNet are available in the original ImageNet paper,\\nImageNet: a large-scale hierarchical image database, by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,\\nKai Li, and Li Fei-Fei (2009).\\n25ImageNet classiﬁcation with deep convolutional neural networks, by Alex Krizhevsky , Ilya Sutskever,\\nand Geoffrey E. Hinton (2012).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b5050f9f-d2fb-48bc-95a4-bf2ab41193ea', embedding=None, metadata={'page_label': '198', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='198\\n\\x0c\\x0c\\x0c Deep learning\\nShould an algorithm be penalized if it labels the image as a soccer ball? Because of this\\nambiguity, an algorithm was considered correct if the actual ImageNet classiﬁcation was\\namong the 5 classiﬁcations the algorithm considered most likely. By this top-5 criterion,\\nKSH’s deep convolutional network achieved an accuracy of 84.7 percent, vastly better than\\nthe next-best contest entry, which achieved an accuracy of 73.8 percent. Using the more\\nrestrictive metric of getting the label exactly right, KSH’s network achieved an accuracy of\\n63.3 percent.\\nIt’s worth brieﬂy describing KSH’s network, since it has inspired much subsequent work.\\nIt’s also, as we shall see, closely related to the networks we trained earlier in this chapter,\\nalbeit more elaborate. KSH used a deep convolutional neural network, trained on two GPUs.\\nThey used two GPUs because the particular type of GPU they were using (an NVIDIA GeForce\\nGTX 580) didn’t have enough on-chip memory to store their entire network. So they split\\nthe network into two parts, partitioned across the two GPUs.\\nThe KSH network has 7 layers of hidden neurons. The ﬁrst 5 hidden layers are convolu-\\ntional layers (some with max-pooling), while the next 2 layers are fully-connected layers.\\nThe output layer is a 1,000-unit softmax layer, corresponding to the 1,000 image classes.\\nHere’s a sketch of the network, taken from the KSH paper26. The details are explained below.\\nNote that many layers are split into 2 parts, corresponding to the 2 GPUs.\\nThe input layer contains 3 ×224 ×224 neurons, representing the RGB values for a 224 ×224\\nimage. Recall that, as mentioned earlier, ImageNet contains images of varying resolution.\\nThis poses a problem, since a neural network’s input layer is usually of a ﬁxed size. KSH dealt\\nwith this by rescaling each image so the shorter side had length 256. They then cropped\\nout a 256 ×256 area in the center of the rescaled image. Finally, KSH extracted random\\n224 ×224 subimages (and horizontal reﬂections) from the 256 ×256 images. They did this\\nrandom cropping as a way of expanding the training data, and thus reducing overﬁtting.\\nThis is particularly helpful in a large network such as KSH’s. It was these224 ×224 images\\nwhich were used as inputs to the network. In most cases the cropped image still contains the\\nmain object from the uncropped image.\\nMoving on to the hidden layers in KSH’s network, the ﬁrst hidden layer is a convolutional\\nlayer, with a max-pooling step. It uses local receptive ﬁelds of size 11 ×11, and a stride\\nlength of 4 pixels. There are a total of 96 feature maps. The feature maps are split into two\\ngroups of 48 each, with the ﬁrst 48 feature maps residing on one GPU, and the second 48\\nfeature maps residing on the other GPU. The max-pooling in this and later layers is done in\\n3 ×3 regions, but the pooling regions are allowed to overlap, and are just 2 pixels apart.\\n26Thanks to Ilya Sutskever.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='49c999d4-83f5-4eef-abd4-86ca2d0dfc17', embedding=None, metadata={'page_label': '199', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4. Recent progress in image recognition\\n\\x0c\\x0c\\x0c 199\\nThe second hidden layer is also a convolutional layer, with a max-pooling step. It uses\\n5 ×5 local receptive ﬁelds, and there’s a total of 256 feature maps, split into 128 on each\\nGPU. Note that the feature maps only use 48 input channels, not the full 96 output from the\\nprevious layer (as would usually be the case). This is because any single feature map only\\nuses inputs from the same GPU. In this sense the network departs from the convolutional\\narchitecture we described earlier in the chapter, though obviously the basic idea is still the\\nsame.\\nThe third, fourth and ﬁfth hidden layers are convolutional layers, but unlike the previous\\nlayers, they do not involve max-pooling. Their respectives parameters are: (3) 384 feature\\nmaps, with 3 ×3 local receptive ﬁelds, and 256 input channels; (4) 384 feature maps, with\\n3×3 local receptive ﬁelds, and 192 input channels; and (5) 256 feature maps, with3×3 local\\nreceptive ﬁelds, and 192 input channels. Note that the third layer involves some inter-GPU\\ncommunication (as depicted in the ﬁgure) in order that the feature maps use all 256 input\\nchannels.\\nThe sixth and seventh hidden layers are fully-connected layers, with 4,096 neurons in\\neach layer.\\nThe output layer is a 1,000-unit softmax layer.\\nThe KSH network takes advantage of many techniques. Instead of using the sigmoid or\\ntanh activation functions, KSH use rectiﬁed linear units, which sped up training signiﬁcantly .\\nKSH’s network had roughly 60 million learned parameters, and was thus, even with the large\\ntraining set, susceptible to overﬁtting. To overcome this, they expanded the training set using\\nthe random cropping strategy we discussed above. They also further addressed overﬁtting\\nby using a variant of l2 regularization, and dropout. The network itself was trained using\\nmomentum-based mini-batch stochastic gradient descent.\\nThat’s an overview of many of the core ideas in the KSH paper. I’ve omitted some\\ndetails, for which you should look at the paper. You can also look at Alex Krizhevsky’s\\ncuda-convnet (and successors), which contains code implementing many of the ideas. A\\nTheano-based implementation has also been developed27, with the code available here. The\\ncode is recognizably along similar lines to that developed in this chapter, although the use of\\nmultiple GPUs complicates things somewhat. The Caffe neural nets framework also includes\\na version of the KSH network, see their Model Zoo for details.\\nThe 2014 ILSVRC competition: Since 2012, rapid progress continues to be made.\\nConsider the 2014 ILSVRC competition. As in 2012, it involved a training set of 1.2 million\\nimages, in 1,000 categories, and the ﬁgure of merit was whether the top 5 predictions\\nincluded the correct category . The winning team, based primarily at Google28, used a deep\\nconvolutional network with 22 layers of neurons. They called their network GoogLeNet,\\nas a homage to LeNet-5. GoogLeNet achieved a top-5 accuracy of 93.33 percent, a giant\\nimprovement over the 2013 winner (Clarifai, with 88.3 percent), and the 2012 winner (KSH,\\nwith 84.7 percent).\\nJust how good is GoogLeNet’s 93.33 percent accuracy? In 2014 a team of researchers\\nwrote a survey paper about the ILSVRC competition29. One of the questions they address is\\nhow well humans perform on ILSVRC. To do this, they built a system which lets humans\\n27Theano-based large-scale visual recognition with multiple GPUs, by Weiguang Ding, Ruoyan Wang,\\nFei Mao, and Graham Taylor (2014).\\n28Going deeper with convolutions, by Christian Szegedy , Wei Liu, Yangqing Jia, Pierre Sermanet, Scott\\nReed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich (2014).\\n29ImageNet large scale visual recognition challenge, by Olga Russakovsky , Jia Deng, Hao Su, Jonathan\\nKrause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy , Aditya Khosla, Michael Bernstein,\\nAlexander C. Berg, and Li Fei-Fei (2014).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2078f30b-fc02-410e-9b6a-94ecdbfdb2c9', embedding=None, metadata={'page_label': '200', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='200\\n\\x0c\\x0c\\x0c Deep learning\\nclassify ILSVRC images. As one of the authors, Andrej Karpathy , explains in an informative\\nblog post, it was a lot of trouble to get the humans up to GoogLeNet’s performance:\\n...the task of labeling images with 5 out of 1000 categories quickly turned\\nout to be extremely challenging, even for some friends in the lab who have\\nbeen working on ILSVRC and its classes for a while. First we thought\\nwe would put it up on [Amazon Mechanical Turk]. Then we thought\\nwe could recruit paid undergrads. Then I organized a labeling party of\\nintense labeling effort only among the (expert labelers) in our lab. Then I\\ndeveloped a modiﬁed interface that used GoogLeNet predictions to prune\\nthe number of categories from 1000 to only about 100. It was still too\\nhard – people kept missing categories and getting up to ranges of 13–15%\\nerror rates. In the end I realized that to get anywhere competitively close\\nto GoogLeNet, it was most efﬁcient if I sat down and went through the\\npainfully long training process and the subsequent careful annotation\\nprocess myself... The labeling happened at a rate of about 1 per minute,\\nbut this decreased over time... Some images are easily recognized, while\\nsome images (such as those of ﬁne-grained breeds of dogs, birds, or\\nmonkeys) can require multiple minutes of concentrated effort. I became\\nvery good at identifying breeds of dogs... Based on the sample of images\\nI worked on, the GoogLeNet classiﬁcation error turned out to be 6.8%...\\nMy own error in the end turned out to be 5.1%, approximately 1.7%\\nbetter.\\nIn other words, an expert human, working painstakingly, was with great effort able to\\nnarrowly beat the deep neural network. In fact, Karpathy reports that a second human\\nexpert, trained on a smaller sample of images, was only able to attain a 12.0 percent top-5\\nerror rate, signiﬁcantly below GoogLeNet’s performance. About half the errors were due to\\nthe expert “failing to spot and consider the ground truth label as an option”.\\nThese are astonishing results. Indeed, since this work, several teams have reported\\nsystems whose top-5 error rate is actually better than 5.1%. This has sometimes been\\nreported in the media as the systems having better-than-human vision. While the results are\\ngenuinely exciting, there are many caveats that make it misleading to think of the systems\\nas having better-than-human vision. The ILSVRC challenge is in many ways a rather limited\\nproblem – a crawl of the open web is not necessarily representative of images found in\\napplications! And, of course, the top-5 criterion is quite artiﬁcial. We are still a long way\\nfrom solving the problem of image recognition or, more broadly , computer vision. Still, it’s\\nextremely encouraging to see so much progress made on such a challenging problem, over\\njust a few years.\\nOther activity: I’ve focused on ImageNet, but there’s a considerable amount of other\\nactivity using neural nets to do image recognition. Let me brieﬂy describe a few interesting\\nrecent results, just to give the ﬂavour of some current work.\\nOne encouraging practical set of results comes from a team at Google, who applied deep\\nconvolutional networks to the problem of recognizing street numbers in Google’s Street View\\nimagery30. In their paper, they report detecting and automatically transcribing nearly 100\\nmillion street numbers at an accuracy similar to that of a human operator. The system is fast:\\ntheir system transcribed all of Street View’s images of street numbers in France in less than\\n30Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Net-\\nworks, by Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet (2013).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='3d0fb148-197d-4741-8eb6-ede458920c9f', embedding=None, metadata={'page_label': '201', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4. Recent progress in image recognition\\n\\x0c\\x0c\\x0c 201\\nan hour! They say: “Having this new dataset signiﬁcantly increased the geocoding quality of\\nGoogle Maps in several countries especially the ones that did not already have other sources\\nof good geocoding.” And they go on to make the broader claim: “We believe with this model\\nwe have solved [optical character recognition] for short sequences [of characters] for many\\napplications.”\\nI’ve perhaps given the impression that it’s all a parade of encouraging results. Of course,\\nsome of the most interesting work reports on fundamental things we don’t yet understand.\\nFor instance, a 2013 paper31 showed that deep networks may suffer from what are effectively\\nblind spots. Consider the lines of images below. On the left is an ImageNet image classiﬁed\\ncorrectly by their network. On the right is a slightly perturbed image (the perturbation is in\\nthe middle) which is classiﬁed incorrectly by the network. The authors found that there are\\nsuch “adversarial” images for every sample image, not just a few special ones.\\nThis is a disturbing result. The paper used a network based on the same code as KSH’s network\\n– that is, just the type of network that is being increasingly widely used. While such neural\\nnetworks compute functions which are, in principle, continuous, results like this suggest that\\nin practice they’re likely to compute functions which are very nearly discontinuous. Worse,\\nthey’ll be discontinuous in ways that violate our intuition about what is reasonable behavior.\\nThat’s concerning. Furthermore, it’s not yet well understood what’s causing the discontinuity:\\nis it something about the loss function? The activation functions used? The architecture of\\nthe network? Something else? We don’t yet know.\\nNow, these results are not quite as bad as they sound. Although such adversarial images\\nare common, they’re also unlikely in practice. As the paper notes:\\nThe existence of the adversarial negatives appears to be in contradiction\\nwith the network’s ability to achieve high generalization performance.\\nIndeed, if the network can generalize well, how can it be confused by\\nthese adversarial negatives, which are indistinguishable from the regular\\nexamples? The explanation is that the set of adversarial negatives is of\\nextremely low probability, and thus is never (or rarely) observed in the\\n31Intriguing properties of neural networks, by Christian Szegedy , Wojciech Zaremba, Ilya Sutskever,\\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus (2013)\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7ad87c4a-bcdb-4d95-adc0-3d9acd53eaad', embedding=None, metadata={'page_label': '202', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='202\\n\\x0c\\x0c\\x0c Deep learning\\ntest set, yet it is dense (much like the rational numbers), and so it is found\\nnear virtually every test case.\\nNonetheless, it is distressing that we understand neural nets so poorly that this kind of\\nresult should be a recent discovery. Of course, a major beneﬁt of the results is that they\\nhave stimulated much followup work. For example, one recent paper 32 shows that given\\na trained network it’s possible to generate images which look to a human like white noise,\\nbut which the network classiﬁes as being in a known category with a very high degree of\\nconﬁdence. This is another demonstration that we have a long way to go in understanding\\nneural networks and their use in image recognition.\\nDespite results like this, the overall picture is encouraging. We’re seeing rapid progress\\non extremely difﬁcult benchmarks, like ImageNet. We’re also seeing rapid progress in the\\nsolution of real-world problems, like recognizing street numbers in StreetView. But while this\\nis encouraging it’s not enough just to see improvements on benchmarks, or even real-world\\napplications. There are fundamental phenomena which we still understand poorly, such\\nas the existence of adversarial images. When such fundamental problems are still being\\ndiscovered (never mind solved), it is premature to say that we’re near solving the problem of\\nimage recognition. At the same time such problems are an exciting stimulus to further work.\\n6.5 Other approaches to deep neural nets\\nThrough this book, we’ve concentrated on a single problem: classifying the MNIST digits.\\nIt’s a juicy problem which forced us to understand many powerful ideas: stochastic gradient\\ndescent, backpropagation, convolutional nets, regularization, and more. But it’s also a narrow\\nproblem. If you read the neural networks literature, you’ll run into many ideas we haven’t\\ndiscussed: recurrent neural networks, Boltzmann machines, generative models, transfer\\nlearning, reinforcement learning, and so on, on and on â˘A˛ e and on! Neural networks is a\\nvast ﬁeld. However, many important ideas are variations on ideas we’ve already discussed,\\nand can be understood with a little effort. In this section I provide a glimpse of these as yet\\nunseen vistas. The discussion isn’t detailed, nor comprehensive – that would greatly expand\\nthe book. Rather, it’s impressionistic, an attempt to evoke the conceptual richness of the\\nﬁeld, and to relate some of those riches to what we’ve already seen. Through the section, I’ll\\nprovide a few links to other sources, as entrees to learn more. Of course, many of these links\\nwill soon be superseded, and you may wish to search out more recent literature. That point\\nnotwithstanding, I expect many of the underlying ideas to be of lasting interest.\\nRecurrent neural networks (RNNs): In the feedforward nets we’ve been using there\\nis a single input which completely determines the activations of all the neurons through\\nthe remaining layers. It’s a very static picture: everything in the network is ﬁxed, with a\\nfrozen, crystalline quality to it. But suppose we allow the elements in the network to keep\\nchanging in a dynamic way . For instance, the behaviour of hidden neurons might not just be\\ndetermined by the activations in previous hidden layers, but also by the activations at earlier\\ntimes. Indeed, a neuron’s activation might be determined in part by its own activation at an\\nearlier time. That’s certainly not what happens in a feedforward network. Or perhaps the\\nactivations of hidden and output neurons won’t be determined just by the current input to\\nthe network, but also by earlier inputs.\\n32Deep Neural Networks are Easily Fooled: High Conﬁdence Predictions for Unrecognizable Images,\\nby Anh Nguyen, Jason Yosinski, and Jeff Clune (2014).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c3338fea-861c-4ce4-982b-ec506600d243', embedding=None, metadata={'page_label': '203', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.5. Other approaches to deep neural nets\\n\\x0c\\x0c\\x0c 203\\nNeural networks with this kind of time-varying behaviour are known as recurrent neural\\nnetworks or RNNs. There are many different ways of mathematically formalizing the informal\\ndescription of recurrent nets given in the last paragraph. You can get the ﬂavour of some of\\nthese mathematical models by glancing at the Wikipedia article on RNNs. As I write, that\\npage lists no fewer than 13 different models. But mathematical details aside, the broad idea\\nis that RNNs are neural networks in which there is some notion of dynamic change over time.\\nAnd, not surprisingly , they’re particularly useful in analysing data or processes that change\\nover time. Such data and processes arise naturally in problems such as speech or natural\\nlanguage, for example.\\nOne way RNNs are currently being used is to connect neural networks more closely to\\ntraditional ways of thinking about algorithms, ways of thinking based on concepts such\\nas Turing machines and (conventional) programming languages. A 2014 paper developed\\nan RNN which could take as input a character-by-character description of a (very, very\\nsimple!) Python program, and use that description to predict the output. Informally, the\\nnetwork is learning to “understand” certain Python programs. A second paper, also from\\n2014, used RNNs as a starting point to develop what they called a neural Turing machine\\n(NTM). This is a universal computer whose entire structure can be trained using gradient\\ndescent. They trained their NTM to infer algorithms for several simple problems, such as\\nsorting and copying.\\nAs it stands, these are extremely simple toy models. Learning to execute the Python pro-\\ngram print(398345+42598) doesn’t make a network into a full-ﬂedged Python interpreter!\\nIt’s not clear how much further it will be possible to push the ideas. Still, the results are\\nintriguing. Historically, neural networks have done well at pattern recognition problems\\nwhere conventional algorithmic approaches have trouble. Vice versa, conventional algorith-\\nmic approaches are good at solving problems that neural nets aren’t so good at. No-one\\ntoday implements a web server or a database program using a neural network! It’d be great\\nto develop uniﬁed models that integrate the strengths of both neural networks and more\\ntraditional approaches to algorithms. RNNs and ideas inspired by RNNs may help us do that.\\nRNNs have also been used in recent years to attack many other problems. They’ve been\\nparticularly useful in speech recognition. Approaches based on RNNs have, for example,\\nset records for the accuracy of phoneme recognition. They’ve also been used to develop\\nimproved models of the language people use while speaking. Better language models help\\ndisambiguate utterances that otherwise sound alike. A good language model will, for example,\\ntell us that “to inﬁnity and beyond” is much more likely than “two inﬁnity and beyond”,\\ndespite the fact that the phrases sound identical. RNNs have been used to set new records\\nfor certain language benchmarks.\\nThis work is, incidentally , part of a broader use of deep neural nets of all types, not just\\nRNNs, in speech recognition. For example, an approach based on deep nets has achieved\\noutstanding results on large vocabulary continuous speech recognition. And another system\\nbased on deep nets has been deployed in Google’s Android operating system (for related\\ntechnical work, see Vincent Vanhoucke’s 2012–2015 papers).\\nI’ve said a little about what RNNs can do, but not so much about how they work. It\\nperhaps won’t surprise you to learn that many of the ideas used in feedforward networks can\\nalso be used in RNNs. In particular, we can train RNNs using straightforward modiﬁcations to\\ngradient descent and backpropagation. Many other ideas used in feedforward nets, ranging\\nfrom regularization techniques to convolutions to the activation and cost functions used, are\\nalso useful in recurrent nets. And so many of the techniques we’ve developed in the book\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a2434565-44b3-4e2c-8a32-377df317c6d0', embedding=None, metadata={'page_label': '204', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='204\\n\\x0c\\x0c\\x0c Deep learning\\ncan be adapted for use with RNNs.\\nLong short-term memory units (LSTMs): One challenge affecting RNNs is that early\\nmodels turned out to be very difﬁcult to train, harder even than deep feedforward networks.\\nThe reason is the unstable gradient problem discussed in Chapter 5. Recall that the usual\\nmanifestation of this problem is that the gradient gets smaller and smaller as it is propagated\\nback through layers. This makes learning in early layers extremely slow. The problem\\nactually gets worse in RNNs, since gradients aren’t just propagated backward through layers,\\nthey’re propagated backward through time. If the network runs for a long time that can\\nmake the gradient extremely unstable and hard to learn from. Fortunately, it’s possible to\\nincorporate an idea known as long short-term memory units (LSTMs) into RNNs. The units\\nwere introduced by Hochreiter and Schmidhuber in 1997 with the explicit purpose of helping\\naddress the unstable gradient problem. LSTMs make it much easier to get good results when\\ntraining RNNs, and many recent papers (including many that I linked above) make use of\\nLSTMs or related ideas.\\nDeep belief nets, generative models, and Boltzmann machines: Modern interest in\\ndeep learning began in 2006, with papers explaining how to train a type of neural network\\nknown as a deep belief network (DBN)33. DBNs were inﬂuential for several years, but have\\nsince lessened in popularity , while models such as feedforward networks and recurrent neural\\nnets have become fashionable. Despite this, DBNs have several properties that make them\\ninteresting.\\nOne reason DBNs are interesting is that they’re an example of what’s called a generative\\nmodel. In a feedforward network, we specify the input activations, and they determine the\\nactivations of the feature neurons later in the network. A generative model like a DBN can\\nbe used in a similar way, but it’s also possible to specify the values of some of the feature\\nneurons and then “run the network backward”, generating values for the input activations.\\nMore concretely , a DBN trained on images of handwritten digits can (potentially , and with\\nsome care) also be used to generate images that look like handwritten digits. In other words,\\nthe DBN would in some sense be learning to write. In this, a generative model is much like\\nthe human brain: not only can it read digits, it can also write them. In Geoffrey Hinton’s\\nmemorable phrase, to recognize shapes, ﬁrst learn to generate images.\\nA second reason DBNs are interesting is that they can do unsupervised and semi-\\nsupervised learning. For instance, when trained with image data, DBNs can learn useful\\nfeatures for understanding other images, even if the training images are unlabelled. And the\\nability to do unsupervised learning is extremely interesting both for fundamental scientiﬁc\\nreasons, and – if it can be made to work well enough – for practical applications.\\nGiven these attractive features, why have DBNs lessened in popularity as models for\\ndeep learning? Part of the reason is that models such as feedforward and recurrent nets\\nhave achieved many spectacular results, such as their breakthroughs on image and speech\\nrecognition benchmarks. It’s not surprising and quite right that there’s now lots of attention\\nbeing paid to these models. There’s an unfortunate corollary, however. The marketplace\\nof ideas often functions in a winner-take-all fashion, with nearly all attention going to the\\ncurrent fashion-of-the-moment in any given area. It can become extremely difﬁcult for people\\nto work on momentarily unfashionable ideas, even when those ideas are obviously of real\\nlong-term interest. My personal opinion is that DBNs and other generative models likely\\ndeserve more attention than they are currently receiving. And I won’t be surprised if DBNs\\n33See A fast learning algorithm for deep belief nets, by Geoffrey Hinton, Simon Osindero, and Yee-Whye\\nTeh (2006), as well as the related work in Reducing the dimensionality of data with neural networks, by\\nGeoffrey Hinton and Ruslan Salakhutdinov (2006).\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d67f858-0d43-4fd7-89d4-dee6565c4985', embedding=None, metadata={'page_label': '205', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6. On the future of neural networks\\n\\x0c\\x0c\\x0c 205\\nor a related model one day surpass the currently fashionable models. For an introduction to\\nDBNs, see this overview. I’ve also found this article helpful. It isn’t primarily about deep\\nbelief nets, per se, but does contain much useful information about restricted Boltzmann\\nmachines, which are a key component of DBNs.\\nOther ideas: What else is going on in neural networks and deep learning? Well, there’s\\na huge amount of other fascinating work. Active areas of research include using neural\\nnetworks to do natural language processing (see also this informative review paper), machine\\ntranslation, as well as perhaps more surprising applications such as music informatics. There\\nare, of course, many other areas too. In many cases, having read this book you should be able\\nto begin following recent work, although (of course) you’ll need to ﬁll in gaps in presumed\\nbackground knowledge.\\nLet me ﬁnish this section by mentioning a particularly fun paper. It combines deep\\nconvolutional networks with a technique known as reinforcement learning in order to learn\\nto play video games well (see also this followup). The idea is to use the convolutional\\nnetwork to simplify the pixel data from the game screen, turning it into a simpler set of\\nfeatures, which can be used to decide which action to take: “go left”, “go down”, “ﬁre”, and\\nso on. What is particularly interesting is that a single network learned to play seven different\\nclassic video games pretty well, outperforming human experts on three of the games. Now,\\nthis all sounds like a stunt, and there’s no doubt the paper was well marketed, with the\\ntitle “Playing Atari with reinforcement learning”. But looking past the surface gloss, consider\\nthat this system is taking raw pixel data – it doesn’t even know the game rules! – and from\\nthat data learning to do high-quality decision-making in several very different and very\\nadversarial environments, each with its own complex set of rules. That’s pretty neat.\\n6.6 On the future of neural networks\\nIntention-driven user interfaces: There’s an old joke in which an impatient professor tells a\\nconfused student: “don’t listen to what I say; listen to what Imean”. Historically , computers\\nhave often been, like the confused student, in the dark about what their users mean. But this\\nis changing. I still remember my surprise the ﬁrst time I misspelled a Google search query , only\\nto have Google say “Did you mean[corrected query]?” and to offer the corresponding search\\nresults. Google CEO Larry Page once described the perfect search engine as understanding\\nexactly what [your queries] mean and giving you back exactly what you want.\\nThis is a vision of an intention-driven user interface. In this vision, instead of responding\\nto users’ literal queries, search will use machine learning to take vague user input, discern\\nprecisely what was meant, and take action on the basis of those insights.\\nThe idea of intention-driven interfaces can be applied far more broadly than search.\\nOver the next few decades, thousands of companies will build products which use machine\\nlearning to make user interfaces that can tolerate imprecision, while discerning and acting on\\nthe user’s true intent. We’re already seeing early examples of such intention-driven interfaces:\\nApple’s Siri; Wolfram Alpha; IBM’s Watson; systems which can annotate photos and videos;\\nand much more.\\nMost of these products will fail. Inspired user interface design is hard, and I expect\\nmany companies will take powerful machine learning technology and use it to build insipid\\nuser interfaces. The best machine learning in the world won’t help if your user interface\\nconcept stinks. But there will be a residue of products which succeed. Over time that will\\ncause a profound change in how we relate to computers. Not so long ago – let’s say, 2005\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7f732e64-d1f5-435d-b06d-102a275af310', embedding=None, metadata={'page_label': '206', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='206\\n\\x0c\\x0c\\x0c Deep learning\\n– users took it for granted that they needed precision in most interactions with computers.\\nIndeed, computer literacy to a great extent meant internalizing the idea that computers are\\nextremely literal; a single misplaced semi-colon may completely change the nature of an\\ninteraction with a computer. But over the next few decades I expect we’ll develop many\\nsuccessful intention-driven user interfaces, and that will dramatically change what we expect\\nwhen interacting with computers.\\nMachine learning, data science, and the virtuous circle of innovation: Of course,\\nmachine learning isn’t just being used to build intention-driven interfaces. Another notable\\napplication is in data science, where machine learning is used to ﬁnd the “known unknowns”\\nhidden in data. This is already a fashionable area, and much has been written about it, so\\nI won’t say much. But I do want to mention one consequence of this fashion that is not\\nso often remarked: over the long run it’s possible the biggest breakthrough in machine\\nlearning won’t be any single conceptual breakthrough. Rather, the biggest breakthrough will\\nbe that machine learning research becomes proﬁtable, through applications to data science\\nand other areas. If a company can invest 1 dollar in machine learning research and get 1\\ndollar and 10 cents back reasonably rapidly, then a lot of money will end up in machine\\nlearning research. Put another way, machine learning is an engine driving the creation of\\nseveral major new markets and areas of growth in technology . The result will be large teams\\nof people with deep subject expertise, and with access to extraordinary resources. That\\nwill propel machine learning further forward, creating more markets and opportunities, a\\nvirtuous circle of innovation.\\nThe role of neural networks and deep learning: I’ve been talking broadly about\\nmachine learning as a creator of new opportunities for technology . What will be the speciﬁc\\nrole of neural networks and deep learning in all this?\\nTo answer the question, it helps to look at history . Back in the 1980s there was a great\\ndeal of excitement and optimism about neural networks, especially after backpropagation\\nbecame widely known. That excitement faded, and in the 1990s the machine learning baton\\npassed to other techniques, such as support vector machines. Today, neural networks are\\nagain riding high, setting all sorts of records, defeating all comers on many problems. But\\nwho is to say that tomorrow some new approach won’t be developed that sweeps neural\\nnetworks away again? Or perhaps progress with neural networks will stagnate, and nothing\\nwill immediately arise to take their place?\\nFor this reason, it’s much easier to think broadly about the future of machine learning\\nthan about neural networks speciﬁcally. Part of the problem is that we understand neural\\nnetworks so poorly. Why is it that neural networks can generalize so well? How is it that\\nthey avoid overﬁtting as well as they do, given the very large number of parameters they\\nlearn? Why is it that stochastic gradient descent works as well as it does? How well will\\nneural networks perform as data sets are scaled? For instance, if ImageNet was expanded\\nby a factor of 10, would neural networks’ performance improve more or less than other\\nmachine learning techniques? These are all simple, fundamental questions. And, at present,\\nwe understand the answers to these questions very poorly . While that’s the case, it’s difﬁcult\\nto say what role neural networks will play in the future of machine learning.\\nI will make one prediction: I believe deep learning is here to stay. The ability to learn\\nhierarchies of concepts, building up multiple layers of abstraction, seems to be fundamental\\nto making sense of the world. This doesn’t mean tomorrow’s deep learners won’t be radically\\ndifferent than today’s. We could see major changes in the constituent units used, in the\\narchitectures, or in the learning algorithms. Those changes may be dramatic enough that we\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='de7b2d4d-89ef-4b7a-9307-56be4cb12930', embedding=None, metadata={'page_label': '207', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6. On the future of neural networks\\n\\x0c\\x0c\\x0c 207\\nno longer think of the resulting systems as neural networks. But they’d still be doing deep\\nlearning.\\nWill neural networks and deep learning soon lead to artiﬁcial intelligence? In this\\nbook we’ve focused on using neural nets to do speciﬁc tasks, such as classifying images.\\nLet’s broaden our ambitions, and ask: what about general-purpose thinking computers?\\nCan neural networks and deep learning help us solve the problem of (general) artiﬁcial\\nintelligence (AI)? And, if so, given the rapid recent progress of deep learning, can we expect\\ngeneral AI any time soon?\\nAddressing these questions comprehensively would take a separate book. Instead, let\\nme offer one observation. It’s based on an idea known as Conway’s law:\\nAny organization that designs a system... will inevitably produce a design\\nwhose structure is a copy of the organization’s communication structure.\\nSo, for example, Conway’s law suggests that the design of a Boeing 747 aircraft will mirror\\nthe extended organizational structure of Boeing and its contractors at the time the 747 was\\ndesigned. Or for a simple, speciﬁc example, consider a company building a complex software\\napplication. If the application’s dashboard is supposed to be integrated with some machine\\nlearning algorithm, the person building the dashboard better be talking to the company’s\\nmachine learning expert. Conway’s law is merely that observation, writ large.\\nUpon ﬁrst hearing Conway’s law, many people respond either “Well, isn’t that banal\\nand obvious?” or “Isn’t that wrong?” Let me start with the objection that it’s wrong. As an\\ninstance of this objection, consider the question: where does Boeing’s accounting department\\nshow up in the design of the 747? What about their janitorial department? Their internal\\ncatering? And the answer is that these parts of the organization probably don’t show up\\nexplicitly anywhere in the 747. So we should understand Conway’s law as referring only to\\nthose parts of an organization concerned explicitly with design and engineering.\\nWhat about the other objection, that Conway’s law is banal and obvious? This may per-\\nhaps be true, but I don’t think so, for organizations too often act with disregard for Conway’s\\nlaw. Teams building new products are often bloated with legacy hires or, contrariwise, lack a\\nperson with some crucial expertise. Think of all the products which have useless complicating\\nfeatures. Or think of all the products which have obvious major deﬁciencies – e.g., a terrible\\nuser interface. Problems in both classes are often caused by a mismatch between the team\\nthat was needed to produce a good product, and the team that was actually assembled.\\nConway’s law may be obvious, but that doesn’t mean people don’t routinely ignore it.\\nConway’s law applies to the design and engineering of systems where we start out with\\na pretty good understanding of the likely constituent parts, and how to build them. It can’t\\nbe applied directly to the development of artiﬁcial intelligence, because AI isn’t (yet) such a\\nproblem: we don’t know what the constituent parts are. Indeed, we’re not even sure what\\nbasic questions to be asking. In others words, at this point AI is more a problem of science\\nthan of engineering. Imagine beginning the design of the 747 without knowing about jet\\nengines or the principles of aerodynamics. You wouldn’t know what kinds of experts to hire\\ninto your organization. As Wernher von Braun put it, “basic research is what I’m doing when\\nI don’t know what I’m doing”. Is there a version of Conway’s law that applies to problems\\nwhich are more science than engineering?\\nTo gain insight into this question, consider the history of medicine. In the early days,\\nmedicine was the domain of practitioners like Galen and Hippocrates, who studied the\\nentire body. But as our knowledge grew, people were forced to specialize. We discovered\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='23ba502d-02b4-416b-a0e0-607cc9969a56', embedding=None, metadata={'page_label': '208', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='208\\n\\x0c\\x0c\\x0c Deep learning\\nmany deep new ideas34: think of things like the germ theory of disease, for instance, or the\\nunderstanding of how antibodies work, or the understanding that the heart, lungs, veins\\nand arteries form a complete cardiovascular system. Such deep insights formed the basis for\\nsubﬁelds such as epidemiology , immunology , and the cluster of inter-linked ﬁelds around the\\ncardiovascular system. And so the structure of our knowledge has shaped the social structure\\nof medicine. This is particularly striking in the case of immunology: realizing the immune\\nsystem exists and is a system worthy of study is an extremely non-trivial insight. So we have\\nan entire ﬁeld of medicine – with specialists, conferences, even prizes, and so on – organized\\naround something which is not just invisible, it’s arguably not a distinct thing at all.\\nThis is a common pattern that has been repeated in many well-established sciences:\\nnot just medicine, but physics, mathematics, chemistry, and others. The ﬁelds start out\\nmonolithic, with just a few deep ideas. Early experts can master all those ideas. But as time\\npasses that monolithic character changes. We discover many deep new ideas, too many for\\nany one person to really master. As a result, the social structure of the ﬁeld re-organizes\\nand divides around those ideas. Instead of a monolith, we have ﬁelds within ﬁelds within\\nﬁelds, a complex, recursive, self-referential social structure, whose organization mirrors the\\nconnections between our deepest insights. And so the structure of our knowledge shapes the\\nsocial organization of science. But that social shape in turn constrains and helps determine\\nwhat we can discover. This is the scientiﬁc analogue of Conway’s law.\\nSo what’s this got to do with deep learning or AI?\\nWell, since the early days of AI there have been arguments about it that go, on one side,\\n“Hey , it’s not going to be so hard, we’ve got[super-special weapon] on our side”, countered by\\n“[super-special weapon] won’t be enough”. Deep learning is the latest super-special weapon\\nI’ve heard used in such arguments35; earlier versions of the argument used logic, or Prolog,\\nor expert systems, or whatever the most powerful technique of the day was. The problem\\nwith such arguments is that they don’t give you any good way of saying just how powerful\\nany given candidate super-special weapon is. Of course, we’ve just spent a chapter reviewing\\nevidence that deep learning can solve extremely challenging problems. It certainly looks very\\nexciting and promising. But that was also true of systems like Prolog or Eurisko or expert\\nsystems in their day. And so the mere fact that a set of ideas looks very promising doesn’t\\nmean much. How can we tell if deep learning is truly different from these earlier ideas? Is\\nthere some way of measuring how powerful and promising a set of ideas is? Conway’s law\\nsuggests that as a rough and heuristic proxy metric we can evaluate the complexity of the\\nsocial structure associated to those ideas.\\nSo, there are two questions to ask. First, how powerful a set of ideas are associated to\\ndeep learning, according to this metric of social complexity? Second, how powerful a theory\\nwill we need, in order to be able to build a general artiﬁcial intelligence?\\nAs to the ﬁrst question: when we look at deep learning today, it’s an exciting and fast-\\npaced but also relatively monolithic ﬁeld. There are a few deep ideas, and a few main\\nconferences, with substantial overlap between several of the conferences. And there is paper\\nafter paper leveraging the same basic set of ideas: using stochastic gradient descent (or a\\nclose variation) to optimize a cost function. It’s fantastic those ideas are so successful. But\\n34My apologies for overloading “deep”. I won’t deﬁne “deep ideas” precisely , but loosely I mean the\\nkind of idea which is the basis for a rich ﬁeld of enquiry . The backpropagation algorithm and the germ\\ntheory of disease are both good examples.\\n35Interestingly, often not by leading experts in deep learning, who have been quite restrained. See,\\nfor example, this thoughtful post by Yann LeCun. This is a difference from many earlier incarnations of\\nthe argument.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='4ffd47b0-c80e-48cf-ba17-80657c9fd4c8', embedding=None, metadata={'page_label': '209', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.6. On the future of neural networks\\n\\x0c\\x0c\\x0c 209\\nwhat we don’t yet see is lots of well-developed subﬁelds, each exploring their own sets of\\ndeep ideas, pushing deep learning in many directions. And so, according to the metric of\\nsocial complexity , deep learning is, if you’ll forgive the play on words, still a rather shallow\\nﬁeld. It’s still possible for one person to master most of the deepest ideas in the ﬁeld.\\nOn the second question: how complex and powerful a set of ideas will be needed\\nto obtain AI? Of course, the answer to this question is: no-one knows for sure. But in\\nthe appendix I examine some of the existing evidence on this question. I conclude that,\\neven rather optimistically, it’s going to take many, many deep ideas to build an AI. And so\\nConway’s law suggests that to get to such a point we will necessarily see the emergence\\nof many interrelating disciplines, with a complex and surprising structure mirroring the\\nstructure in our deepest insights. We don’t yet see this rich social structure in the use of\\nneural networks and deep learning. And so, I believe that we are several decades (at least)\\nfrom using deep learning to develop general AI.\\nI’ve gone to a lot of trouble to construct an argument which is tentative, perhaps seems\\nrather obvious, and which has an indeﬁnite conclusion. This will no doubt frustrate people\\nwho crave certainty. Reading around online, I see many people who loudly assert very\\ndeﬁnite, very strongly held opinions about AI, often on the basis of ﬂimsy reasoning and\\nnon-existent evidence. My frank opinion is this: it’s too early to say . As the old joke goes, if\\nyou ask a scientist how far away some discovery is and they say “10 years” (or more), what\\nthey mean is “I’ve got no idea”. AI, like controlled fusion and a few other technologies, has\\nbeen 10 years away for 60 plus years. On the ﬂipside, what we deﬁnitely do have in deep\\nlearning is a powerful technique whose limits have not yet been found, and many wide-open\\nfundamental problems. That’s an exciting creative opportunity .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='8ab653d0-4712-49e1-9784-9b8030b48355', embedding=None, metadata={'page_label': '210', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='210\\n\\x0c\\x0c\\x0c Deep learning', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='964d687d-904e-4055-9de2-5b0161c80e4c', embedding=None, metadata={'page_label': '211', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 211\\nAAAAA\\nIs there a simple algorithm for\\nintelligence?\\nIn this book, we’ve focused on the nuts and bolts of neural networks: how they work, and\\nhow they can be used to solve pattern recognition problems. This is material with many\\nimmediate practical applications. But, of course, one reason for interest in neural nets is the\\nhope that one day they will go far beyond such basic pattern recognition problems. Perhaps\\nthey, or some other approach based on digital computers, will eventually be used to build\\nthinking machines, machines that match or surpass human intelligence? This notion far\\nexceeds the material discussed in the book – or what anyone in the world knows how to do.\\nBut it’s fun to speculate.\\nThere has been much debate about whether it’s even possible for computers to match\\nhuman intelligence. I’m not going to engage with that question. Despite ongoing dispute, I\\nbelieve it’s not in serious doubt that an intelligent computer is possible – although it may be\\nextremely complicated, and perhaps far beyond current technology – and current naysayers\\nwill one day seem much like the vitalists.\\nRather, the question I explore here is whether there is a simple set of principles which\\ncan be used to explain intelligence? In particular, and more concretely, is there a simple\\nalgorithm for intelligence?\\nThe idea that there is a truly simple algorithm for intelligence is a bold idea. It perhaps\\nsounds too optimistic to be true. Many people have a strong intuitive sense that intelligence\\nhas considerable irreducible complexity. They’re so impressed by the amazing variety and\\nﬂexibility of human thought that they conclude that a simple algorithm for intelligence must\\nbe impossible. Despite this intuition, I don’t think it’s wise to rush to judgement. The history\\nof science is ﬁlled with instances where a phenomenon initially appeared extremely complex,\\nbut was later explained by some simple but powerful set of ideas.\\nConsider, for example, the early days of astronomy . Humans have known since ancient\\ntimes that there is a menagerie of objects in the sky: the sun, the moon, the planets, the\\ncomets, and the stars. These objects behave in very different ways – stars move in a stately ,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b4767147-0700-4ba3-a0f0-0f311a6b6b58', embedding=None, metadata={'page_label': '212', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='212\\n\\x0c\\x0c\\x0c Is there a simple algorithm for intelligence?\\nregular way across the sky, for example, while comets appear as if out of nowhere, streak\\nacross the sky, and then disappear. In the 16th century only a foolish optimist could have\\nimagined that all these objects’ motions could be explained by a simple set of principles.\\nBut in the 17th century Newton formulated his theory of universal gravitation, which not\\nonly explained all these motions, but also explained terrestrial phenomena such as the tides\\nand the behaviour of Earth-bound projecticles. The 16th century’s foolish optimist seems in\\nretrospect like a pessimist, asking for too little.\\nOf course, science contains many more such examples. Consider the myriad chemical\\nsubstances making up our world, so beautifully explained by Mendeleev’s periodic table,\\nwhich is, in turn, explained by a few simple rules which may be obtained from quantum\\nmechanics. Or the puzzle of how there is so much complexity and diversity in the biological\\nworld, whose origin turns out to lie in the principle of evolution by natural selection. These\\nand many other examples suggest that it would not be wise to rule out a simple explanation\\nof intelligence merely on the grounds that what our brains – currently the best examples of\\nintelligence – are doing appears to be very complicated1.\\nContrariwise, and despite these optimistic examples, it is also logically possible that\\nintelligence can only be explained by a large number of fundamentally distinct mechanisms.\\nIn the case of our brains, those many mechanisms may perhaps have evolved in response\\nto many different selection pressures in our species’ evolutionary history. If this point of\\nview is correct, then intelligence involves considerable irreducible complexity , and no simple\\nalgorithm for intelligence is possible.\\nWhich of these two points of view is correct?\\nTo get insight into this question, let’s ask a closely related question, which is whether\\nthere’s a simple explanation of how human brains work. In particular, let’s look at some ways\\nof quantifying the complexity of the brain. Our ﬁrst approach is the view of the brain from\\nconnectomics. This is all about the raw wiring: how many neurons there are in the brain,\\nhow many glial cells, and how many connections there are between the neurons. You’ve\\nprobably heard the numbers before – the brain contains on the order of 100 billion neurons,\\n100 billion glial cells, and 100 trillion connections between neurons. Those numbers are\\nstaggering. They’re also intimidating. If we need to understand the details of all those\\nconnections (not to mention the neurons and glial cells) in order to understand how the\\nbrain works, then we’re certainly not going to end up with a simple algorithm for intelligence.\\nThere’s a second, more optimistic point of view, the view of the brain from molecular\\nbiology . The idea is to ask how much genetic information is needed to describe the brain’s\\narchitecture. To get a handle on this question, we’ll start by considering the genetic differences\\nbetween humans and chimpanzees. You’ve probably heard the sound bite that “human beings\\nare 98 percent chimpanzee”. This saying is sometimes varied – popular variations also give\\nthe number as 95 or 99 percent. The variations occur because the numbers were originally\\nestimated by comparing samples of the human and chimp genomes, not the entire genomes.\\nHowever, in 2007 the entire chimpanzee genome was sequenced (see also here), and we\\nnow know that human and chimp DNA differ at roughly 125 million DNA base pairs. That’s\\nout of a total of roughly 3 billion DNA base pairs in each genome. So it’s not right to say\\nhuman beings are 98 percent chimpanzee – we’re more like 96 percent chimpanzee.\\n1Through this appendix I assume that for a computer to be considered intelligent its capabilities must\\nmatch or exceed human thinking ability . And so I’ll regard the question “Is there a simple algorithm for\\nintelligence?” as equivalent to “Is there a simple algorithm which can ‘think’ along essentially the same\\nlines as the human brain?” It’s worth noting, however, that there may well be forms of intelligence that\\ndon’t subsume human thought, but nonetheless go beyond it in interesting ways.\\nA', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7211c3a3-7612-47c6-9295-f88e9d7f81dc', embedding=None, metadata={'page_label': '213', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 213\\nHow much information is in that 125 million base pairs? Each base pair can be labelled\\nby one of four possibilities – the “letters” of the genetic code, the bases adenine, cytosine,\\nguanine, and thymine. So each base pair can be described using two bits of information\\n– just enough information to specify one of the four labels. So 125 million base pairs is\\nequivalent to 250 million bits of information. That’s the genetic difference between humans\\nand chimps!\\nOf course, that 250 million bits accounts for all the genetic differences between humans\\nand chimps. We’re only interested in the difference associated to the brain. Unfortunately,\\nno-one knows what fraction of the total genetic difference is needed to explain the difference\\nbetween the brains. But let’s assume for the sake of argument that about half that 250\\nmillion bits accounts for the brain differences. That’s a total of 125 million bits.\\n125 million bits is an impressively large number. Let’s get a sense for how large it is\\nby translating it into more human terms. In particular, how much would be an equivalent\\namount of English text? It turns out that the information content of English text is about\\n1 bit per letter. That sounds low – after all, the alphabet has 26 letters – but there is a\\ntremendous amount of redundancy in English text. Of course, you might argue that our\\ngenomes are redundant, too, so two bits per base pair is an overestimate. But we’ll ignore\\nthat, since at worst it means that we’re overestimating our brain’s genetic complexity . With\\nthese assumptions, we see that the genetic difference between our brains and chimp brains\\nis equivalent to about 125 million letters, or about 25 million English words. That’s about\\n30 times as much as the King James Bible.\\nThat’s a lot of information. But it’s not incomprehensibly large. It’s on a human scale.\\nMaybe no single human could ever understand all that’s written in that code, but a group\\nof people could perhaps understand it collectively , through appropriate specialization. And\\nalthough it’s a lot of information, it’s minuscule when compared to the information required\\nto describe the 100 billion neurons, 100 billion glial cells, and 100 trillion connections in\\nour brains. Even if we use a simple, coarse description – say , 10 ﬂoating point numbers to\\ncharacterize each connection – that would require about 70 quadrillion bits. That means the\\ngenetic description is a factor of about half a billion less complex than the full connectome\\nfor the human brain.\\nWhat we learn from this is that our genome cannot possibly contain a detailed description\\nof all our neural connections. Rather, it must specify just the broad architecture and basic\\nprinciples underlying the brain. But that architecture and those principles seem to be enough\\nto guarantee that we humans will grow up to be intelligent. Of course, there are caveats\\n– growing children need a healthy, stimulating environment and good nutrition to achieve\\ntheir intellectual potential. But provided we grow up in a reasonable environment, a healthy\\nhuman will have remarkable intelligence. In some sense, the information in our genes\\ncontains the essence of how we think. And furthermore, the principles contained in that\\ngenetic information seem likely to be within our ability to collectively grasp.\\nAll the numbers above are very rough estimates. It’s possible that 125 million bits is\\na tremendous overestimate, that there is some much more compact set of core principles\\nunderlying human thought. Maybe most of that 125 million bits is just ﬁne-tuning of relatively\\nminor details. Or maybe we were overly conservative in how we computed the numbers.\\nObviously , that’d be great if it were true! For our current purposes, the key point is this: the\\narchitecture of the brain is complicated, but it’s not nearly as complicated as you might think\\nbased on the number of connections in the brain. The view of the brain from molecular\\nbiology suggests we humans ought to one day be able to understand the basic principles\\nA', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a35011e8-1e5c-4057-9a0c-ed690463e08b', embedding=None, metadata={'page_label': '214', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='214\\n\\x0c\\x0c\\x0c Is there a simple algorithm for intelligence?\\nbehind the brain’s architecture.\\nIn the last few paragraphs I’ve ignored the fact that 125 million bits merely quantiﬁes\\nthe genetic difference between human and chimp brains. Not all our brain function is\\ndue to those 125 million bits. Chimps are remarkable thinkers in their own right. Maybe\\nthe key to intelligence lies mostly in the mental abilities (and genetic information) that\\nchimps and humans have in common. If this is correct, then human brains might be just a\\nminor upgrade to chimpanzee brains, at least in terms of the complexity of the underlying\\nprinciples. Despite the conventional human chauvinism about our unique capabilities, this\\nisn’t inconceivable: the chimpanzee and human genetic lines diverged just 5 million years\\nago, a blink in evolutionary timescales. However, in the absence of a more compelling\\nargument, I’m sympathetic to the conventional human chauvinism: my guess is that the\\nmost interesting principles underlying human thought lie in that 125 million bits, not in the\\npart of the genome we share with chimpanzees.\\nAdopting the view of the brain from molecular biology gave us a reduction of roughly\\nnine orders of magnitude in the complexity of our description. While encouraging, it doesn’t\\ntell us whether or not a truly simple algorithm for intelligence is possible. Can we get any\\nfurther reductions in complexity? And, more to the point, can we settle the question of\\nwhether a simple algorithm for intelligence is possible?\\nUnfortunately , there isn’t yet any evidence strong enough to decisively settle this ques-\\ntion. Let me describe some of the available evidence, with the caveat that this is a very\\nbrief and incomplete overview, meant to convey the ﬂavour of some recent work, not to\\ncomprehensively survey what is known.\\nAmong the evidence suggesting that there may be a simple algorithm for intelligence\\nis an experiment reported in April 2000 in the journal Nature. A team of scientists led by\\nMriganka Sur “rewired” the brains of newborn ferrets. Usually, the signal from a ferret’s\\neyes is transmitted to a part of the brain known as the visual cortex. But for these ferrets\\nthe scientists took the signal from the eyes and rerouted it so it instead went to the auditory\\ncortex, i.e, the brain region that’s usually used for hearing.\\nTo understand what happened when they did this, we need to know a bit about the visual\\ncortex. The visual cortex contains many orientation columns. These are little slabs of neurons,\\neach of which responds to visual stimuli from some particular direction. You can think of the\\norientation columns as tiny directional sensors: when someone shines a bright light from\\nsome particular direction, a corresponding orientation column is activated. If the light is\\nmoved, a different orientation column is activated. One of the most important high-level\\nstructures in the visual cortex is the orientation map, which charts how the orientation\\ncolumns are laid out.\\nWhat the scientists found is that when the visual signal from the ferrets’ eyes was rerouted\\nto the auditory cortex, the auditory cortex changed. Orientation columns and an orientation\\nmap began to emerge in the auditory cortex. It was more disorderly than the orientation map\\nusually found in the visual cortex, but unmistakably similar. Furthermore, the scientists did\\nsome simple tests of how the ferrets responded to visual stimuli, training them to respond\\ndifferently when lights ﬂashed from different directions. These tests suggested that the\\nferrets could still learn to “see”, at least in a rudimentary fashion, using the auditory cortex.\\nThis is an astonishing result. It suggests that there are common principles underlying\\nhow different parts of the brain learn to respond to sensory data. That commonality pro-\\nvides at least some support for the idea that there is a set of simple principles underlying\\nintelligence. However, we shouldn’t kid ourselves about how good the ferrets’ vision was in\\nA', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7d0870e9-bc7f-4ab0-9fb9-6d69dd1cbc51', embedding=None, metadata={'page_label': '215', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='\\x0c\\x0c\\x0c 215\\nthese experiments. The behavioural tests tested only very gross aspects of vision. And, of\\ncourse, we can’t ask the ferrets if they’ve “learned to see”. So the experiments don’t prove\\nthat the rewired auditory cortex was giving the ferrets a high-ﬁdelity visual experience. And\\nso they provide only limited evidence in favour of the idea that common principles underlie\\nhow different parts of the brain learn.\\nWhat evidence is there against the idea of a simple algorithm for intelligence? Some\\nevidence comes from the ﬁelds of evolutionary psychology and neuroanatomy. Since the\\n1960s evolutionary psychologists have discovered a wide range of human universals, complex\\nbehaviours common to all humans, across cultures and upbringing. These human universals\\ninclude the incest taboo between mother and son, the use of music and dance, as well\\nas much complex linguistic structure, such as the use of swear words (i.e., taboo words),\\npronouns, and even structures as basic as the verb. Complementing these results, a great\\ndeal of evidence from neuroanatomy shows that many human behaviours are controlled\\nby particular localized areas of the brain, and those areas seem to be similar in all people.\\nTaken together, these ﬁndings suggest that many very specialized behaviours are hardwired\\ninto particular parts of our brains.\\nSome people conclude from these results that separate explanations must be required for\\nthese many brain functions, and that as a consequence there is an irreducible complexity to\\nthe brain’s function, a complexity that makes a simple explanation for the brain’s operation\\n(and, perhaps, a simple algorithm for intelligence) impossible. For example, one well-\\nknown artiﬁcial intelligence researcher with this point of view is Marvin Minsky. In the\\n1970s and 1980s Minsky developed his “Society of Mind” theory, based on the idea that\\nhuman intelligence is the result of a large society of individually simple (but very different)\\ncomputational processes which Minsky calls agents. In his book describing the theory, Minsky\\nsums up what he sees as the power of this point of view:\\nWhat magical trick makes us intelligent? The trick is that there is no trick. The power\\nof intelligence stems from our vast diversity, not from any single, perfect principle. In a\\nresponse 2 to reviews of his book, Minsky elaborated on the motivation for the Society\\nof Mind, giving an argument similar to that stated above, based on neuroanatomy and\\nevolutionary psychology:\\nWe now know that the brain itself is composed of hundreds of different\\nregions and nuclei, each with signiﬁcantly different architectural elements\\nand arrangements, and that many of them are involved with demonstrably\\ndifferent aspects of our mental activities. This modern mass of knowledge\\nshows that many phenomena traditionally described by commonsense\\nterms like “intelligence” or “understanding” actually involve complex\\nassemblies of machinery .\\nMinsky is, of course, not the only person to hold a point of view along these lines; I’m\\nmerely giving him as an example of a supporter of this line of argument. I ﬁnd the argument\\ninteresting, but don’t believe the evidence is compelling. While it’s true that the brain\\nis composed of a large number of different regions, with different functions, it does not\\ntherefore follow that a simple explanation for the brain’s function is impossible. Perhaps\\nthose architectural differences arise out of common underlying principles, much as the\\nmotion of comets, the planets, the sun and the stars all arise from a single gravitational force.\\nNeither Minsky nor anyone else has argued convincingly against such underlying principles.\\n2In Contemplating Minds: A Forum for Artiﬁcial Intelligence, edited by William J. Clancey , Stephen\\nW . Smoliar, and Mark Steﬁk (MIT Press, 1994).\\nA', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='7fa83b42-c83a-4796-bcdb-c0e652d9a294', embedding=None, metadata={'page_label': '216', 'file_name': 'neural_network.pdf', 'file_path': '/content/GenAI---RAG-using-LangChain/pdf_data_to_read/neural_network.pdf', 'file_type': 'application/pdf', 'file_size': 6101665, 'creation_date': '2025-05-08', 'last_modified_date': '2025-05-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='216\\n\\x0c\\x0c\\x0c Is there a simple algorithm for intelligence?\\nMy own prejudice is in favour of there being a simple algorithm for intelligence. And\\nthe main reason I like the idea, above and beyond the (inconclusive) arguments above, is\\nthat it’s an optimistic idea. When it comes to research, an unjustiﬁed optimism is often more\\nproductive than a seemingly better justiﬁed pessimism, for an optimist has the courage to\\nset out and try new things. That’s the path to discovery , even if what is discovered is perhaps\\nnot what was originally hoped. A pessimist may be more “correct” in some narrow sense,\\nbut will discover less than the optimist.\\nThis point of view is in stark contrast to the way we usually judge ideas: by attempting\\nto ﬁgure out whether they are right or wrong. That’s a sensible strategy for dealing with\\nthe routine minutiae of day-to-day research. But it can be the wrong way of judging a big,\\nbold idea, the sort of idea that deﬁnes an entire research program. Sometimes, we have only\\nweak evidence about whether such an idea is correct or not. We can meekly refuse to follow\\nthe idea, instead spending all our time squinting at the available evidence, trying to discern\\nwhat’s true. Or we can accept that no-one yet knows, and instead work hard on developing\\nthe big, bold idea, in the understanding that while we have no guarantee of success, it is\\nonly thus that our understanding advances.\\nWith all that said, in its most optimistic form, I don’t believe we’ll ever ﬁnd a simple\\nalgorithm for intelligence. To be more concrete, I don’t believe we’ll ever ﬁnd a really short\\nPython (or C or Lisp, or whatever) program – let’s say, anywhere up to a thousand lines\\nof code – which implements artiﬁcial intelligence. Nor do I think we’ll ever ﬁnd a really\\neasily-described neural network that can implement artiﬁcial intelligence. But I do believe\\nit’s worth acting as though we could ﬁnd such a program or network. That’s the path to\\ninsight, and by pursuing that path we may one day understand enough to write a longer\\nprogram or build a more sophisticated network which does exhibit intelligence. And so it’s\\nworth acting as though an extremely simple algorithm for intelligence exists.\\nIn the 1980s, the eminent mathematician and computer scientist Jack Schwartz was\\ninvited to a debate between artiﬁcial intelligence proponents and artiﬁcial intelligence\\nskeptics. The debate became unruly , with the proponents making over-the-top claims about\\nthe amazing things just round the corner, and the skeptics doubling down on their pessimism,\\nclaiming artiﬁcial intelligence was outright impossible. Schwartz was an outsider to the\\ndebate, and remained silent as the discussion heated up. During a lull, he was asked to\\nspeak up and state his thoughts on the issues under discussion. He said: “Well, some of\\nthese developments may lie one hundred Nobel prizes away” (ref, page 22). It seems to me\\na perfect response. The key to artiﬁcial intelligence is simple, powerful ideas, and we can\\nand should search optimistically for those ideas. But we’re going to need many such ideas,\\nand we’ve still got a long way to go!\\nA', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're usng the HuggingFaceEmbedding instead of OpenAI, since OpenAI expects penny to pay.\n",
        "\n",
        " OpenAI, which means LlamaIndex is trying to use OpenAI embeddings even though you've configured Gemini as the LLM. (Llama Index default's embedding is OpenAI),\n",
        "\n",
        " Gemini does not support embeddings yet, so you need to use an open-source alternative like huggingface or local models."
      ],
      "metadata": {
        "id": "uXVVBjFloL9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Set up LLM and hugging face embedding model\n",
        "llm = Gemini(api_key=sec_key)\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"/content/GenAI---RAG-using-LangChain/pdf_data_to_read\").load_data()\n",
        "\n",
        "# Build index with custom LLM and embedding model\n",
        "# We're customizing the llama index vector store index by passing the customized llm and embed model.\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        "    show_progress=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "7ccbf9d5168f41d58d787c1fe220131c",
            "69b5848868ac4b40906248c5d934e0b5",
            "e753ec0d43314d93bc4ad1f7e17f9a29",
            "861ea62a335d46669382f33ef08bc147",
            "15efa18adc1d474baac2fba955107a9f",
            "27c8b0ed9cee48649afde1d498358747",
            "de9facaa01b24993b60e9c2d84959682",
            "9cd6d9af78f24cb39db3e23ceabe64c5",
            "93c2d2fcf1a24974937a17352d6997c0",
            "d021bfed8e7d4e8e905fb37d70cd2b9a",
            "a879e08059a54dcb856f8610545b2bda",
            "adb51c87ec7442c790d86f607490feb0",
            "d1b2bc3c5b004f6580c0f6348068cdbd",
            "0ecfa7d428f74a4b99a8a8aa417ae052",
            "afd274423a8446789f4f62e7020a04f3",
            "c3f8ba4a46e4470b9fc0e8b949e37e33",
            "89c9a092d9744580ab7c0ce0858e1066",
            "410861be74324a30ab03eebd428e64a9",
            "8dbb9ea46ac341e9a2a193f5186efece",
            "5b3072e26cc642159371d4499b26f5ab",
            "5d295e53eff94f389b07a00da24c00e4",
            "69e107d0eb734d778e6ea8863a99c648"
          ]
        },
        "collapsed": true,
        "id": "RDTjZLvJciNE",
        "outputId": "61885ba6-6f54-4722-d4a1-f29c7325518f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a88d090040d7>:6: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
            "  llm = Gemini(api_key=sec_key)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/255 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ccbf9d5168f41d58d787c1fe220131c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/258 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adb51c87ec7442c790d86f607490feb0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVoLi-iMgA9I",
        "outputId": "d34d07cb-6898-43ca-bf43-0810a9d98281"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7bcc3d73f610>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still query engine of llama index do use the openAI API, to by pass that operation, here we configure it with llm = llm"
      ],
      "metadata": {
        "id": "fsqxhCP5qCyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    llm=llm  # Your Gemini instance\n",
        ")"
      ],
      "metadata": {
        "id": "iASPwbW_fB6C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is softmax function?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "LwTIXZnzffKP",
        "outputId": "65187b77-a6c6-4021-ba4d-51d4033a902b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax is a way of rescaling values and compressing them together to form a probability distribution. It ensures that all output activations are positive and sum to 1.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Snippet to retrieve the quality results from llama index store.\n",
        "\n",
        "1. VectorIndexRetriever - This creates a retriever that fetches the top 4 most similar documents (or \"nodes\") from your VectorStoreIndex based on their embedding similarity to the user query.\n",
        "\n",
        "2. SimilarityPostprocessor - This is a post-processing filter. After retrieval, it removes documents whose similarity score is below 0.80, ensuring only highly relevant results are passed to the LLM.\n",
        "\n",
        "3. RetrieverQueryEngine - Query engine which wraps the above functions."
      ],
      "metadata": {
        "id": "0fJyFVK5qYux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note\n",
        "The **RetrieverQueryEngine** does not automatically use the LLM from the index.\n",
        "\n",
        "Unlike index.as_query_engine(), it does not implicitly inherit the LLM. So if you construct RetrieverQueryEngine directly, it defaults back to OpenAI, unless you manually provide a response_synthesizer that wraps your desired LLM (Gemini, in your case)."
      ],
      "metadata": {
        "id": "eIZerkUEq_QW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7uWRWrAUY9b8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "\n",
        "retriever=VectorIndexRetriever(index=index,similarity_top_k=4)\n",
        "postprocessor=SimilarityPostprocessor(similarity_cutoff=0.60)\n",
        "\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(llm=llm) #Response synthesizer should be configured with the desired llm else VectorIndexRetriever & Query Engine will use the OpenAI as default.\n",
        "\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[postprocessor],\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g_6ZrX2gY9b-"
      },
      "outputs": [],
      "source": [
        "response=query_engine.query(\"what is softmax function?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r7ckODWdY9cA",
        "outputId": "4f579aff-6f43-499d-a175-98c3be541d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: Softmax is a way of rescaling values and compressing\n",
            "them together to form a probability distribution. The exponentials in\n",
            "the equation for softmax ensure that all output activations are\n",
            "positive, and the sum in the denominator ensures that the softmax\n",
            "outputs sum to 1.\n",
            "______________________________________________________________________\n",
            "Source Node 1/1\n",
            "Node ID: c5e0b0c3-57c5-4142-a18a-9ba3d9bc0473\n",
            "Similarity: 0.6017603866170474\n",
            "Text: 72     Improving the way neural networks learn behave. Just to\n",
            "review where we’re at: the exponentials in Equation (3.24) ensure that\n",
            "all the output activations are positive. And the sum in the\n",
            "denominator of Equation (3.24) ensures that the softmax outputs sum to\n",
            "1. So that particular form no longer appears so mysterious: rather, it\n",
            "is a natura...\n",
            "Softmax is a way of rescaling values and compressing them together to form a probability distribution. The exponentials in the equation for softmax ensure that all output activations are positive, and the sum in the denominator ensures that the softmax outputs sum to 1.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "pprint_response(response,show_source=True)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ccbf9d5168f41d58d787c1fe220131c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69b5848868ac4b40906248c5d934e0b5",
              "IPY_MODEL_e753ec0d43314d93bc4ad1f7e17f9a29",
              "IPY_MODEL_861ea62a335d46669382f33ef08bc147"
            ],
            "layout": "IPY_MODEL_15efa18adc1d474baac2fba955107a9f"
          }
        },
        "69b5848868ac4b40906248c5d934e0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c8b0ed9cee48649afde1d498358747",
            "placeholder": "​",
            "style": "IPY_MODEL_de9facaa01b24993b60e9c2d84959682",
            "value": "Parsing nodes: 100%"
          }
        },
        "e753ec0d43314d93bc4ad1f7e17f9a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd6d9af78f24cb39db3e23ceabe64c5",
            "max": 255,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93c2d2fcf1a24974937a17352d6997c0",
            "value": 255
          }
        },
        "861ea62a335d46669382f33ef08bc147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d021bfed8e7d4e8e905fb37d70cd2b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_a879e08059a54dcb856f8610545b2bda",
            "value": " 255/255 [00:01&lt;00:00, 180.39it/s]"
          }
        },
        "15efa18adc1d474baac2fba955107a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c8b0ed9cee48649afde1d498358747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9facaa01b24993b60e9c2d84959682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd6d9af78f24cb39db3e23ceabe64c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c2d2fcf1a24974937a17352d6997c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d021bfed8e7d4e8e905fb37d70cd2b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a879e08059a54dcb856f8610545b2bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adb51c87ec7442c790d86f607490feb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1b2bc3c5b004f6580c0f6348068cdbd",
              "IPY_MODEL_0ecfa7d428f74a4b99a8a8aa417ae052",
              "IPY_MODEL_afd274423a8446789f4f62e7020a04f3"
            ],
            "layout": "IPY_MODEL_c3f8ba4a46e4470b9fc0e8b949e37e33"
          }
        },
        "d1b2bc3c5b004f6580c0f6348068cdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89c9a092d9744580ab7c0ce0858e1066",
            "placeholder": "​",
            "style": "IPY_MODEL_410861be74324a30ab03eebd428e64a9",
            "value": "Generating embeddings: 100%"
          }
        },
        "0ecfa7d428f74a4b99a8a8aa417ae052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dbb9ea46ac341e9a2a193f5186efece",
            "max": 258,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b3072e26cc642159371d4499b26f5ab",
            "value": 258
          }
        },
        "afd274423a8446789f4f62e7020a04f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d295e53eff94f389b07a00da24c00e4",
            "placeholder": "​",
            "style": "IPY_MODEL_69e107d0eb734d778e6ea8863a99c648",
            "value": " 258/258 [00:01&lt;00:00, 229.94it/s]"
          }
        },
        "c3f8ba4a46e4470b9fc0e8b949e37e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89c9a092d9744580ab7c0ce0858e1066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "410861be74324a30ab03eebd428e64a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dbb9ea46ac341e9a2a193f5186efece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3072e26cc642159371d4499b26f5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d295e53eff94f389b07a00da24c00e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e107d0eb734d778e6ea8863a99c648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}